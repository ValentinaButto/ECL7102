---
title: "Régression linéaire"
date: "<br/>7 octobre 2019"
output: 
  xaringan::moon_reader:
    css: ["metropolis", "metropolis-fonts", "styles-xar.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.dim = c(6, 4))
library(dplyr)
library(ggplot2)
library(cowplot)
```

# Question éclair

Les affirmations suivantes décrivent l’effet de différents prédicteurs sur la probabilité d’initiation d’un feu de forêt. Laquelle décrit un effet d’**interaction** de deux prédicteurs sur la réponse?

(a)	La probabilité augmente avec l’âge du peuplement pour un peuplement mixte, mais diminue avec l’âge du peuplement pour un peuplement de conifères.

(b)	La probabilité augmente avec l’âge du peuplement, mais diminue avec la quantité de précipitation.

(c)	La probabilité dépend de l’âge du peuplement et de la quantité de précipitation, et les peuplements plus âgés tendent aussi à recevoir plus de précipitations. 

---

class: inverse, middle

# Régression: Vue d'ensemble

---

# Rôle de l'analyse de régression

Modèles qui représentent la relation mathématique entre une variable *réponse* et une ou plusieurs *prédicteurs*. 

--

- Analyser les résultats d'une expérience avec une ou plusieurs variables de traitement numériques (ex.: température, dose).

--

- Séparer l'effet de traitements discrets (variables catégorielles) de celui d'autres conditions expérimentales représentées par des variables numériques (analyse de la covariance).

--

- Déterminer l'importance des associations entre des variables mesurées dans la nature (sans supposer de lien de causalité).

--

- Utiliser les associations entre prédicteurs et réponse afin de prédire la valeur de cette dernière pour de nouvelles observations.

---

# Prochains cours

| | |
|------------|---------------------|
| 7 octobre | Régression linéaire |
| 14 octobre | *Congé*
| 21 octobre | Sélection de modèles <sup>*</sup> |
| 28 octobre | *Congé* |
| 4 novembre | Régression logistique |
| 11 novembre | Régression de poisson |
| 18 et 25 novembre | Modèles mixtes |

<sup>*</sup> Changement d'ordre par rapport au plan de cours

---

# Objectifs du cours d'aujourd'hui

- Estimer et interpréter les paramètres d'une régression linéaire simple et ceux d'une régression linéaire multiple, avec ou sans interactions.

- Vérifier les suppositions d'un modèle de régression à partir des graphiques de diagnostic.

- Différencier l'intervalle de confiance d'une droite de régression et l'intervalle de prédiction de nouvelles observations.

- Identifier les problèmes dûs à la collinéarité de plusieurs prédicteurs.

---

class: inverse, middle

# Régression linéaire simple

---

# Régression linéaire simple

Désigne le cas où il y a un seul prédicteur numérique ( $x$ ).

$$y = \beta_0 + \beta_1 x + \epsilon$$ 

---

# Exemple

Tableau de données `plant_growth_rate` tiré du livre *Getting Started with R, An Introduction for Biologists*: croissance de plantes en fonction de l'humidité du sol.

```{r}
pgr <- read.csv("../donnees/plant_growth_rate.csv")
str(pgr)
```

---

# Exemple

But: Trouver la droite qui passe le plus "près" des points du graphique de $y$ vs. $x$.

```{r, echo = FALSE}
ggplot(pgr, aes(x = soil.moisture.content, y = plant.growth.rate)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE)
```

---

# Méthode des moindres carrés

- Les meilleurs estimateurs non-biaisés de $\beta_0$ et $\beta_1$ sont ceux qui minimisent la somme du carré des résidus.

$$\sum_{k = 1}^n \epsilon_k^2 = \sum_{k = 1}^n (y_k - (\beta_0 + \beta_1 x_k))^2$$

---

# Méthode des moindres carrés

- L'estimateur de la pente $\beta_1$ est égal à la covariance de $x$ et $y$ divisée par la variance de $x$:

$$\hat{\beta_1} = \frac{\sum_{k = 1}^n (x_k - \bar{x})(y_k - \bar{y})}{\sum_{k = 1}^n (x_k - \bar{x})^2}$$

--

- L'estimateur de l'ordonnée à l'origine $\beta_0$ est égal à:

$$\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$$  

--

- Puisque $\bar{y} = \hat{\beta_0} + \hat{\beta_1} \bar{x}$, la droite estimée passe par le point $(\bar{x}, \bar{y})$.

---

# Coefficient de détermination

- Comme pour l'ANOVA, $R^2$ représente la fraction de la variance expliquée par le modèle.

$$R^2 = 1 - \frac{\sum_{k=1}^n (y_k - \hat{y_k})^2}{\sum_{k=1}^n (y_k - \bar{y})^2}$$

--

- $\hat{y_k} = \hat{\beta_0} + \hat{\beta_1} x_k$ est la valeur attendue (moyenne) pour $y_k$ selon le modèle.

--

- Pour une régression linéaire simple, la racine carrée de $R^2$ est égale à la corrélation entre $x$ et $y$.

---

# Intervalle de confiance vs. prédiction

- Intervalle de confiance de la droite de régression: représente l'incertitude sur la valeur **moyenne** de la réponse pour une certaine valeur du prédicteur.

--

- Intervalle de prédiction: représente l'incertitude sur la valeur de la réponse pour une **nouvelle observation**, en connaissant la valeur des prédicteurs.

--

- Il n'est pas prudent d'utiliser le résultat d'une régression pour prédire la réponse hors de l'étendue des valeurs des prédicteurs présentes dans les données.

---

class: inverse, middle

# Suppositions du modèle de régression linéaire

---

# Suppositions du modèle linéaire

Comme pour l'ANOVA, les résidus sont:

- indépendants et
- normalement distribués 
- avec la même variance.

--

En plus:

- la relation entre la réponse moyenne et les prédicteurs est linéaire 
- les prédicteurs sont mesurés sans erreur (ou erreur négligeable).

---

# Linéarité

Une transformation peut convertir un modèle non-linéaire en modèle linéaire.

Exemple: Transformation logarithmique

$$y = a x^b$$

--

$$\log(y) = a + b \log(x)$$

---

# Linéarité

L'équation reliant $x$ et $y$ peut contenir des fonctions non-linéaires de $x$, en autant qu'elle soit une fonction linéaire des *coefficients*.

--

Exemple: Modèle quadratique

$$y = \beta_0 + \beta_1 x + \beta_2 x^2$$

---

# Indépendance des résidus

- La portion des observations $y$ non-expliquée par les prédicteurs $x$ est indépendante d'une observation à l'autre. 

--

- La non-indépendance des résidus est souvent due à une proximité de certaines observations dans l'espace et dans le temps.

--

- Une solution possible est d'inclure dans le modèle les facteurs pouvant causer cette dépendance temporelle et spatiale.

--

- La non-indépendance des résidus ne biaise pas les estimés des coefficients du modèle, mais leur incertitude sera sous-estimée. Les intervalles de confiance et les tests d'hypothèse sur la significativité des coefficients ne seront pas valides.

---

# Graphiques de diagnostic

Pour le graphique des résidus vs. valeurs attendues (*residuals vs. fitted*):

- Les résidus sont-ils disperés aléatoirement autour de zéro?

- La variance des résidus est-elle approximativement constante le long de l'axe des $x$?

---

# Effet de levier

- Une observation avec un fort effet de levier (*leverage*) a une plus grande influence sur les coefficients de la régression.

- Le plus souvent, l'observation est isolée et loin de la moyenne des prédicteurs. 

--

- La distance de Cook (*Cook's distance*) $D$ mesure à la fois l'effet de levier et la magnitude du résidu. Un point avec un grand $D$ ( $D > 1$ ) peut faire dévier la régression par rapport à la tendance des autres points.

---

class: inverse, middle

# Régression linéaire multiple

---

# Régression linéaire multiple

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_m x_m + \epsilon$$
--

$$y = \beta_0 + \sum_{i = 1}^m \beta_i x_i + \epsilon$$ 

--

Les coefficients $\beta$ sont estimés à partir de la méthode des moindres carrés.

---

# Analyse de la covariance

Modèle incluant un prédicteur catégoriel et un prédicteur numérique.

## Exemple

Le tableau de données `compensation.csv` tiré du livre de Crawley, *Statistics: An introduction using R* contient des données sur la masse des graines produites par une espèce de plante (*Fruit*) en fonction de la taille des racines (*Root*) et selon que la plante subisse ou non un pâturage (*Grazing*). 

```{r}
comp <- read.csv("../donnees/compensation.csv")
str(comp)
```

---

# Modèle avec interaction

- L'interaction est représentée par un prédicteur supplémentaire égal au produit des deux variables qui interagissent.

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \epsilon$$

--

- Le modèle avec interaction est équivalent à estimer séparément la droite de régression (ordonnée à l'origine et pente) pour chacun des traitements. 

---

# Plusieurs prédicteurs numériques

## Exemple 

Le tableau de données `hills` du package *MASS* contient les records de temps (*time*, en minutes) pour des courses de vélo en Écosse en fonction de la distance horizontale (*dist*, en milles) et le dénivelé total du parcours (*climb*, en pieds).

```{r, warning = FALSE, message = FALSE}
library(MASS)
str(hills)
```

---

# Normalisation des variables

- Pour chaque point, la variable normalisée indique l'écart à la moyenne de la variable originale, exprimé en multiple de l'écart-type de la variable originale. 

--

- Puisque les prédicteurs normalisés prennent une valeur de 0 à leur moyenne, la valeur de l'ordonnée à l'origine de la régression est la moyenne générale de la réponse.

--

- La normalisation des prédicteurs ne fait que changer l'échelle des effets estimés. La significativité de l'effet de chaque prédicteur et les prédictions du modèle restent les mêmes.

---

# Interaction entre variables continues

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \epsilon$$


- $\beta_0$: ordonnée à l'origine ( $x_1 = 0$ et $x_2 = 0$ );

--

- $\beta_1$: pente de $y$ vs. $x_1$ si $x_2 = 0$;
- $\beta_2$: pente de $y$ vs. $x_2$ si $x_1 = 0$;

--

- $\beta_{12}$: changement de la pente de $y$ vs. $x_1$ lorsque $x_2$ augmente de 1, *ou* changement de la pente de $y$ vs. $x_2$ lorsque $x_1$ augmente de 1.

---

# Collinéarité

- La corrélation entre deux prédicteurs complique l'estimation des effets de chaque prédicteur. 

--

- Puisque les coefficients du modèle représente l'effet d'un prédicteur lorsque les autres demeurent constants, lorsque plusieurs d'entre eux varient ensemble, il devient difficile d'isoler l'effet de chacun. 

--

- Avec plus de deux prédicteurs, ce problème survient si l'un des prédicteurs est corrélé avec une combinaison linéaire des autres prédicteurs (*collinéarité*).

---

# Facteur d'inflation de la variance (VIF)

- Le VIF est égal à $1 - 1/R^2$, où $R^2$ est le coefficient de détermination d'un modèle linéaire du prédicteur considéré en fonction de tous les autres.

--

- Exemple: si un des prédicteurs peut être déterminé à partir de la valeur des autres avec un $R^2$ de 0.9, VIF = 10. 

--

- Lorsque le VIF de certains prédicteurs dépasse 10, il est recommandé d'éliminer un des prédicteurs redondants.

---

# Exemple

Le tableau de données `msleep` inclus avec le package *ggplot*, contient des données sur le sommeil de différentes espèces de mammifères. 

```{r}
str(msleep)
```

---

# Résumé

- La fonction `lm` effectue l'ajustement d'un modèle de régression linéaire dans R.

--

- Dans une régression linéaire multiple (sans interaction), le coefficient associé à un prédicteur mesure l'effet d'une variation de 1 du prédicteur sur la réponse, si les autres prédicteurs demeurent constants.

--

- Pour un modèle sans interaction, l'effet d'une variable catégorielle peut être représenté par une translation de la droite de régression entre les traitements.

---

# Résumé

- Le facteur d'inflation de la variance (VIF) indique si la valeur d'un prédicteur est fortement corrélée à celles des autres prédicteurs. Un VIF élevé rend difficile l'estimation des coefficients pour les prédicteurs corrélés.

--

- L'intervalle de confiance d'une droite de régression représente l'incertitude sur la valeur moyenne de $y$ pour des valeurs données des prédicteurs. L'intervalle de prédiction représente l'incertitude sur la valeur d'une observation future de $y$, connaissant la valeur des prédicteurs.


