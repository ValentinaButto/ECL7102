---
title: "Analyses multivariées, Partie 1"
date: "<br/>2 décembre 2019"
output: 
  xaringan::moon_reader:
    css: ["metropolis", "metropolis-fonts", "styles-xar.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.dim = c(6, 4))
library(dplyr)
library(ggplot2)
library(cowplot)
```

```{css, echo = FALSE}
.small { font-size: 70% }
```

# Objectifs

- Décrire différents rôles des techniques d'analyse multivariées.

- Décrire l'utilité des méthodes d'ordination et de regroupement pour réduire la dimensionalité d'un jeu de données.

- Exécuter une analyse en composantes principales et pouvoir interpréter les résultats.

- Diviser les observations en classes à partir des méthodes de regroupement hiérarchique.

---

# Pourquoi les analyses multivariées?

Les analyses multivariées servent à modéliser la *distribution conjointe* de plusieurs variables (distribution des variables individuelles et corrélations). Par exemple:

--

- visualiser les similitudes entre sites d'observation en fonction de plusieurs variables mesurées pour chaque site;

--

- expliquer une réponse multivariée;

--
    
- expliquer une réponse univariée en fonction d'un grand nombre de prédicteurs partiellement corrélés.
    
---

# Structure des données multivariées

Matrice $X$ représentant $n$ observations de $p$ variables.

$$  
\begin{bmatrix}
x_{11} & x_{12} & ... & x_{1p} \\
x_{21} & x_{22} & ... & x_{2p} \\
... & ... & ... & ... \\
x_{n1} & ... & ... & x_{np}
\end{bmatrix}
$$

---

# Exemple

Données climatiques pour 49 grandes villes (source: WorldClim).

.small[

```{r}
villes <- read.csv("../donnees/cities_climate.csv")
head(villes)
```

]

---

# Normalisation des données

- Les méthodes d'ordination et de regroupement sont basées sur le concept de *distance* entre les points dans l'espace à $p$ dimensions. 

--

- On ne peut pas comparer cette distance selon plusieurs variables avec des unités différentes.

--

- C'est pourquoi on commence par normaliser les variables: soustraire de chaque colonne sa moyenne et diviser par son écart-type (fonction `scale` dans R). Le résultat est une matrice où chaque colonne a une moyenne de 0 et un écart-type de 1. 

---

# Réduction de la dimensionnalité

- Comme on ne peut pas visualiser un nuage de points en 9 dimensions, il serait utile de réduire la dimensionnalité des données tout en conservant le maximum d'information sur la variation entre les villes.

--

- Les méthodes d'**ordination** effectuent une transformation des $p$ variables originales en un nombre plus restreint de nouvelles variables, en reproduisant le plus fidèlement possible les distances entre les points dans l'espace original en $p$ dimensions. Ex.: analyse en composantes principales.

--

- Les méthodes de **regroupement** (*clustering*) divisent les observations en un nombre discret de groupes (ou catégories) en fonction de leur proximité dans l'espace à $p$ dimensions. Ex.: classification hiérarchique ascendante.

---

class: inverse, middle

# Analyse en composantes principales (ACP)

---

# Introduction

Nuage de points de 3 variables climatiques pour les 49 villes:

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.dim = c(8, 6)}
library(plotly)
plot_ly(villes, x = ~t_mean, y = ~t_sd, z = ~p_ann) %>%
    add_markers(size = 1) %>%
    add_text(text = ~city, size = 0.5)
```

---

# Introduction

Considérons le cas extrême où une des trois variables est parfaitement prédite par les deux autres (collinéarité parfaite), par exemple $z = 2x - 5y$.

```{r, echo = FALSE, fig.dim = c(8, 6)}
df <- data.frame(x = rnorm(100, 0, 1), y = rnorm(100, 0, 1))
df <- mutate(df, z = 2*x - 5*y)
plot_ly(df, x = ~x, y = ~y, z = ~z) %>%
    add_markers(size = 1) %>%
    add_mesh(opacity = 0.3)
```

---

# Fonctionnement de l'ACP

L'ACP est une transformation de la matrice $X$ ( $n$ observations $\times ~ p$ variables, préalablement normalisées) choisie pour que:

--

- le premier axe soit dans la direction de variation maximale des données;
    
--

- le deuxième axe soit dans la direction de variation maximale qui est orthogonale (perpendiculaire) au premier;
    
--

- le troisième axe soit dans la direction de variation maximale qui est orthogonale aux deux premiers; et ainsi de suite.
    
--
    
Ces axes (composantes principales) sont donc orthogonaux (non-corrélés) et classés en ordre décroissant de variance expliquée.

---

# Fonctionnement de l'ACP

- Mathématiquement, l'ACP requiert le calcul des valeurs propres et vecteurs propres de la matrice des covariances entre paires de variables dans $X$. Les vecteurs propres indiquent la direction des composantes principales et les valeurs propres indiquent la variance des données sur chaque axe.

--

- L'ACP est basée sur la théorie des distributions normales multivariées, donc fonctionne mieux lorsque les variables s'approchent de la normalité.

--

- Nous verrons au prochain cours des méthodes d'ordination adaptées aux données de présence-absence et d'abondance communes en écologie.

---

# ACP avec R

- `prcomp`: Exécute une ACP à partir d'une matrice de données multivariées.

--

- Le résultat de `prcomp` indique la transformation linéaire effectuée sur les variables originales pour former les composantes principales (`$rotation`), ainsi que les coordonnées transformées de chaque observation (`$x`).

--

- `summary`: Sommaire d'une ACP, qui donne pour chaque composante principale son écart-type, la fraction de la variance totale contenue dans cette composante et la fraction cumulative de la variance pour les composantes jusqu'à celle-ci. 

---

# ACP avec R

- `biplot`: Présente un diagramme de double projection, avec la position des observations et direction des variables originales.

    + Les observations plus rapprochées sont plus similaires selon les composantes affichées.
    + Les variables pointant dans une direction semblable sont positivement corrélées, celles pointant dans des directions opposées sont négativement corrélées.

---

# ACP et régression

- L'ACP peut transformer un grand nombre de prédicteurs corrélés en un plus petit nombre de variables non-corrélées.

--

- Principal inconvénient: les effets sont plus difficiles à interpéter.

--

- Tout de même utile dans les cas où on s'intéresse d'abord à prédire la réponse et que l'effet d'un prédicteur individuel est moins important.

    + Exemple: Prédire la composition de végétation à partir d'images de télédétection hyperspectrale (réflectance pour des centaines de longueurs d'onde).
    
---

# Combien de composantes choisir?

- Variances associées aux composantes principales; leur moyenne est 1 pour des données normalisées.

--

- Critère simple: conserver les composantes avec une variance supérieure à 1.

--

- D'autres critères basés sur des tests statistiques formels, voir Peres-Neto et al. (2005) pour une comparaison.

--

- Dans une analyse exploratoire, le choix du nombre de composantes n'est pas critique. Pour une ACP suivie d'une régression, on peut procéder par sélection de modèles.

---

class: inverse, middle

# Classification hiérarchique ascendante

---

# Classification hiérarchique ascendante 

- La classification hiérarchique ascendante vise à créer des groupes d'observations semblables selon une série de variables. 

--

- Le nom vient du fait qu'on commence avec les observations individuelles, qui sont regroupées progressivement jusqu'à ce qu'on obtienne un seul groupe.

--

- La structure résultante est un arbre ou *dendrogramme* montrant les relations de proximité pour l'ensemble des observations.

---

# Matrice de distance

- L'algorithme de classification hiérarchique requiert une matrice de la distance $d_{ij}$ pour chaque paire d'observations $i$ et $j$.

$$  
\begin{bmatrix}
0 \\
d_{21} & 0  \\
d_{31} & d_{32} & 0 \\
... \\
d_{n1} & d_{n2} & ... & d_{n(n-1)} & 0
\end{bmatrix}
$$

--

- La fonction `dist` dans R calcule la matrice de distance entre les rangées d'une matrice. 

--

- Comme pour l'ACP, les différentes variables doivent être normalisées au préalable.

---

# Matrice de distance

- Pour des variables numériques, la distance la plus couramment utilisée est la *distance euclidienne*:

$$d_{ij} = \sqrt{\sum_{k = 1}^p (x_{ik} - x_{jk})^2}$$

--

- En deux dimensions, cela correspond à la distance en ligne droite calculée par le théorème de Pythagore:

$$d_{ij} = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$$

---

# Algorithme de classification

- Supposons qu'on ait la matrice de distance suivante entre 4 observations.

$$
\begin{array}
&& A & B & C \\
B & 11 \\
C & 13 & 6 \\ 
D & 7 & 11 & 9 
\end{array}
$$ 

--

- L'algorithme regroupe d'abord les deux observations les plus rapprochées, soit B et C (distance de 6). 

--

- Ensuite, il remplace B et C par une observation BC et calcule la distance entre ce groupe et chacune des autres observations existantes.

---

# Algorithme de classification

- Différents critères possibles pour la distance entre un groupe et une observation.

--

- Utilisons une règle ou la distance à partir d'un groupe est la moyenne des distances des éléments de ce groupe.

--

$$
\begin{array}
&& A & B & C \\
B & 11 \\
C & 13 & 6 \\ 
D & 7 & 11 & 9 
\end{array}
$$ 

---

# Algorithme de classification

- Différents critères possibles pour la distance entre un groupe et une observation.

- Utilisons une règle ou la distance à partir d'un groupe est la moyenne des distances des éléments de ce groupe.

$$
\begin{array}
& & A & BC \\
BC & 12 & \\
D & 7 & 10 
\end{array}
$$ 

---

# Algorithme de classification

- Le nouveau groupe (BC) est traité comme une observation et on répète le processus jusqu'à ce qu'il n'y ait que deux groupes, joints à la dernière étape.

$$
\begin{array}
& & A & BC \\
BC & 12 & \\
D & 7 & 10 
\end{array}
$$ 

---

# Algorithme de classification

- Le nouveau groupe (BC) est traité comme une observation et on répète le processus jusqu'à ce qu'il n'y ait que deux groupes, joints à la dernière étape.

$$
\begin{array}
&  &AD  \\
BC & 11
\end{array}
$$ 

---

# Critères de regroupement

L'argument `method` indique comment calculer la distance entre groupes A et B:

--

- saut minimum (`method = "single"`): distance minimale entre une observation dans A et une observation dans B.

--

- saut maximum (`method = "complete"`): distance maximale entre une observation dans A et une observation dans B.

--

- lien moyen (`method = "average"`): moyenne des distances sur l'ensemble des paires d'observations dont une est dans A et une dans B.

--

- la critère de Ward (`method = "ward.D2"`): regroupement qui minimise l'augmentation de la variance totale intra-groupe. 

---

# Classification hiérarchique dans R

- `dist`: Calcule une matrice de distance.

- `hclust`: Effectue une classification hiérarchique ascendante à partir d'une matrice de distance, selon une méthode donnée.

- `plot`: Appliquée au résultat de `hclust`, produit un dendrogramme.

- `cutree`: Coupe le dendrogramme à une certaine hauteur *h*, ou pour obtenir un certain nombre de groupes *k*. 

---

# Résumé

- Les analyses multivariées servent à décrire et expliquer la distribution conjointe de plusieurs variables.

--

- Lorsque les variables représentent différentes unités, il est important de les normaliser afin d'obtenir des échelles comparables.

--

- L'ordination vise à produire dans un nombre de dimensions réduit (souvent 2) la représentation la plus fidèle possible de la variation entre les observations multivariées.

---

# Résumé

- L'analyse en composantes principales (ACP) est une méthode d'ordination qui effectue une transformation linéaire (rotation) des variables originales, afin d'obtenir des composantes principales qui sont non-corrélées et classées en ordre décroissant de variance. 

--

- Les coordonnées des observations et les variables originales peut être représentées simultanément dans l'espace des composantes principales (diagramme de double projection).

---

# Résumé

- Les méthodes de regroupement visent à classer les observations multivariées dans un petit nombre de groupes d'observations les plus similaires possibles. 

--

- La classification hiérarchique ascendante effectue un regroupement graduel à partir des observations individuelles et en fonction de différentes règles qui définissent la distance entre groupes. Cette classification produit un dendrogramme.

---

# Références

- Manly, B.F. et Alberto, J.A.N. (2016) *Multivariate statistical methods: a primer*. Chapman and Hall/CRC.

- Legendre, P. et Legendre, L. (2012) *Numerical Ecology*, 3e éd. Elsevier.

- Borcard, D., Gillet, F. et Legendre, P. (2018) *Numerical Ecology with R*, 2e éd. Springer.
