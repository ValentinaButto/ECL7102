<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Statistical models, parameters and estimators</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical models, parameters and estimators</h1>
<h4 class="date"><em><br/>Sept. 4, 2018</em></h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<p>Statistical distributions</p>
<ul>
<li>Describe the characteristics and use of normal and log-normal distributions.</li>
<li>Know the relationship between probability density and cumulative probability for a continuous variable, and calculate these quantities in R.</li>
<li>Compare data to a reference distribution with a quantile-quantile plot.</li>
</ul>
<p>Parameter estimation</p>
<ul>
<li>Estimate the mean and variance of a population from a sample.</li>
<li>Define the bias and standard error of an estimator.</li>
<li>Calculate the properties of an estimator by simulation.</li>
<li>Interpret a confidence interval and calculate the confidence interval for the mean of a normal distribution.</li>
</ul>
</div>
<div id="statistics-parameters-and-estimators" class="section level1">
<h1>Statistics, parameters and estimators</h1>
<p>At the last class, we saw a series of descriptive statistics: mean, variances, quantiles and others. In general, a <em>statistic</em> is a quantity calculated from observations of random variables.</p>
<p>In this class, we will consider the observations as the result of a random process described by a statistical model including some <em>parameters</em>. Our main goal will be to determine to what extent a statistic computed from the observations constitutes a good <em>estimator</em> of the desired parameter.</p>
<p>For example, if we measure the weight of red squirrels and average these measurements (a statistic), what is our estimate of the average weight of the red squirrel population (a parameter)? What is its margin of error?</p>
<blockquote>
<p>In general, a parameter is a theoretical quantity. In our example, even if we could imagine a census of all the squirrels, the weight of the individuals varies constantly and the composition of the population too (because of the births, deaths and migrations).</p>
</blockquote>
<p>At the end of the course, we will be able to describe the <em>bias</em>, <em>variance</em> and <em>confidence interval</em> of an estimator. Before we get there, we will first review the basic mathematical concepts for describing statistical models, as well as a particularly important model, the normal distribution.</p>
</div>
<div id="probability-distributions" class="section level1">
<h1>Probability distributions</h1>
<p>A distribution is a function that associates a probability with each possible value of a random variable.</p>
<div id="discrete-distribution" class="section level2">
<h2>Discrete distribution</h2>
<p>When the variable is discrete, each value has a probability mass, the sum of which must be equal to 1. For example, if <span class="math inline">\(x\)</span> is the number obtained by rolling a balanced six-sided die, the probability of <span class="math inline">\(x\)</span> is 1/6 for each of the numbers from 1 to 6. Since the probability is the same for each value, this would be a <strong>uniform distribution</strong>.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="continuous-distribution" class="section level2">
<h2>Continuous Distribution</h2>
<p>When the variable is continuous, the number of possible values is infinite, so the probability of precisely obtaining a given value is zero. The distribution function therefore associates a <em>density</em> of probability with a given value.</p>
<p>For example, here is a uniform probability distribution between 0 and 6. The probability density is constant (1/6) in the interval and zero outside.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>For a continuous distribution, the probability that the variable is within a given interval is the integral (the area under the curve) of the probability density in that interval.</p>
<p>Here, the probability density is rectangular, so it is easy to calculate the probability of an interval. For example, the probability of getting a value between 2.5 and 3 is 1/2 (width of the interval) x 1/6 (probability density) = 1/12 (~ 0.083). This value corresponds to the area of the filled rectangle in the graph below.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The integral of the probability density over the set of values of <span class="math inline">\(x\)</span> (total probability) must be equal to 1.</p>
</div>
</div>
<div id="the-normal-distribution" class="section level1">
<h1>The normal distribution</h1>
<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>In R, the following command generates one value (<code>n = 1</code>) from the uniform distribution between 0 and 6.</p>
<pre class="r"><code>x &lt;- runif(n = 1, min = 0, max = 6)
x</code></pre>
<pre><code>## [1] 4.89407</code></pre>
<p>A quantity computed from random variables is itself a random variable. Suppose, then, that we are interested in the sum of several draws from this uniform distribution. We create a function that generates <span class="math inline">\(n\)</span> values of the distribution and calculates their sum. We then generate 10,000 values of this sum (with <code>replicate</code>) for a given value of <span class="math inline">\(n\)</span>.</p>
<pre class="r"><code># Sum of n random variables with a uniform distribution between min and max
sum_unif &lt;- function(n, min, max) {
    sum(runif(n, min, max))
}

n &lt;- 10
x &lt;- replicate(10000, sum_unif(n, 0, 6))</code></pre>
<p>Here is a histogram of the values of the sum for different values of <span class="math inline">\(n\)</span>. What do you notice?</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The sum of 2 values has a triangular distribution, but starting from <span class="math inline">\(n\)</span> = 5, we see the bell shape of a normal distribution.</p>
</div>
<div id="law-of-large-numbers" class="section level2">
<h2>Law of large numbers</h2>
<p>In the example, we used a <strong>simulation</strong> to illustrate the distribution of random variables obtained by summing 2, 5, 10 and 20 values of a uniform random variable between 0 and 6. Each of these four variables (sum of 2, 5, 10 and 20) has a certain theoretical distribution, which we do not know. However, by simulating the process that generates this variable a large number of times (here, 10,000), we obtain a virtual sample whose empirical distribution (histogram) is close to the theoretical distribution.</p>
<p>Thus, the larger the size of a random sample, the larger the distribution of values in that sample approaches the distribution of the variable in the population. This is called the <strong>law of large numbers</strong>.</p>
<p>The law of large numbers explains why our simulated sample offers a good approximation of the theoretical distribution of each sum. The fact that this theoretical distribution approaches a normal distribution when the number of variables in the sum increases is another important principle in statistics, the central limit theorem.</p>
</div>
<div id="central-limit-theorem" class="section level2">
<h2>Central limit theorem</h2>
<p>The central limit theorem stipulates that when we sum a large number of independent random variables, regardless of the distribution of individual variables, the distribution of their sum approximates a normal distribution.</p>
<p><em>To be strict, some technical conditions should be included about the variables that are summed, but the simplified definition above is sufficient for this course.</em></p>
<p>This property of the normal distribution partly explains why it constitutes such an important model in statistics. As discussed in the introduction to the course, a statistical distribution is often used to represent the unexplained variation of a variable due to a large number of unobserved processes in a complex system. If we suppose that this variation is due to several small effects that are independent and additive, then it is natural that the result approaches a normal distribution. However, it is important to check this assumption for a given variable.</p>
</div>
<div id="normal-distribution" class="section level2">
<h2>Normal distribution</h2>
<p>If a variable <span class="math inline">\(x\)</span> follows a normal (also called Gaussian) distribution, its probability density is given by:</p>
<p><span class="math display">\[f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2}\]</span></p>
<p>This distribution has two <em>parameters</em>, <span class="math inline">\(\mu\)</span> (which is the average of <span class="math inline">\(x\)</span>) and <span class="math inline">\(\ sigma\)</span> (which is its standard deviation).</p>
<p>On a graph of <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(\mu\)</span> is the position of the center of the distribution, while <span class="math inline">\(\sigma\)</span> is its dispersion; as <span class="math inline">\(\sigma\)</span> increases, the distribution widens and becomes less concentrated around its mean.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-7-1.png" width="100%" /></p>
</div>
<div id="standard-normal-distribution" class="section level2">
<h2>Standard normal distribution</h2>
<p>If a variable <span class="math inline">\(x\)</span> follows a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, we can obtain a standardized version of <span class="math inline">\(x\)</span> (denoted <span class="math inline">\(z\)</span>) by subtracting <span class="math inline">\(\mu\)</span>, then dividing by <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[z = \frac{x - \mu}{\sigma}\]</span></p>
<blockquote>
<p>In R, the <code>scale(x)</code> function applied to a vector <code>x</code> standardizes it (by subtracting the mean of <code>x</code> and dividing by the standard deviation).</p>
</blockquote>
<p>The variable <span class="math inline">\(z\)</span> then follows a standard normal distribution, that is, with <span class="math inline">\(\mu\)</span> = 0 and <span class="math inline">\(\sigma\)</span> = 1:</p>
<p><span class="math display">\[f(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} z^2}\]</span></p>
<p>In other words, any normal distribution can be obtained from <span class="math inline">\(f(z)\)</span> by moving the center by a distance <span class="math inline">\(\mu\)</span> and widening the distribution by a factor <span class="math inline">\(\sigma\)</span>.</p>
<p>The values of <span class="math inline">\(z\)</span> represent the distance from the mean, expressed in standard deviation units, eg: <span class="math inline">\(z\)</span> = -1.5 means one standard deviation and half below average.</p>
</div>
<div id="cumulative-distribution" class="section level2">
<h2>Cumulative distribution</h2>
<p>We have previously seen that the probability that a continuous random variable is found in a certain interval corresponds to the area under the curve (the integral) of the probability density in this interval.</p>
<p>The cumulative distribution of a random variable is, for each value <span class="math inline">\(x\)</span>, the probability that the value of the variable is less than or equal to <span class="math inline">\(x\)</span>. It is therefore equal to the area under the curve of the probability density on the left of <span class="math inline">\(x\)</span>.</p>
<p>Here is an illustration of the cumulative distribution <span class="math inline">\(F(z)\)</span> of a standard normal variable <span class="math inline">\(z\)</span>.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>From the cumulative distribution <span class="math inline">\(F(x)\)</span>, we can easily calculate the probability in an interval (<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>) by subtraction, i.e. <span class="math inline">\(F(x_2)\)</span> - <span class="math inline">\(F(x_1)\)</span>.</p>
</div>
<div id="distribution-functions-in-r" class="section level2">
<h2>Distribution functions in R</h2>
<p>R provides four functions to work with the normal distribution. In each case, the mean (<code>mean</code>) and standard deviation (<code>sd</code>) of the distribution must be specified.</p>
<ul>
<li><p><code>rnorm(n, mean, sd)</code> generates <code>n</code> random values from a normal distribution with average<code>mean</code> and standard deviation <code>sd</code>.</p></li>
<li><p><code>dnorm(x, mean, sd)</code> gives the probability density associated with a value <code>x</code>.</p></li>
<li><p><code>pnorm(q, mean, sd)</code> gives the cumulative probability associated with a value <code>q</code>.</p></li>
<li><p><code>qnorm(p, mean, sd)</code> gives the value (<code>q</code> for quantile) associated with a given cumulative probability <code>p</code>.</p></li>
</ul>
<p>Similar functions are defined for other frequently used distributions, as we will see later.</p>
<p>For example, for the standard normal distribution:</p>
<ul>
<li>the cumulative probability at 2 standard deviations above the mean is 98%;</li>
</ul>
<pre class="r"><code>pnorm(2, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] 0.9772499</code></pre>
<ul>
<li>the probability of being within one standard deviation on either side of the mean is 68%;</li>
</ul>
<pre class="r"><code>pnorm(1, mean = 0, sd = 1) - pnorm(-1, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] 0.6826895</code></pre>
<ul>
<li>the third quartile (75% cumulative probability) is at 0.67 standard deviation above the mean.</li>
</ul>
<pre class="r"><code>qnorm(0.75, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] 0.6744898</code></pre>
</div>
<div id="quantile-quantile-plot" class="section level2">
<h2>Quantile-quantile plot</h2>
<p>The quantile-quantile plot (or Q-Q plot) is used to visualize the correspondence between two statistical distributions; most often, we want to compare a sample to a given theoretical distribution.</p>
<p>For example, suppose we have 99 observations of a variable and we want to check that its distribution is approximately normal. We sort the observations in ascending order and associate the first observation with the 1st percentile of the standard normal distribution, the second observation with the 2nd percentile, and so on until the 99th percentile. If the sample comes from a normal distribution, the scatter plot produced by this association will form a straight line.</p>
<p><em>Indeed, if <span class="math inline">\(x\)</span> follows a normal distribution, then <span class="math inline">\(x = \mu + \sigma z\)</span> where <span class="math inline">\(z\)</span> is a standard normal variable.</em></p>
<p>In R, we can compare a vector to the normal distribution with the <code>qqnorm</code> function and add a straight line to the graph with the<code>qqline</code> function.</p>
<pre class="r"><code>test &lt;- rnorm(99, mean = 6, sd = 4)

qqnorm(test)
qqline(test)</code></pre>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>As you can see, for a random sample drawn from a normal distribution, the correspondence is very good; however, there is some variation due to the sampling of a limited number of points.</p>
<p>Now, let’s look at the Q-Q plot of tree diameter in the Kejimkujik dataset, as seen in the last class.</p>
<pre class="r"><code>kejim &lt;- read.csv(&quot;../donnees/cours1_kejimkujik.csv&quot;)

dhp &lt;- kejim$DHP

qqnorm(dhp)
qqline(dhp)</code></pre>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The distribution is clearly not normal. More specifically, we find that:</p>
<ul>
<li><p>For the values below the average (on the left), the points are above the line, so the quantiles of the sample are higher than those of a normal distribution. Being higher, they are closer to the mean.</p></li>
<li><p>For values above the mean (on the right), the quantiles of the sample are also higher than those of the normal distribution. But in this case, they would then be farther from the average.</p></li>
</ul>
<p>Thus, the Q-Q plot indicates that the distribution is asymmetrical with quantiles closer together on the left and further away on the right. Since this is a fairly obvious difference, it could be detected more easily with a histogram (below). However, the Q-Q plot can detect more subtle differences, so it is useful to learn how to read and interpret this graph.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
</div>
<div id="the-log-normal-distribution" class="section level1">
<h1>The log-normal distribution</h1>
<div id="definition" class="section level2">
<h2>Definition</h2>
<p>A variable <span class="math inline">\(x\)</span> follows a log-normal distribution if <span class="math inline">\(y = log(x)\)</span> follows a normal distribution.</p>
<p>Equivalently, if <span class="math inline">\(y\)</span> follows a normal distribution, <span class="math inline">\(x = e^y\)</span> follows a log-normal distribution.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="properties-of-logarithms" class="section level2">
<h2>Properties of logarithms</h2>
<ul>
<li><p><span class="math inline">\(log(x)\)</span> is only defined for <span class="math inline">\(x &gt; 0\)</span>;</p></li>
<li><p><span class="math inline">\(log(x) = 0\)</span> if <span class="math inline">\(x = 1\)</span>. Negative and positive logarithms represent values under and over 1, respectively.</p></li>
<li><p>The logarithm transforms products into sums, and ratios into differences.</p></li>
</ul>
<p><span class="math display">\[log(xw) = log(x) + log(w)\]</span> <span class="math display">\[log(x/v) = log(x) - log(v)\]</span></p>
<ul>
<li><p>Therefore, on a logarithmic scale, the distance between two numbers is proportional to their ratio in the original scale.</p></li>
<li><p>Unless we specify otherwise, the logarithms are natural logarithms (base <span class="math inline">\(e\)</span>). However, a base change only causes a change of scale and does not affect the shape of the distribution. For example, to convert to base 10:</p></li>
</ul>
<p><span class="math display">\[log_{10}(x) = \frac{log(x)}{log(10)}\]</span></p>
</div>
<div id="use-of-the-log-normal-distribution" class="section level2">
<h2>Use of the log-normal distribution</h2>
<p>If the normal distribution tends to be associated with additive processes (sum of many independent effects), the log-normal distribution is associated with multiplicative processes. For example, if a population increases by 5%, 10% and 3% in three consecutive years, the cumulative increase corresponds to the multiplication: 1.05 x 1.10 x 1.03 = 1.19, or a 19% increase. In a multiplicative process, the larger a variable, the more it can grow, which explains why the resulting distribution is asymmetric and stretched to the right.</p>
<p>Remember that the DBH distribution of all trees in the Kejimkujik dataset had this type of asymmetry. It is also plausible that the diameter of a tree is log-normally distributed, considering that larger trees can capture more resources to grow.</p>
<p>To check if the distribution of DBH is approximately log-normal, see the Q-Q plot for the logarithm of DBH.</p>
<pre class="r"><code>qqnorm(log(dhp))
qqline(log(dhp))</code></pre>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>A better match is observed than for the untransformed data, except for the smallest DBH values in the sample, which are still higher than predicted by the reference distribution. Do you have a hypothesis for this anomaly near the minimum? (Hint: Which trees are not sampled?)</p>
</div>
<div id="logarithmic-transformation" class="section level2">
<h2>Logarithmic transformation</h2>
<p>Since most of the methods we will see are based on the normal distribution, the easiest way to process a log-normal variable is to apply a logarithmic transformation to it and work with the normal variable thus obtained. However, you must be cautious when interpreting the results. In particular, the mean of <span class="math inline">\(log(x)\)</span> is <em>not</em> equal to the logarithm of the mean of <span class="math inline">\(x\)</span>, as this graph shows.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>In the graph above, the two distributions of <span class="math inline">\(y = log(x)\)</span> have the same mode (peak of the distribution), the same median and the same mean at 0. However, on the original scale, the mean of <span class="math inline">\(x\)</span> is higher for the blue distribution, while its mode is smaller; both distributions have the same median value (equal to 1).</p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>A discrete distribution is represented by a probability mass function; a continuous distribution is represented by a probability density function.</p></li>
<li><p>The cumulative distribution of a variable at a point <span class="math inline">\(x\)</span> gives the probability that this variable is less than or equal to <span class="math inline">\(x\)</span>.</p></li>
<li><p>Examples of continuous distributions: uniform, normal, log-normal. (We will see more examples of discrete distributions later in the session.)</p></li>
<li><p>The normal distribution is characterized by its average <span class="math inline">\(\mu\)</span> and its standard deviation <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>Any normal distribution can be reduced to the standard normal distribution (<span class="math inline">\(\mu\)</span> = 0, <span class="math inline">\(\sigma\)</span> = 1) with the linear transformation: <span class="math inline">\(z = (x - \mu) / \sigma\)</span>.</p></li>
<li><p>A logarithmic transformation converts multiplicative effects into additive effects, and log-normal distributions into normal distributions.</p></li>
<li><p>The quantile-quantile plot provides a visual way to compare data to a reference distribution.</p></li>
</ul>
</div>
<div id="parameter-estimation" class="section level1">
<h1>Parameter estimation</h1>
<div id="estimation-of-the-mean" class="section level2">
<h2>Estimation of the mean</h2>
<p>Suppose we measure a variable <span class="math inline">\(x\)</span> on a sample of <span class="math inline">\(n\)</span> randomly selected individuals in a population. (We will discuss sampling strategies further in the next class.) We use the sample mean:</p>
<p><span class="math display">\[\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i\]</span> as the estimator of <span class="math inline">\(\mu\)</span>, the mean of the population distribution of <span class="math inline">\(x\)</span>. For now, we do not assume that <span class="math inline">\(x\)</span> follows a normal distribution.</p>
<p>For this example, imagine that the 1161 trees in the Kejimkujik dataset represent the entire population, and that we sample some of those trees.</p>
<pre class="r"><code># dhp is the vector of DBH for 1161 trees
paste(&quot;The population has a mean DBH of&quot;, round(mean(dhp), 2), &quot;cm with a standard deviation of&quot;, round(sd(dhp), 2), &quot;cm.&quot;)</code></pre>
<pre><code>## [1] &quot;The population has a mean DBH of 21.76 cm with a standard deviation of 12.25 cm.&quot;</code></pre>
<p>In R, the <code>sample</code> function draws a random sample from the elements of a vector.</p>
<pre class="r"><code>mean(sample(dhp, 20)) # mean DBH for a sample of n = 20 trees</code></pre>
<pre><code>## [1] 21.9375</code></pre>
<p>The histograms below show the distributions (out of 10,000 replicates) of the mean DBH with a sample size <span class="math inline">\(n\)</span> = 10, 20 or 40.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>As the sample size increases, the distribution becomes less dispersed, but also more symmetrical. This second observation is a consequence of the central limit theorem. Since the mean is calculated from the sum of the sampled values, its distribution approaches the normal distribution for a sufficiently large <span class="math inline">\(n\)</span>.</p>
<p>For a variable <span class="math inline">\(x\)</span> that is distributed with a mean <span class="math inline">\(\mu\)</span> and a variance <span class="math inline">\(\sigma^2\)</span>, we can prove that <span class="math inline">\(\bar{x}\)</span> has a mean equal to <span class="math inline">\(\mu\)</span> and a variance equal to <span class="math inline">\(\sigma^2 / n\)</span>. The standard deviation of <span class="math inline">\(\bar{x}\)</span>, which in this context is called the <em>standard error</em>, is therefore inversely proportional to the square root of <span class="math inline">\(n\)</span>.</p>
<p>Standard error of the mean: <span class="math display">\[\sigma_{\bar{x}} = \frac{\sigma_{x}}{\sqrt{n}}\]</span></p>
<p>The mean and standard error of <span class="math inline">\(\bar{x}\)</span> calculated from the 10,000 samples simulated above are consistent with the theoretical predictions.</p>
<table>
<thead>
<tr class="header">
<th align="right">n</th>
<th align="right">Mean (cm)</th>
<th align="right">Standard error (cm)</th>
<th align="right"><span class="math inline">\(\sigma / \sqrt{n}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">10</td>
<td align="right">21.76</td>
<td align="right">3.85</td>
<td align="right">3.87</td>
</tr>
<tr class="even">
<td align="right">20</td>
<td align="right">21.81</td>
<td align="right">2.75</td>
<td align="right">2.74</td>
</tr>
<tr class="odd">
<td align="right">40</td>
<td align="right">21.78</td>
<td align="right">1.91</td>
<td align="right">1.94</td>
</tr>
</tbody>
</table>
<p>Since the mean of the estimator is equal to the value of the estimated parameter (<span class="math inline">\(\mu\)</span>), <span class="math inline">\(\bar{x}\)</span> is an <em>unbiased</em> estimator of <span class="math inline">\(\mu\)</span>.</p>
</div>
<div id="standard-deviation-or-standard-error" class="section level2">
<h2>Standard deviation or standard error</h2>
<p>It is important not to confuse the standard deviation of <span class="math inline">\(x\)</span> with the standard error of an estimator, such as <span class="math inline">\(\bar{x}\)</span>. The standard deviation of <span class="math inline">\(x\)</span> measures the dispersion of the individual values of the variable relative to their mean. The standard error of <span class="math inline">\(\bar{x}\)</span> measures the dispersion of the sample mean relative to the population mean. The standard error decreases with the size of the sample.</p>
<p>Since the standard error decreases according to <span class="math inline">\(\sqrt{n}\)</span> rather than <span class="math inline">\(n\)</span>, if we want to reduce this standard error by half, we must increase the sample size by a factor of 4.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Note also that the standard error depends only on the size of the sample, not on the population size. This is true as long as the sample is small relative to the population. When sampling a significant fraction of the population (say more than 5%), the actual standard error is smaller than <span class="math inline">\(\sigma / \sqrt{n}\)</span>.</p>
</div>
<div id="estimation-of-the-variance" class="section level2">
<h2>Estimation of the variance</h2>
<p>To estimate the variance <span class="math inline">\(\sigma^2\)</span> of a variable <span class="math inline">\(x\)</span>, one could calculate the variance of the sample with the equation seen at the last class.</p>
<p><span class="math display">\[s^2 = \frac{1}{n} \sum_{i = 1}^n \left( x_i - \bar{x} \right)^2  \]</span></p>
<p>Here, we <span class="math inline">\(s^2\)</span> for the variance of a sample to differentiate from the population parameter <span class="math inline">\(\sigma^2\)</span>.</p>
<p>As before, we test this estimator by simulating 10,000 samples from the DBH vector with <span class="math inline">\(n\)</span> = 10, 20, and 40. The following table shows the average of <span class="math inline">\(s^2\)</span> and its ratio to the popultion parameter <span class="math inline">\(\sigma^2\)</span> (150.1 cm<span class="math inline">\(^2\)</span>).</p>
<table>
<thead>
<tr class="header">
<th align="right">n</th>
<th align="right">Mean of <span class="math inline">\(s^2\)</span> (cm<span class="math inline">\(^2\)</span>)</th>
<th align="right">Mean of <span class="math inline">\(s^2\)</span> / <span class="math inline">\(\sigma^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">10</td>
<td align="right">136.3</td>
<td align="right">0.90</td>
</tr>
<tr class="even">
<td align="right">20</td>
<td align="right">143.1</td>
<td align="right">0.95</td>
</tr>
<tr class="odd">
<td align="right">40</td>
<td align="right">146.6</td>
<td align="right">0.97</td>
</tr>
</tbody>
</table>
<p>This result shows that the calculated variance of the sample systematically underestimates the variance of the population. It is therefore a <em>biased</em> estimator. Why is this the case?</p>
<p>The problem is that the estimator <span class="math inline">\(s^2\)</span> is not based on the population mean, but on its estimate <span class="math inline">\(\bar{x}\)</span> calculated from the same sample. By definition, the sample is always centered on <span class="math inline">\(\bar{x}\)</span>, but <span class="math inline">\(\bar{x}\)</span> is at some distance from <span class="math inline">\(\mu\)</span>. Therefore, the deviations from <span class="math inline">\(\mu\)</span> are slightly larger than the deviations from <span class="math inline">\(\bar{x}\)</span>.</p>
<p>In fact, the estimator defined above underestimates the variance of the population by a ratio <span class="math inline">\((n-1)/n\)</span>, as shown in the last column of the table (0.9 = 9/10, 0.95 = 19/20). In that case, the bias can be corrected by multiplying the estimator by <span class="math inline">\(n / (n-1)\)</span>, giving the unbiased estimator:</p>
<p><span class="math display">\[s^2 = \frac{1}{n - 1} \sum_{i = 1}^n \left( x_i - \bar{x} \right)^2\]</span></p>
<p>Its square root provides an estimator for the population standard deviation:</p>
<p><span class="math display">\[s = \sqrt{\frac{1}{n - 1} \sum_{i = 1}^n \left( x_i - \bar{x} \right)^2}\]</span></p>
<p>Unlike <span class="math inline">\(s^2\)</span>, the <span class="math inline">\(s\)</span> estimator for the standard deviation is biased, but it remains the most commonly used one, since there is no simple and unbiased formula for standard deviation.</p>
<p>Finally, we also use <span class="math inline">\(s\)</span> as the <span class="math inline">\(\sigma\)</span> estimator for calculating the standard error of <span class="math inline">\(\bar{x}\)</span> (<span class="math inline">\(s / \sqrt{n}\)</span>).</p>
</div>
<div id="bias-and-standard-error-of-an-estimator" class="section level2">
<h2>Bias and standard error of an estimator</h2>
<p>The notions of bias and standard error were briefly presented in the previous section.</p>
<p>More generally, if we estimate a parameter <span class="math inline">\(\theta\)</span> (e.g. <span class="math inline">\(\mu\)</span>) with an estimator <span class="math inline">\(\hat{\theta}\)</span> (e.g. <span class="math inline">\(\bar{x}\)</span>), we can divide the <em>mean square error</em> between <span class="math inline">\(\hat {\theta}\)</span> and <span class="math inline">\(\theta\)</span> into two components. (In the equation below, the function <span class="math inline">\(E[]\)</span> is another way of representing the mean.)</p>
<p><span class="math display">\[E[(\hat{\theta} - \theta)^2] = E[(\hat{\theta} - E[\hat{\theta}])^2] + (E[\hat{\theta}] - \theta)^2\]</span></p>
<p>This equation tells us that the mean square deviation between an estimator and the parameter is the sum of:</p>
<ul>
<li><p>the mean square deviation between the estimator and its mean (that is, the variance of the estimator, or the square of its standard error);</p></li>
<li><p>the square of the difference between the mean of the estimator and the parameter (this difference is the bias);</p></li>
</ul>
<p>So, we have the following relation: <em>Mean square error = (Standard error)<span class="math inline">\(^2\)</span> + (Bias)<span class="math inline">\(^2\)</span></em>.</p>
<p>These two sources of error have different properties. The standard error is due to the limited size of the sample and decreases as <span class="math inline">\(n\)</span> increases. Bias is a systematic error that does not depend on the size of the sample, but may be due to a biased estimator or unrepresentative sampling of the population.</p>
</div>
</div>
<div id="confidence-interval" class="section level1">
<h1>Confidence interval</h1>
<div id="estimator-with-a-normal-distribution" class="section level2">
<h2>Estimator with a normal distribution</h2>
<p>If a sample is drawn from a distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, we have seen that the sample mean <span class="math inline">\(\bar{x}\)</span> has a mean of <span class="math inline">\(\mu\)</span> and a standard deviation equal to <span class="math inline">\(\sigma / \sqrt{n}\)</span>.</p>
<p>Suppose that <span class="math inline">\(\bar{x}\)</span> follows a normal distribution. This is always the case when <span class="math inline">\(x\)</span> itself is normally distributed. But thanks to the central limit theorem, it is also a good approximation for other distributions of <span class="math inline">\(x\)</span>, as long as the sample is large enough.</p>
<p>In this case, the variable <span class="math inline">\(z\)</span> that we will define as:</p>
<p><span class="math display">\[ z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \]</span></p>
<p>follows a standard normal distribution. We can therefore use that theoretical distribution to determine the probability that <span class="math inline">\(\bar{x}\)</span> is found in a given interval.</p>
</div>
<div id="interval-with-a-given-probability" class="section level2">
<h2>Interval with a given probability</h2>
<p>Alternatively, we can determine the interval of <span class="math inline">\(\bar{x}\)</span> corresponding to a given probability around the mean.</p>
<p>For example, the interval between the first quartile (cumulative probability of 25%) and the third quartile (cumulative probability of 75%) corresponds to a probability of 50%. These quantiles can be determined in R with <code>qnorm</code>.</p>
<pre class="r"><code>c(qnorm(0.25), qnorm(0.75))</code></pre>
<pre><code>## [1] -0.6744898  0.6744898</code></pre>
<p><em>Note</em>: By default, <code>qnorm</code> uses the parameters <code>mean = 0</code> and <code>sd = 1</code>.</p>
<p>The interval is symmetrical around the mean (0) since the normal distribution is symmetrical and our chosen quantiles are equally distant from 50%.</p>
<p>Let’s convert this interval of <span class="math inline">\(z\)</span> to an interval of <span class="math inline">\(\bar{x}\)</span>:</p>
<p><span class="math display">\[ \left( -0.674 \le \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \le 0.674 \right)\]</span></p>
<p><span class="math display">\[ \left( - 0.674 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 0.674 \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>There is a 50% probability that the sample mean <span class="math inline">\(\bar{x}\)</span> is in a range of 0.674 standard errors on each side of the parameter <span class="math inline">\(\mu\)</span>.</p>
<p>Suppose we represent the value of <span class="math inline">\(z\)</span> corresponding to a cumulative probability <span class="math inline">\(p\)</span> as <span class="math inline">\(z_p\)</span>. For example, <span class="math inline">\(z_{0.25}\)</span> is the first quartile. Thus, we rewrite the interval above as:</p>
<p><span class="math display">\[ \left( z_{0.25} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{0.75} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>For a 90% probability interval, we would replace <span class="math inline">\(z_{0.25}\)</span> and <span class="math inline">\(z_{0.75}\)</span> with <span class="math inline">\(z_{0.05}\)</span> and <span class="math inline">\(z_{0.95}\)</span>. Indeed, a 90% interval excludes 10% of the distribution and since we want a centered interval, we exclude 5% of both ends of the distribution, as indicated by the red part of the distribution in the graph below.</p>
<p><img src="2E-Statistical_models_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>More generally, if we represent the probability outside the interval as <span class="math inline">\(\alpha\)</span>, an interval containing (100% - <span class="math inline">\(\alpha\)</span>) of the distribution of <span class="math inline">\(\bar{x}\)</span> is given by:</p>
<p><span class="math display">\[ \left( z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>For historical reasons, the 95% interval corresponding to <span class="math inline">\(\alpha\)</span> = 0.05 is used most often:</p>
<p><span class="math display">\[ \left( z_{0.025} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le  z_{0.975} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>By replacing the quantiles by their values, we obtain:</p>
<p><span class="math display">\[ \left(- 1.96 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 1.96 \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
</div>
<div id="confidence-interval-1" class="section level2">
<h2>Confidence interval</h2>
<p>To summarize, if we sample a variable <span class="math inline">\(x\)</span> and calculate its sample mean <span class="math inline">\(\bar{x}\)</span>, we can say, for example, that we have a 95% probability of getting an estimate <span class="math inline">\(\bar{x}\)</span> that is <span class="math inline">\(\pm\)</span> 1.96 standard errors around the parameter <span class="math inline">\(\mu\)</span>, which is unknown.</p>
<p><em>This always assumes our model is good, that is, the <span class="math inline">\(\bar{x}\)</span> statistic is well represented by a normal distribution.</em></p>
<p>So, after calculating <span class="math inline">\(\bar{x}\)</span> and calculating its standard error, we establish an interval of 1.96 standard errors on each side of <span class="math inline">\(\bar{x}\)</span>:</p>
<p><span class="math display">\[ \left(\bar{x} - 1.96 \frac{\sigma}{\sqrt{n}}, \bar{x} + 1.96 \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>According to our model, we can say that for 95% of the possible samples of <span class="math inline">\(x\)</span>, the interval thus calculated will contain the value of <span class="math inline">\(\mu\)</span>. This is a 95% <strong>confidence interval</strong> for <span class="math inline">\(\bar{x}\)</span>.</p>
</div>
<div id="interpretation-of-the-confidence-interval" class="section level2">
<h2>Interpretation of the confidence interval</h2>
<ul>
<li><p>The probability associated with a confidence interval is based on the variability of <span class="math inline">\(\bar{x}\)</span> from one sample to another. It constitutes a probability <em>a priori</em> (before having sampled).</p></li>
<li><p>The parameter <span class="math inline">\(\mu\)</span> is fixed. Once the estimated <span class="math inline">\(\bar{x}\)</span> is obtained for a given sample, the confidence interval either contains <span class="math inline">\(\mu\)</span> or does not contain it.</p></li>
<li><p>In particular, when we obtain a confidence interval for a given sample, it is incorrect to say that “the parameter <span class="math inline">\(\mu\)</span> has 95% probability of being within this interval”.</p></li>
</ul>
</div>
<div id="confidence-interval-of-a-mean" class="section level2">
<h2>Confidence interval of a mean</h2>
<p>We have seen that the (100% - <span class="math inline">\(\alpha\)</span>) confidence interval of the mean <span class="math inline">\(\bar{x}\)</span> is given by:</p>
<p><span class="math display">\[ \left( \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + z_{1 - \alpha/2} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>The only problem with this equation is that we do not know the parameter <span class="math inline">\(\sigma\)</span>. And if we replace <span class="math inline">\(\sigma\)</span> with its estimator <span class="math inline">\(s\)</span>, the probability associated with the interval becomes less than (100% - <span class="math inline">\(\alpha\)</span>). In practice, we need to widen the interval to take into account our imperfect knowledge of the standard deviation of the data.</p>
<p>The solution to this problem was discovered by William Gosset, who published it under the pseudonym Student. When using an estimate of the standard deviation, the confidence interval is no longer based on the standard normal distribution <span class="math inline">\(z\)</span>, but on the Student <span class="math inline">\(t\)</span> distribution.</p>
<p>The <span class="math inline">\(t\)</span> distribution has a parameter, the number of degrees of freedom, which in this case corresponds to <span class="math inline">\(n\)</span> - 1. Thus, the corrected version of the (100% - <span class="math inline">\(\alpha\)</span>) confidence interval for <span class="math inline">\(\bar{x}\)</span> is:</p>
<p><span class="math display">\[ \left( \bar{x} + t_{(n-1)\alpha/2} \frac{s}{\sqrt{n}}, \bar{x} + t_{(n-1)1 - \alpha/2} \frac{s}{\sqrt{n}} \right)\]</span></p>
<p>where the <span class="math inline">\(n-1\)</span> in parentheses indicates the number of degrees of freedom of the <span class="math inline">\(t\)</span> distribution. The smaller <span class="math inline">\(n\)</span> is, the larger the difference between the <span class="math inline">\(t\)</span> distribution and the standard normal distribution <span class="math inline">\(z\)</span>.</p>
</div>
</div>
<div id="summary-1" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>An estimator is biased when its average over all possible samples differs from the value of the parameter to be estimated.</p></li>
<li><p>The standard error measures the dispersion of an estimator from one sample to another, and decreases with the size of the sample.</p></li>
<li><p>A confidence interval is defined around an estimate so that across all the possible samples, there is a specific probability that the confidence interval obtained contains the value of the parameter to be estimated.</p></li>
</ul>
</div>
<div id="reference" class="section level1">
<h1>Reference</h1>
<p>The website <a href="https://students.brown.edu/seeing-theory/">Seeing Theory</a> presents several statistical concepts in a visual and interactive way. For example, chapters 3 (<em>Probability Distributions</em>) and 4 (<em>Frequentist Inference</em>) relate to the concepts seen in this class.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
