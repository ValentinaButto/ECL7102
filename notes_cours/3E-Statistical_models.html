<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Statistical models and confidence intervals</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical models and confidence intervals</h1>
<h4 class="date"><br/>Sept. 14, 2020</h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li><p>Describe the characteristics and use of normal and log-normal distributions.</p></li>
<li><p>Know the relationship between probability density and cumulative probability for a continuous variable, and calculate these quantities in R.</p></li>
<li><p>Compare data to a reference distribution with a quantile-quantile plot.</p></li>
<li><p>Interpret a confidence interval and calculate the confidence interval for the mean of a normal distribution.</p></li>
</ul>
</div>
<div id="probability-distributions" class="section level1">
<h1>Probability distributions</h1>
<p>A distribution is a function that associates a probability with each possible value of a random variable.</p>
<div id="discrete-distribution" class="section level2">
<h2>Discrete distribution</h2>
<p>When the variable is discrete, each value has a probability mass, the sum of which must be equal to 1. For example, if <span class="math inline">\(x\)</span> is the number obtained by rolling a balanced six-sided die, the probability of <span class="math inline">\(x\)</span> is 1/6 for each of the numbers from 1 to 6. Since the probability is the same for each value, this would be a <strong>uniform distribution</strong>.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="continuous-distribution" class="section level2">
<h2>Continuous distribution</h2>
<p>When the variable is continuous, the number of possible values is infinite, so the probability of precisely obtaining a given value is zero. The distribution function therefore associates a <em>density</em> of probability with a given value.</p>
<p>For example, here is a uniform probability distribution between 0 and 6. The probability density is constant (1/6) in the interval and zero outside.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>For a continuous distribution, the probability that the variable is within a given interval is the integral (the area under the curve) of the probability density in that interval.</p>
<p>Here, the probability density is rectangular, so it is easy to calculate the probability of an interval. For example, the probability of getting a value between 2.5 and 3 is 1/2 (width of the interval) x 1/6 (probability density) = 1/12 (~ 0.083). This value corresponds to the area of the filled rectangle in the graph below.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The integral of the probability density over all possible values of <span class="math inline">\(x\)</span> (total probability) must be equal to 1.</p>
</div>
<div id="law-of-large-numbers" class="section level2">
<h2>Law of large numbers</h2>
<p>In R, the following command generates ten random values (<code>n = 10</code>) drawn from the (continuous) uniform distribution between 0 and 6, which we saw in the preceding section.</p>
<pre class="r"><code>x &lt;- runif(n = 10, min = 0, max = 6)
round(x, 2) # round shows values with 2 decimals</code></pre>
<pre><code>##  [1] 5.02 5.02 3.36 5.26 4.66 4.25 3.09 0.92 5.38 0.42</code></pre>
<p>The histograms below show the distribution of values within samples with different <span class="math inline">\(n\)</span>. What do you notice?</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The <code>runif</code> function simulates sampling from a variable that follows a uniform distribution. The greater the size of that random sample, the more the distribution of values in the sample approaches the population distribution. This is what we call the <strong>law of large numbers</strong>.</p>
</div>
</div>
<div id="the-normal-distribution" class="section level1">
<h1>The normal distribution</h1>
<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>In a complex system, the variables we observe result from the combined effect of many processes that we can not directly perceive. For example, the height of a person is influenced by a large number of genetic and environmental factors, the yield of a field depends on the weather for each day of the growing season as well as the micro-habitat perceived by each plant, etc. Modeling each of these processes is usually not possible. Fortunately, when many factors act independently on the same variable, their total effect tends to converge towards some well-known statistical distributions. We will see here a simple example of this phenomenon.</p>
<p>Suppose we are interested in a random variable that is itself the sum of <span class="math inline">\(n\)</span> independent variables, and that each of these variables follows the uniform distribution between 0 and 6 presented above. Even if we do not know the distribution of this sum, the law of large numbers allows us to approximate it from simulations. So we create a function that generates <span class="math inline">\(n\)</span> values of the uniform distribution and calculates their sum, then we generate 10,000 values of that sum (with <code>replicate</code>) for a given value of <span class="math inline">\(n\)</span>.</p>
<pre class="r"><code># Sum of n random variables with a uniform distribution between min and max
sum_unif &lt;- function(n, min, max) {
    sum(runif(n, min, max))
}

n &lt;- 10
x &lt;- replicate(10000, sum_unif(n, 0, 6))</code></pre>
<p>Here is a histogram of the values of the sum for different values of <span class="math inline">\(n\)</span>. What do you notice?</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The sum of 2 values has a triangular distribution, but starting from <span class="math inline">\(n\)</span> = 5, we see the bell shape of a normal distribution. This is a specific case of a more general statistical law, the central limit theorem.</p>
</div>
<div id="central-limit-theorem" class="section level2">
<h2>Central limit theorem</h2>
<p>The central limit theorem stipulates that when we sum a large number of independent random variables, regardless of the distribution of individual variables, the distribution of their sum approximates a normal distribution.</p>
<p><em>To be strict, some technical conditions should be included about the variables that are summed, but the simplified definition above is sufficient for this course.</em></p>
<p>This property of the normal distribution partly explains why it constitutes such an important model in statistics. As we mentioned earlier, a statistical distribution is often used to represent the unexplained variation of a variable due to a large number of unobserved processes in a complex system. If we suppose that this variation is due to several small effects that are independent and additive, then it is natural that the result approaches a normal distribution. However, it is important to check this assumption for a given variable.</p>
</div>
<div id="normal-distribution" class="section level2">
<h2>Normal distribution</h2>
<p>If a variable <span class="math inline">\(x\)</span> follows a normal (also called Gaussian) distribution, its probability density is given by:</p>
<p><span class="math display">\[f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2}\]</span></p>
<p>This distribution has two <em>parameters</em>, <span class="math inline">\(\mu\)</span> (which is the mean of <span class="math inline">\(x\)</span>) and <span class="math inline">\(\sigma\)</span> (which is its standard deviation).</p>
<p>On a graph of <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(\mu\)</span> is the position of the center of the distribution, while <span class="math inline">\(\sigma\)</span> is its dispersion; as <span class="math inline">\(\sigma\)</span> increases, the distribution widens and becomes less concentrated around its mean.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-8-1.png" width="100%" /></p>
</div>
<div id="standard-normal-distribution" class="section level2">
<h2>Standard normal distribution</h2>
<p>If a variable <span class="math inline">\(x\)</span> follows a normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, we can obtain a standardized version of <span class="math inline">\(x\)</span> (denoted <span class="math inline">\(z\)</span>) by subtracting <span class="math inline">\(\mu\)</span>, then dividing by <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[z = \frac{x - \mu}{\sigma}\]</span></p>
<blockquote>
<p>In R, the <code>scale(x)</code> function applied to a vector <code>x</code> standardizes it (by subtracting the mean of <code>x</code> and dividing by the standard deviation).</p>
</blockquote>
<p>The variable <span class="math inline">\(z\)</span> then follows a standard normal distribution, that is, with <span class="math inline">\(\mu\)</span> = 0 and <span class="math inline">\(\sigma\)</span> = 1:</p>
<p><span class="math display">\[f(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} z^2}\]</span></p>
<p>In other words, any normal distribution can be obtained from <span class="math inline">\(f(z)\)</span> by moving the center by a distance <span class="math inline">\(\mu\)</span> and widening the distribution by a factor <span class="math inline">\(\sigma\)</span>.</p>
<p>The values of <span class="math inline">\(z\)</span> represent the distance from the mean, expressed in standard deviation units, eg: <span class="math inline">\(z\)</span> = -1.5 means one standard deviation and half below the mean.</p>
</div>
</div>
<div id="cumulative-distribution" class="section level1">
<h1>Cumulative distribution</h1>
<p>We have previously seen that the probability that a continuous random variable is found in a certain interval corresponds to the area under the curve (the integral) of the probability density in this interval.</p>
<p>The cumulative distribution of a random variable is, for each value <span class="math inline">\(x\)</span>, the probability that the value of the variable is less than or equal to <span class="math inline">\(x\)</span>. It is therefore equal to the area under the curve of the probability density on the left of <span class="math inline">\(x\)</span>.</p>
<p>Here is an illustration of the cumulative distribution <span class="math inline">\(F(z)\)</span> of a standard normal variable <span class="math inline">\(z\)</span>.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>From the cumulative distribution <span class="math inline">\(F(x)\)</span>, we can easily calculate the probability in an interval (<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>) by subtraction, i.e. <span class="math inline">\(F(x_2)\)</span> - <span class="math inline">\(F(x_1)\)</span>.</p>
</div>
<div id="distribution-functions-in-r" class="section level1">
<h1>Distribution functions in R</h1>
<p>R provides four functions to work with the normal distribution. In each case, the mean (<code>mean</code>) and standard deviation (<code>sd</code>) of the distribution must be specified.</p>
<ul>
<li><p><code>rnorm(n, mean, sd)</code> generates <code>n</code> random values from a normal distribution with a given <code>mean</code> and standard deviation (<code>sd</code>).</p></li>
<li><p><code>dnorm(x, mean, sd)</code> gives the probability density associated with a value <code>x</code>.</p></li>
<li><p><code>pnorm(q, mean, sd)</code> gives the cumulative probability associated with a value <code>q</code>.</p></li>
<li><p><code>qnorm(p, mean, sd)</code> gives the value (<code>q</code> for quantile) associated with a given cumulative probability <code>p</code>.</p></li>
</ul>
<p>Similar functions are defined for other frequently used distributions, as we will see later.</p>
<p>For example, for the standard normal distribution:</p>
<ul>
<li>the cumulative probability at 2 standard deviations above the mean is 98%;</li>
</ul>
<pre class="r"><code>pnorm(2, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] 0.9772499</code></pre>
<ul>
<li>the probability of being within one standard deviation on either side of the mean is 68%;</li>
</ul>
<pre class="r"><code>pnorm(1, mean = 0, sd = 1) - pnorm(-1, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] 0.6826895</code></pre>
<ul>
<li>the third quartile (75% cumulative probability) is at 0.67 standard deviation above the mean.</li>
</ul>
<pre class="r"><code>qnorm(0.75, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] 0.6744898</code></pre>
</div>
<div id="quantile-quantile-plot" class="section level1">
<h1>Quantile-quantile plot</h1>
<p>The quantile-quantile plot (or Q-Q plot) is used to visualize the correspondence between two statistical distributions; most often, we want to compare a sample to a given theoretical distribution.</p>
<p>For example, suppose we have 99 observations of a variable and we want to check that its distribution is approximately normal. We sort the observations in ascending order and associate the first observation with the 1st percentile of the standard normal distribution, the second observation with the 2nd percentile, and so on until the 99th percentile. If the sample comes from a normal distribution, the scatter plot produced by this association will form a straight line.</p>
<p><em>Indeed, if <span class="math inline">\(x\)</span> follows a normal distribution, then <span class="math inline">\(x = \mu + \sigma z\)</span> where <span class="math inline">\(z\)</span> is a standard normal variable.</em></p>
<p>In R, we can compare a vector to the normal distribution with the <code>qqnorm</code> function and add a straight line to the graph with the<code>qqline</code> function.</p>
<pre class="r"><code>test &lt;- rnorm(99, mean = 6, sd = 4)

qqnorm(test)
qqline(test)</code></pre>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>As you can see, for a random sample drawn from a normal distribution, the correspondence is very good; however, there is some variation due to the sampling of a limited number of points.</p>
<p>Now, let’s look at the Q-Q plot of tree diameter in the Kejimkujik dataset, as seen in the last class.</p>
<pre class="r"><code>kejim &lt;- read.csv(&quot;../donnees/cours1_kejimkujik.csv&quot;)

dhp &lt;- kejim$dhp

qqnorm(dhp)
qqline(dhp)</code></pre>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>The distribution is clearly not normal. More specifically, we find that:</p>
<ul>
<li><p>For the values below the mean (on the left), the points are above the line, so the quantiles of the sample are higher than those of a normal distribution. Being higher, they are closer to the mean.</p></li>
<li><p>For values above the mean (on the right), the quantiles of the sample are also higher than those of the normal distribution. But in this case, they would then be farther from the mean.</p></li>
</ul>
<p>Thus, the Q-Q plot indicates that the distribution is asymmetrical with quantiles closer together on the left and further away on the right. Since this is a fairly obvious difference, it could be detected more easily with a histogram (below). However, the Q-Q plot can detect more subtle differences, so it is useful to learn how to read and interpret this graph.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<div id="exercise" class="section level2">
<h2>Exercise</h2>
<p>Here is a quantile-quantile plot comparing a sample to a normal distribution. Describe how this sample differs from the theoretical distribution.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
</div>
<div id="the-log-normal-distribution" class="section level1">
<h1>The log-normal distribution</h1>
<div id="definition" class="section level2">
<h2>Definition</h2>
<p>A variable <span class="math inline">\(x\)</span> follows a log-normal distribution if <span class="math inline">\(y = log(x)\)</span> follows a normal distribution.</p>
<p>Equivalently, if <span class="math inline">\(y\)</span> follows a normal distribution, <span class="math inline">\(x = e^y\)</span> follows a log-normal distribution.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="properties-of-logarithms" class="section level2">
<h2>Properties of logarithms</h2>
<ul>
<li><p><span class="math inline">\(log(x)\)</span> is only defined for <span class="math inline">\(x &gt; 0\)</span>;</p></li>
<li><p><span class="math inline">\(log(x) = 0\)</span> if <span class="math inline">\(x = 1\)</span>. Negative and positive logarithms represent values under and over 1, respectively.</p></li>
<li><p>The logarithm transforms products into sums, and ratios into differences.</p></li>
</ul>
<p><span class="math display">\[log(xw) = log(x) + log(w)\]</span> <span class="math display">\[log(x/v) = log(x) - log(v)\]</span></p>
<ul>
<li><p>Therefore, on a logarithmic scale, the distance between two numbers is proportional to their ratio in the original scale.</p></li>
<li><p>Unless we specify otherwise, the logarithms are natural logarithms (base <span class="math inline">\(e\)</span>). However, a base change only causes a change of scale and does not affect the shape of the distribution. For example, to convert to base 10:</p></li>
</ul>
<p><span class="math display">\[log_{10}(x) = \frac{log(x)}{log(10)}\]</span></p>
</div>
<div id="use-of-the-log-normal-distribution" class="section level2">
<h2>Use of the log-normal distribution</h2>
<p>If the normal distribution tends to be associated with additive processes (sum of many independent effects), the log-normal distribution is associated with multiplicative processes. For example, if a population increases by 5%, 10% and 3% in three consecutive years, the cumulative increase corresponds to the multiplication: 1.05 x 1.10 x 1.03 = 1.19, or a 19% increase. In a multiplicative process, the larger a variable, the more it can grow, which explains why the resulting distribution is asymmetric and stretched to the right.</p>
<p>Remember that the DBH distribution of all trees in the Kejimkujik dataset had this type of asymmetry. To check if the distribution of DBH is approximately log-normal, see the Q-Q plot for the logarithm of DBH.</p>
<pre class="r"><code>qqnorm(log(dhp))
qqline(log(dhp))</code></pre>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>A better match is observed than for the untransformed data, except for the smallest DBH values in the sample, which are still higher than predicted by the reference distribution. Do you have a hypothesis for this anomaly near the minimum? (Hint: Which trees are not sampled?)</p>
</div>
<div id="logarithmic-transformation" class="section level2">
<h2>Logarithmic transformation</h2>
<p>In the next few weeks, we will see several statistical methods that all assume that the observed variable is explained by additive effects, with a random component following a normal distribution.</p>
<p>Thus, if the process we are interested in is multiplicative and the measured variable approaches a log-normal distribution, we can model this variable after applying a logarithmic transformation. However, you must be cautious when interpreting the results. In particular, the mean of <span class="math inline">\(log(x)\)</span> is <em>not</em> equal to the logarithm of the mean of <span class="math inline">\(x\)</span>, as this graph shows.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>In the graph above, the two distributions of <span class="math inline">\(y = log(x)\)</span> have the same mode (peak of the distribution), the same median and the same mean at 0. However, on the original scale, the mean of <span class="math inline">\(x\)</span> is higher for the blue distribution, while its mode is smaller; both distributions have the same median value (equal to 1).</p>
</div>
<div id="exercise-1" class="section level2">
<h2>Exercise</h2>
<p>Among the following variables, which ones do you think would most likely follow a normal distribution, and why?</p>
<ol style="list-style-type: lower-alpha">
<li><p>The mean temperature in September (varying across years) in Rouyn-Noranda.</p></li>
<li><p>The population of cities and towns in Québec.</p></li>
<li><p>The number of followers of a social media account (e.g. Twitter).</p></li>
<li><p>Weekly bread sales at a supermarket.</p></li>
</ol>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>A discrete distribution is represented by a probability mass function; a continuous distribution is represented by a probability density function.</p></li>
<li><p>The cumulative distribution of a variable at a point <span class="math inline">\(x\)</span> gives the probability that this variable is less than or equal to <span class="math inline">\(x\)</span>.</p></li>
<li><p>Examples of continuous distributions: uniform, normal, log-normal. (We will see more examples of discrete distributions later in the session.)</p></li>
<li><p>The normal distribution is characterized by its mean <span class="math inline">\(\mu\)</span> and its standard deviation <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>Any normal distribution can be reduced to the standard normal distribution (<span class="math inline">\(\mu\)</span> = 0, <span class="math inline">\(\sigma\)</span> = 1) with the linear transformation: <span class="math inline">\(z = (x - \mu) / \sigma\)</span>.</p></li>
<li><p>A logarithmic transformation converts multiplicative effects into additive effects, and log-normal distributions into normal distributions.</p></li>
<li><p>The quantile-quantile plot provides a visual way to compare data to a reference distribution.</p></li>
</ul>
</div>
<div id="confidence-interval" class="section level1">
<h1>Confidence interval</h1>
<div id="estimator-with-a-normal-distribution" class="section level2">
<h2>Estimator with a normal distribution</h2>
<p>If a sample is drawn from a distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, we have seen that the sample mean <span class="math inline">\(\bar{x}\)</span> has a mean of <span class="math inline">\(\mu\)</span> and a standard deviation equal to <span class="math inline">\(\sigma / \sqrt{n}\)</span>.</p>
<p>Suppose that <span class="math inline">\(\bar{x}\)</span> follows a normal distribution. This is always the case when <span class="math inline">\(x\)</span> itself is normally distributed. But thanks to the central limit theorem, it is also a good approximation for other distributions of <span class="math inline">\(x\)</span>, as long as the sample is large enough.</p>
<p>In this case, the variable <span class="math inline">\(z\)</span> that we will define as:</p>
<p><span class="math display">\[ z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \]</span></p>
<p>follows a standard normal distribution. We can therefore use that theoretical distribution to determine the probability that <span class="math inline">\(\bar{x}\)</span> is found in a given interval.</p>
</div>
<div id="interval-with-a-given-probability" class="section level2">
<h2>Interval with a given probability</h2>
<p>Alternatively, we can determine the interval of <span class="math inline">\(\bar{x}\)</span> corresponding to a given probability around the mean.</p>
<p>For example, the interval between the first quartile (cumulative probability of 25%) and the third quartile (cumulative probability of 75%) corresponds to a probability of 50%. These quantiles can be determined in R with <code>qnorm</code>.</p>
<pre class="r"><code>c(qnorm(0.25), qnorm(0.75))</code></pre>
<pre><code>## [1] -0.6744898  0.6744898</code></pre>
<p><em>Note</em>: By default, <code>qnorm</code> uses the parameters <code>mean = 0</code> and <code>sd = 1</code>.</p>
<p>The interval is symmetrical around the mean (0) since the normal distribution is symmetrical and our chosen quantiles are equally distant from 50%.</p>
<p>Let’s convert this interval of <span class="math inline">\(z\)</span> to an interval of <span class="math inline">\(\bar{x}\)</span>:</p>
<p><span class="math display">\[\left( -0.674 \le \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \le 0.674 \right)\]</span></p>
<p><span class="math display">\[\left( - 0.674 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 0.674 \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>There is a 50% probability that the sample mean <span class="math inline">\(\bar{x}\)</span> is in a range of 0.674 standard errors on each side of the parameter <span class="math inline">\(\mu\)</span>.</p>
<p>Suppose we represent the value of <span class="math inline">\(z\)</span> corresponding to a cumulative probability <span class="math inline">\(p\)</span> as <span class="math inline">\(z_p\)</span>. For example, <span class="math inline">\(z_{0.25}\)</span> is the first quartile. Thus, we rewrite the interval above as:</p>
<p><span class="math display">\[\left( z_{0.25} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{0.75} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>For a 90% probability interval, we would replace <span class="math inline">\(z_{0.25}\)</span> and <span class="math inline">\(z_{0.75}\)</span> with <span class="math inline">\(z_{0.05}\)</span> and <span class="math inline">\(z_{0.95}\)</span>. Indeed, a 90% interval excludes 10% of the distribution and since we want a centered interval, we exclude 5% of both ends of the distribution, as indicated by the red part of the distribution in the graph below.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>More generally, if we represent the probability outside the interval as <span class="math inline">\(\alpha\)</span>, an interval containing (100% - <span class="math inline">\(\alpha\)</span>) of the distribution of <span class="math inline">\(\bar{x}\)</span> is given by:</p>
<p><span class="math display">\[\left( z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>For historical reasons, the 95% interval corresponding to <span class="math inline">\(\alpha\)</span> = 0.05 is used most often:</p>
<p><span class="math display">\[\left( z_{0.025} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le  z_{0.975} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>By replacing the quantiles by their values, we obtain:</p>
<p><span class="math display">\[\left(- 1.96 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 1.96 \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
</div>
<div id="confidence-interval-1" class="section level2">
<h2>Confidence interval</h2>
<p>To summarize, if we sample a variable <span class="math inline">\(x\)</span> and calculate its sample mean <span class="math inline">\(\bar{x}\)</span>, we can say, for example, that we have a 95% probability of getting an estimate <span class="math inline">\(\bar{x}\)</span> that is <span class="math inline">\(\pm\)</span> 1.96 standard errors around the parameter <span class="math inline">\(\mu\)</span>, which is unknown.</p>
<p><em>This always assumes our model is good, that is, the <span class="math inline">\(\bar{x}\)</span> statistic is well represented by a normal distribution.</em></p>
<p>So, after calculating <span class="math inline">\(\bar{x}\)</span> and calculating its standard error, we establish an interval of 1.96 standard errors on each side of <span class="math inline">\(\bar{x}\)</span>:</p>
<p><span class="math display">\[\left(\bar{x} - 1.96 \frac{\sigma}{\sqrt{n}}, \bar{x} + 1.96 \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>According to our model, we can say that for 95% of the possible samples of <span class="math inline">\(x\)</span>, the interval thus calculated will contain the value of <span class="math inline">\(\mu\)</span>. This is a 95% <strong>confidence interval</strong> for <span class="math inline">\(\bar{x}\)</span>.</p>
</div>
<div id="interpretation-of-the-confidence-interval" class="section level2">
<h2>Interpretation of the confidence interval</h2>
<ul>
<li><p>The probability associated with a confidence interval is based on the variability of <span class="math inline">\(\bar{x}\)</span> from one sample to another. It constitutes a probability <em>a priori</em> (before having sampled).</p></li>
<li><p>The parameter <span class="math inline">\(\mu\)</span> is fixed. Once the estimated <span class="math inline">\(\bar{x}\)</span> is obtained for a given sample, the confidence interval either contains <span class="math inline">\(\mu\)</span> or does not contain it.</p></li>
<li><p>Since a parameter is not a random variable, it does not have a statistical distribution. It is therefore inaccurate to say, after we obtain a confidence interval for a given sample, that “the parameter <span class="math inline">\(\mu\)</span> has 95% probability of being within this interval”.</p></li>
</ul>
</div>
<div id="confidence-interval-of-a-mean" class="section level2">
<h2>Confidence interval of a mean</h2>
<p>We have seen that the (100% - <span class="math inline">\(\alpha\)</span>) confidence interval of the mean <span class="math inline">\(\bar{x}\)</span> is given by:</p>
<p><span class="math display">\[\left( \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + z_{1 - \alpha/2} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>The only problem with this equation is that we do not know the parameter <span class="math inline">\(\sigma\)</span>. And if we replace <span class="math inline">\(\sigma\)</span> with its estimator <span class="math inline">\(s\)</span>, the probability associated with the interval becomes less than (100% - <span class="math inline">\(\alpha\)</span>). In practice, we need to widen the interval to take into account our imperfect knowledge of the standard deviation of the data.</p>
<p>The solution to this problem was discovered by William Gosset, who published it under the pseudonym Student. When using an estimate of the standard deviation, the confidence interval is no longer based on the standard normal distribution <span class="math inline">\(z\)</span>, but on the Student <span class="math inline">\(t\)</span> distribution.</p>
<p>The <span class="math inline">\(t\)</span> distribution has a parameter, the number of degrees of freedom, which in this case corresponds to <span class="math inline">\(n\)</span> - 1. Thus, the corrected version of the (100% - <span class="math inline">\(\alpha\)</span>) confidence interval for <span class="math inline">\(\bar{x}\)</span> is:</p>
<p><span class="math display">\[\left( \bar{x} + t_{(n-1)\alpha/2} \frac{s}{\sqrt{n}}, \bar{x} + t_{(n-1)1 - \alpha/2} \frac{s}{\sqrt{n}} \right)\]</span></p>
<p>where the <span class="math inline">\(n-1\)</span> in parentheses indicates the number of degrees of freedom of the <span class="math inline">\(t\)</span> distribution.</p>
</div>
<div id="the-t-distribution" class="section level2">
<h2>The t Distribution</h2>
<p>The graph below compares the standard normal distribution (<span class="math inline">\(z\)</span>) with <span class="math inline">\(t\)</span> distributions having 4 and 9 degrees of freedom. The smaller the number of degrees of freedom, the further the <span class="math inline">\(t\)</span> distribution is from the normal. In particular, the standard deviation increases and values far from the mean have a higher probability, which explains why the confidence interval constructed from the <span class="math inline">\(t\)</span> distribution is wider.</p>
<p><img src="3E-Statistical_models_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>While this is difficult to tell from the graph, the <span class="math inline">\(t\)</span> distribution also has a different shape. Even when compared with a normal distribution with the same standard deviation, the <span class="math inline">\(t\)</span> distribution puts a greater probability on extreme values (very far from the mean).</p>
<p>When the number of degrees of freedom is high, such as when we compute the mean of a large sample, the <span class="math inline">\(t\)</span> distribution approaches the standard normal distribution.</p>
</div>
</div>
<div id="summary-1" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>A confidence interval is defined around an estimate so that across all the possible samples, there is a specific probability that the confidence interval obtained contains the value of the parameter to be estimated.</p></li>
<li><p>Due to the central limit theorem, the difference between the sample mean and the population mean often follows an approximately normal distribution.</p></li>
<li><p>Student’s <span class="math inline">\(t\)</span> distribution replaces the standard normal distribution when estimating the confidence interval for the sample mean, when the population standard deviation is unknown. That distribution has more frequent extreme values, especially with a low number of degrees of freedom.</p></li>
</ul>
</div>
<div id="reference" class="section level1">
<h1>Reference</h1>
<p>The website <a href="https://students.brown.edu/seeing-theory/">Seeing Theory</a> presents several statistical concepts in a visual and interactive way. For example, chapters 3 (<em>Probability Distributions</em>) and 4 (<em>Frequentist Inference</em>) relate to the concepts seen in this class.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
