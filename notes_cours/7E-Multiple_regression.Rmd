---
title: "Multiple linear regression"
date: "<br/>October 14, 2020"
output:
    html_document:
        self_contained: false
        lib_dir: libs
        theme: spacelab
        toc: true
        toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cowplot)
theme_set(theme_cowplot())
```

# Objectives

- Estimate and interpret the parameters of a linear regression including several categorical and/or numerical variables.

- Explain the meaning of an interaction between two variables and interpret its coefficient.

- Use the *emmeans* package to compare the mean response between the different levels of a categorical variable.

- Know how and why to standardize predictors in multiple linear regression.


# Multiple linear regression

The multiple linear regression model represents the relationship between a response variable and $m$ predictors $x_1$, $x_2$, ..., $x_m$.

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_m x_m + \epsilon = \beta_0 + \sum_{i = 1}^m \beta_i x_i + \epsilon $$ 

As in the case of simple linear regression, the $\beta$ coefficients can be computed from the method of least squares. In this model, each $\beta_i$ coefficient (except $\beta_0$) is the partial derivative of $y$ with respect to a predictor $x_i$. In other words, this coefficient represents the mean difference in $y$ associated with a change of 1 unit in $x_i$ *if all other predictors are the same*.

A regression model can include multiple categorical or numerical predictors. In this class, we will present examples including:

- one categorical predictor and one numerical predictor (in an experimental context, this model is called an analysis of covariance or ANCOVA);

- two categorical predictors (two-way ANOVA);

- two numerical predictors.


# Analysis of covariance

The data frame [compensation.csv](../donnees/compensation.csv) is taken from Crawley's book, *Statistics: An introduction using R*. It contains data on seed mass produced by a plant species (*Fruit*) based on root size (*Root*) and whether or not the plant is grazed (*Grazing*).

```{r}
comp <- read.csv("../donnees/compensation.csv")
str(comp)
```

Let's first inspect the data.

```{r}
ggplot(comp, aes(x = Root, y = Fruit, color = Grazing)) +
    geom_point() +
    scale_color_brewer(palette = "Dark2")
```

The graph shows the existence of a linear relationship between root size and seed production, as well as the effect of treatment: for the same root size, grazing reduces seed production. Note that if we had not measured the roots, we might think that grazing has a positive effect.

```{r}
ggplot(comp, aes(x = Grazing, y = Fruit)) +
    geom_boxplot()
```

This is because the plants undergoing grazing had (on average) larger roots initially. Root size is therefore a *confounding* variable, that is, one that is correlated with both the response and with the treatment being studied. It must be included in the model to properly assess the effect of grazing.

Here is a linear model where the effect of the two predictors is additive:

```{r}
mod_comp <- lm(Fruit ~ Grazing + Root, data = comp)
summary(mod_comp)
```

## Results interpretation

If $x_1$ is the grazing variable (0 = Grazed, 1 = Ungrazed according to the default encoding in R) and $x_2$ is the root size, the mathematical expression of this model is:

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon $$

To simplify the interpretation of the coefficients, we separate the case with grazing ($x_1 = 0$):

$$ y = \beta_0 + \beta_2 x_2 + \epsilon $$

and the case without grazing ($x_1 = 1$):

$$ y = \beta_0 + \beta_1 + \beta_2 x_2 + \epsilon $$

The coefficients can now be interpreted as follows:

- $\beta_0$ (`Intercept` in the summary table) is the intercept of the *Fruit* vs. *Root* regression line with grazing.
- $\beta_1$ (`GrazingUngrazed`) is the effect of the absence of grazing on the intercept of the *Fruit* vs. *Root* line.
- $\beta_2$ (`Root`) is the slope of the *Fruit* vs. *Root* regression line with or without grazing.

Since the slope is the same with or without grazing, the coefficient $\beta_1$ corresponds to a translation on the $y$ axis of the regression line. This model of the *additive* effects of a treatment and a numerical variable thus results in two parallel lines, which corresponds quite well to our visualization of the data. In addition, the value of $R^2$ (0.93) indicates that the model accounts for much of the observed variation in the data.

Even a large value of $R^2$ does not necessarily mean that the model is appropriate. We must still look at the diagnostic charts below. Except for a few extreme values in the Q-Q plot, the assumptions seem to be met.

```{r, echo = FALSE}
par(mfrow = c(2,2))
plot(mod_comp)
par(mfrow = c(1,1))
```

Note that some extreme points are labelled with the corresponding row number in the data frame, to facilitate the identification of problematic points.

The $F$-test reported at the bottom of the results summary of `lm` corresponds to the null hypothesis of no effect for all predictors.

We can also obtain a conventional ANOVA table by applying the `anova` function to the `lm` result.

```{r}
anova(mod_comp)
```

This table shows how much of the sum of the squared differences is explained by each predictor, as well as the residual.

## Order of predictors

The functions `aov` and `anova` in R treat the predictors sequentially, i.e. the effect of each predictor is calculated relative to the residuals of the model including the previous predictors. In our example, the sum of the squared deviations for the predictor *Root* is based on the residuals of the model including only *Grazing*.

This is called a "Type I sum of squares" in statistics. In particular, this means that the ANOVA table would not necessarily be the same if we change the order of the predictors, e.g.: `Fruit ~ Root + Grazing`. Other R packages can perform an ANOVA with Type II and III sums of squares, but these are outside the scope of this course.

As mentioned earlier, the multiple linear regression coefficients estimate the partial effect of each predictor, that is, the effect of a difference in that predictor between two cases that do not differ for any other predictor. For this reason, the order of the predictors does not influence the estimates obtained with `lm`.

## Model with interaction

The previous model assumes that the effects of root size and grazing on seed mass are additive: in other words, the difference between the two grazing treatments is the same for each value of *Root* and the slope of *Fruit vs. Root* is the same for cases with and without grazing.

To consider the possibility that the effect of one predictor on the response depends on the value of another predictor, we need to specify an **interaction** between these two predictors. In R, the interaction is indicated by a multiplication symbol `*` between predictors in the model formula instead of the addition symbol `+`.

```{r}
mod_comp_inter <- lm(Fruit ~ Grazing * Root, data = comp)
summary(mod_comp_inter)
```

If $x_1$ is the grazing variable (0 = Grazed, 1 = Ungrazed according to the default encoding in R) and $x_2$ is the root size, the mathematical expression of this model is:

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \epsilon $$

The interaction is thus equivalent to adding a new predictor to the model, equal to the product of the two interacting variables. Let's separate again into two equations according to the treatment:

With grazing ($x_1 = 0$):

$$ y = \beta_0 + \beta_2 x_2 $$

Without grazing ($x_1 = 1$):

$$ y = (\beta_0 + \beta_1) + (\beta_2 + \beta_{12}) x_2 $$

For this model with interaction, the interpretation of the coefficients changes a bit:

- $\beta_0$ (`Intercept` in the summary table) is the intercept of the *Fruit* vs. *Root* line without grazing.
- $\beta_1$ (`GrazingUngrazed`) is the effect of the absence of grazing on the intercept of *Fruit* vs. *Root*.
- $\beta_2$ (`Root`) is the slope of the *Fruit* vs. *Root* line with grazing.
- $\beta_{12}$ (`GrazingUngrazed:Root`) is the effect of the absence of grazing on the slope of the *Fruit* vs. *Root* line.

The interaction model is therefore equivalent to separately estimating the regression line (intercept and slope) for each of the two treatments.

Compared to the additive model, note that the effect of no grazing (`GrazingUngrazed`) now has a much higher standard error and a larger $p$ value.

```{r}
summary(mod_comp)$coefficients
summary(mod_comp_inter)$coefficients
```

This is because the intercept, corresponding to *Root* = 0, is far from the range of the data (*Root* values are all between 4 and 11). Therefore, a small change of slope in the middle of the graph can lead to a significant change in intercept; the uncertainty of the interaction coefficient (difference in slope) also affects the estimate of the difference in intercept.

By consulting the ANOVA table, we can confirm that the interaction is not significant, the additive model is therefore preferable.

```{r}
anova(mod_comp_inter)
```

Why is the effect of grazing significant here while the *GrazingUngrazed* coefficient was not significant in the `lm` result? In the ANOVA table, we test whether there is a significant difference in the mean of *Fruit* between plants that are grazed or not, rather than whether there is a significant difference in intercept between two lines (which is what `GrazingUngrazed` measures in the linear model).


# Two-way ANOVA

## Example

To illustrate two-way ANOVA, we will first use the [growth.csv](../donnees/growth.csv) dataset from the textbook *Statistics: An Introduction Using R*. The experiment compares the weight gain of 48 animals following three types of diet with four types of supplements. There are 12 groups (all combinations of the 3 diets and 4 supplements) of 4 individuals each.

```{r}
growth <- read.csv("../donnees/growth.csv")
str(growth)
```

```{r}
ggplot(growth, aes(x = supplement, y = gain, color = diet)) +
    # position_dodge puts horizontal space between points of different colors
    geom_point(position = position_dodge(width = 0.3)) + 
    scale_color_brewer(palette = "Dark2")
```

At first glance, it seems plausible that the effects of the diet and the supplement are additive, since the difference between the diets is similar from one supplement to another and the difference between the supplements is similar from one diet to another. Moreover, the ANOVA table of the model with interaction does not show a significant effect of this interaction:

```{r}
aov_growth_inter <- aov(gain ~ diet * supplement, data = growth)
summary(aov_growth_inter)
```

Note that it is possible to use the `aov` function here because we only have categorical variables and the sample is balanced (4 replicates for each combination of diet and supplement).

Here are the results of the additive model. Both factors have a significant effect, and diet explains a larger portion of the variance in weight gain (based on the sum of the square deviations) than the supplement.

```{r}
aov_growth_add <- aov(gain ~ diet + supplement, data = growth)
summary(aov_growth_add)
```

The diagnostic graphs do not show any problem:

```{r, echo = FALSE}
par(mfrow = c(1, 2))
plot(aov_growth_add, which = 1:2)
```

According to Tukey's range test, we see that all three diets have a different effect (wheat < oats < barley). Among the supplements, *agrimore* and *supersupp* have a greater effect than *supergain* and *control*.

```{r}
TukeyHSD(aov_growth_add)
```

## Contrast representation

Here are the results of the same model fitted with `lm`:

```{r}
lm_growth_add <- lm(gain ~ diet + supplement, data = growth)
summary(lm_growth_add)
```

Remember that by default, R uses a treatment coding to represent the categorical variables in a linear regression, where the first level of the factor (in alphabetical order) is used as a reference. Here, *barley* and *agrimore* are the reference levels for the diet and the supplement, respectively. We can therefore interpret each coefficient in this way:

- the intercept is the mean weight gain for the reference levels (barley and agrimore);

- the coefficients `dietoats` and `dietwheat` give the mean difference in gain between the corresponding diet (oats or wheat) and the barley diet;

- the last three coefficients give the mean difference in gain between the corresponding supplement and the *agrimore* supplement.

The mean weight gain for any combination of diet and supplement can be obtained by summing the corresponding coefficients. For example, the main gain for an oat diet with the *supergain* supplement is: 26.12 (intercept) - 3.09 (oats) - 3.38 (supergain) = 19.65.

As seen in the last class, we can modify the contrasts to better represent the questions of interest. The code below converts the two predictors into factors, selects the control group as the reference for *supplement* and applies an effect coding for *diet*.

```{r}
growth <- mutate(growth, diet = as.factor(diet),
                 supplement = relevel(as.factor(supplement), ref = "control")) 
contrasts(growth$diet) <- "contr.sum"
colnames(contrasts(growth$diet)) <- c("barley" , "oats")

lm_growth_add <- lm(gain ~ diet + supplement, data = growth)
summary(lm_growth_add)
```

In this case, we can interpret the coefficients in this way:

- the intercept is the mean gain for the control group (*control*), averaged over the three diets;

- the `dietbarley` and `dietoats` coefficients give the mean difference in gain of the barley and oat diets compared to the mean of the three diets. The mean difference for the third diet (wheat) can be obtained by taking the opposite of the sum of the other effects: -(3.02 - 0.07) = -2.95.

- The last three coefficients give the mean difference in gain between each supplement and the control group.

## Model with interaction

The [antibiot.csv](../donnees/antibiot.csv) dataset contains measures of bacterial spread (surface covered in mm$^2$) as a function of the humidity ("sec" = dry, "humide" = humid) and the concentration of antibiotic ("faible" = low, "modérée" = moderate, "élevée" = high).

```{r}
# fileEncoding = "UTF-8" is to read the French accents correctly
antibiot <- read.csv("../donnees/antibiot.csv", fileEncoding = "UTF-8")
str(antibiot)
```

We must manually specify the levels of the *Concentration* factor to override the default alphabetical ordering.

```{r}
antibiot$Concentration <- factor(antibiot$Concentration, 
                                 levels = c("faible", "modérée", "élevée"))
levels(antibiot$Concentration)
```

Here is the graph of these data. Would a model with additive effects of antibiotic concentration and humidity level be appropriate here?

```{r}
ggplot(antibiot, aes(x = Concentration, y = Surface, color = Humidité)) +
    geom_point(position = position_dodge(width = 0.3)) + 
    scale_color_brewer(palette = "Dark2")
```

There is a clear *interaction* between the two factors. Notably, humid conditions are associated with a larger bacterial surface area for low and moderate antibiotic concentrations, but dry conditions have a larger bacterial surface when concentration is high.

Here is the summary and diagnostic plots for the model of bacterial spread as a function of the interaction between the two factors.

```{r}
aov_antibio <- aov(Surface ~ Concentration * Humidité, antibiot)
summary(aov_antibio)
```

```{r, echo = FALSE}
par(mfrow = c(1, 2))
plot(aov_antibio, which = 1:2)
```

The interaction between 3 levels of concentration and 2 levels of humidity defines 6 groups, so there are 15 possible pairwise comparison for this interaction, as can be seen from the result of `TukeyHSD`. 

```{r}
TukeyHSD(aov_antibio)
```

We will see in the next section a simpler way to visualize these comparisons.

The linear model corresponding to this ANOVA has 6 coefficients:

```{r}
lm_antibio <- lm(Surface ~ Concentration * Humidité, antibiot)
summary(lm_antibio)
```

- The intercept is the mean surface area for the reference levels (low and humid).

- The coefficients `Concentrationmodérée` and `Concentrationélevée` give the difference in mean surface area due to the increase in concentration from low to moderate and from low to high, for wet conditions.

- The coefficient `Humiditésec` gives the mean difference in surface area between the dry and wet conditions, for a low concentration.

- Finally, the interaction coefficients show the difference between the mean surface areas for the combinations "moderate and dry" and "high and dry", compared to the mean predicted by the additive effects only. In other words, the mean bacterial surface area for the combination "moderate and dry" is equal to: 5.87 (intercept) - 0.83 (moderate concentration) - 3.61 (dry) + 0.87 (moderate-dry interaction) = 2.30.

## Visualization of the effects with the *emmeans* package

The previous example shows that in the presence of an interaction, it is difficult to calculate the mean response for a given combination of treatments. The *emmeans* (estimated marginal means) package automatically calculates the means for each combination of treatments, as well as their confidence intervals. 

Below, we apply the `emmeans` function to the result of the `lm_antibio` model. The second argument of the function specifies the predictors to be considered: these are given as a formula as in the `lm` function, but without the response variable to the left of the `~`.

```{r}
library(emmeans)
em_antibio <- emmeans(lm_antibio, ~ Concentration * Humidité)
em_antibio
```

The `plot` function applied to the result of `emmeans` shows the means with their confidence intervals.

```{r}
plot(em_antibio)
```

It is a `ggplot` graph, so it can be customized with the functions we saw earlier.

```{r}
plot(em_antibio) +
    labs(x = "Mean bacterial surface area (mm sq.)")
```

The confidence intervals for each mean do not directly allow us to determine whether two means are significantly different from each other. To do this, we specify `compare = TRUE`, which adds comparison arrows, based on a Tukey test, to the graph. Overlapping arrows on the response variable axis indicate that the means are not significantly different (at a default threshold $\alpha = 0.05$).

```{r}
plot(em_antibio, comparisons = TRUE)
```

The comparisons shown here are the same as those obtained previously with the Tukey range test, but the visualization of the effects is simplified. In addition, the `TukeyHSD` function can only be applied to the result of `aov`, while `emmeans` can be applied to all the regression models we will see in this course. 

When a model is additive, we can estimate means for a single factor. In this case, the estimate is the mean response for each level of the factor, taking the mean of all other predictors. In the example below, therefore, we calculate the mean weight gain for each supplement by averaging the estimates for that supplement with each of the three diets.

```{r}
em_growth_supp <- emmeans(lm_growth_add, ~ supplement)
em_growth_supp
plot(em_growth_supp)
```


# Regression with multiple numerical predictors

## Example

The `hills` data frame in the *MASS* package (included by default with R) contains the record times (*time*, in minutes) for Scottish bike races based on the horizontal distance (*dist*, in miles) and the total change in altitude (*climb*, in feet).

```{r, warning = FALSE, message = FALSE}
library(MASS)
str(hills)
```

For a data frame with multiple numeric variables, the `plot` function displays a matrix of scatter plots for each pair of variables.

```{r}
plot(hills)
```

The record times seem to depend linearly on the distance and the climb. (The distance and climb also seem to be correlated, we will come back to this in the next class.) We therefore apply a linear model to these data.

```{r}
mod_hills <- lm(time ~ dist + climb, hills)
```

```{r, echo = FALSE}
par(mfrow = c(2,2))
plot(mod_hills)
par(mfrow = c(1,1))
```

Since the rows in this data frame are labelled (`rownames`) in R, those labels appear next to the extreme values in the diagnostic graphs.

According to these graphs, two races (Knock Hill and Bens of Jura) have a record time much longer than expected (large positive residual). These same points also have a great influence on the regression coefficients (according to the fourth graph). In this case, it would be recommended to check if these routes have peculiarities which explain this large difference compared to the model.

## Diagnostic graphs with *lindia*

In addition to the diagnostic graphs obtained with `plot`, it is useful in the case of multiple regression to visualize the residuals as a function of each predictor. The `gg_resX` function of the *lindia* package (for *linear diagnostics*) automatically produces these graphs from the model output.  

```{r, message = FALSE, warning = FALSE}
library(lindia)
gg_resX(mod_hills, ncol = 2) # ncol: number of columns
```

The presence of a trend in the residuals relative to a predictor would indicate a possible non-linear effect for that predictor.

The *lindia* package also produces other diagnostic graphs similar to those obtained with `plot`. You can produce all diagnostic graphs of a model with the `gg_diagnose` function. These are *ggplot2* type graphs, so you can customize them with the usual functions.

## Standardization of variables

Let's look at the summary results:

```{r}
summary(mod_hills)
```

The values of the coefficients mean that on average, each mile of distance adds 6.22 minutes to the record time while each foot of elevation adds 0.01 minute. Since the predictors do not have the same units, the value of the coefficients is not indicative of the importance of each variable. In this case, *dist* varies between 2 and 28 miles whereas *climb* varies between 300 and 7500 feet.

Also, the intercept is not meaningful, since a route cannot have a length of 0.

In order to compare the influence of different predictors, it may be useful to standardize these, that is, to transform each value by subtracting the mean and dividing by the standard deviation. In R, the `scale` function automatically performs this transformation.

```{r}
hills_scl <- hills
hills_scl[, -3] <- scale(hills_scl[, -3]) # we don't standardize the response
mod_hills_scl <- lm(time ~ dist + climb, data = hills_scl)
summary(mod_hills_scl)
```

For each point, the standardized variable represents the deviation of the original variable from its mean, expressed as a multiple of the standard deviation of the original variable. For example, in this version of the model, the coefficient of *dist* indicates the difference in record time associated with an increase of one standard deviation in the horizontal distance. The standardized coefficients thus represent the effect of a variable relative to the typical differences observed for that variable.

Another advantage of this representation is that, since the standardized predictors take a value of 0 at their mean, the value of the intercept is the overall mean of the response (here the mean record time is about 58 minutes).

The standardization of predictors only changes the scale of the estimated effects. The significance of the effect of each predictor and the predictions of the model remain the same.

## Interaction between continuous variables

How to interpret the interaction between two continuous variables? For example:

```{r}
mod_hills_inter <- lm(time ~ dist * climb, hills_scl)
summary(mod_hills_inter)
```

As we saw earlier, the equation for a model with two interacting predictors is:

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \epsilon $$

We can rewrite this equation in two ways:

$$ y = \beta_0 + (\beta_1 + \beta_{12} x_2) x_1 + \beta_2 x_2 $$

$$ y = \beta_0 + \beta_1 x_1 + (\beta_2 + \beta_{12} x_1) x_2 $$

- $\beta_0$ is the value of $y$ if $x_1 = 0$ and $x_2 = 0$;
- $\beta_1$ is the effect on $y$ of a unit increase in $x_1$ if $x_2 = 0$;
- $\beta_2$ is the effect on $y$ of a unit increase in $x_2$ if $x_1 = 0$;
- $\beta_{12}$ is both the change in slope of $y$ vs. $x_1$ if $x_2$ increases by 1, and the change in slope of $y$ vs. $x_2$ if $x_1$ increases by 1.

The standardization of predictors also facilitates the interpretation of these coefficients in the presence of an interaction: for example, if each predictor has a mean of zero, then $\beta_1$ is the effect of $x_1$ on $y$ for at a mean value of $x_2$.

It can be useful to visualize the predictions of the model with interaction. Below, we create a prediction data frame with `expand.grid`, which produces all combinations of values from the specified `dist` and `climb` vectors. To illustrate predictions with `ggplot`, we convert `climb` to a categorical variable (factor) to obtain distinct colors on the graph.

```{r}
hills_nouv <- expand.grid(dist = seq(-2, 2, 0.2), climb = c(-1, 0, 1))
hills_pred <- predict(mod_hills_inter, newdata = hills_nouv, interval = "confidence")
hills_pred <- cbind(hills_nouv, hills_pred)

ggplot(hills_pred, aes(x = dist, y = fit, color = as.factor(climb), 
                       fill = as.factor(climb))) +
    geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3) +
    geom_line() + 
    scale_color_brewer(palette = "Dark2") +
    scale_fill_brewer(palette = "Dark2")
```

This graph illustrates the effect of a positive interaction (positive coefficient of `dist:climb`): as one of the two variables increases, so does the effect of the other variable on the response (the slope of the line).

Here, we used the model based on the standardized predictors to make the predictions; we could have used a model based on the original predictors in order to obtain more easily interpretable scales for *dist* and *climb*.


# Summary

- In multiple linear regression (without interaction), the coefficient associated with a predictor measures the effect of a unit difference in the predictor value on the response, if the other predictors remain the same.

- An interaction between two predictors means that the effect of one predictor on the response (i.e. the slope of the regression line) depends on the value of another predictor.

- The *emmeans* package allows multiple comparisons to be made for the effect of a categorical variable on a response, similar to the Tukey range test, but applicable to any regression model.

- Standardizing the predictors of a regression (subtracting the mean and dividing by the standard deviation) makes it easier to compare coefficients and interpret the intercept (which represents the overall mean of the response variable).