<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Régression logistique</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Régression logistique</h1>
<h4 class="date"><br/>1 novembre 2021</h4>

</div>


<div id="objectifs" class="section level1">
<h1>Objectifs</h1>
<ul>
<li><p>Connaître les composantes d’un modèle linéaire généralisé.</p></li>
<li><p>Utiliser la régression logistique pour modéliser une réponse binaire (ex.: présence/absence) ou binomiale (ex.: nombre de présences et d’absences).</p></li>
<li><p>Interpréter les résultats d’une régression logistique et visualiser les effets des prédicteurs.</p></li>
</ul>
</div>
<div id="motivation" class="section level1">
<h1>Motivation</h1>
<p>Supposons que nous nous intéressons à modéliser une réponse catégorique binaire, par exemple:</p>
<ul>
<li>survie ou mort d’individus dans un intervalle de temps donné;</li>
<li>présence ou absence d’une espèce sur différents sites;</li>
<li>décision d’un animal de demeurer au même site ou migrer à un autre site.</li>
</ul>
<p>Numériquement, cette réponse est codée par des valeurs de 0 ou 1. Dans un contexte de régression, ce qui nous intéresse est la moyenne de cette réponse, soit la proportion des cas où la réponse est 1 (on pourrait aussi dire “la probabilité que la réponse soit 1”).</p>
<p>Nous pourrions modéliser la relation entre cette réponse moyenne et différents prédicteurs avec un modèle de régression linéaire, tel que vu lors des derniers cours:</p>
<p><span class="math display">\[ \hat{y} = \beta_0 + \sum_{i = 1}^m \beta_i x_i \]</span></p>
<p>Toutefois, cette approche pose quelques problèmes:</p>
<ul>
<li>Pour certaines valeurs des prédicteurs, on obtient une réponse prédite <span class="math inline">\(\hat{y}\)</span> inférieure à 0 ou supérieure à 1, ce qui n’a pas de sens dans ce contexte.</li>
<li>Ce type de réponse ne satisfait pas le critère d’homogénéité de la variance du modèle de régression linéaire. La variance des réponses individuelles est beaucoup plus grande lorsque la probabilité est près de 0.5 que lorsqu’elle s’approche de 0 ou 1.</li>
</ul>
<p>Les <strong>modèles linéaires généralisés</strong> (<em>generalized linear models</em> ou GLM) résolvent ces deux problèmes, en permettant de choisir différentes transformations de la prédiction linéaire et différentes distributions pour les réponses individuelles en fonction de la réponse moyenne.</p>
<p>Afin de mieux expliquer le principe de ces modèles, nous reformulons le modèle de régression linéaire multiple, en divisant ce modèle en trois composantes:</p>
<ul>
<li>un prédicteur linéaire (une combinaison linéaire des variables explicatives): <span class="math inline">\(\eta = \beta_0 + \sum_{i = 1}^m \beta_i x_i\)</span>;</li>
<li>un lien entre la réponse moyenne et la valeur du prédicteur (ici, simplement l’égalité): <span class="math inline">\(\hat{y} = \eta\)</span>; et</li>
<li>une distribution des réponses individuelles en fonction de la réponse moyenne (ici, la distribution normale): <span class="math inline">\(y \sim N(\hat{y}, \sigma)\)</span>.</li>
</ul>
<p>Les modèles linéaires généralisés conservent le premier élément (la prédiction dépend d’une combinaison linéaire des variables <span class="math inline">\(x_i\)</span>), mais utilisent différentes fonctions de lien et différentes distributions pour la réponse. La régression logistique que nous verrons aujourd’hui est conçue pour modéliser les variables à réponse binaire, tandis que la régression de Poisson vue au prochain cours est conçue pour les données provenant de comptages (nombres entiers <span class="math inline">\(\ge\)</span> 0).</p>
</div>
<div id="modèle-de-régression-logistique" class="section level1">
<h1>Modèle de régression logistique</h1>
<p>La régression logistique est un exemple de modèle linéaire généralisé où la réponse correspond à une ou plusieurs observations d’un résultat binaire (0 ou 1). Dans ce modèle:</p>
<ul>
<li>la réponse suit une distribution binomiale et</li>
<li>la fonction logit permet de relier la probabilité d’un résultat positif (<span class="math inline">\(y = 1\)</span>) à la valeur du prédicteur linéaire.</li>
</ul>
<div id="distribution-binomiale" class="section level2">
<h2>Distribution binomiale</h2>
<p>Imaginons <span class="math inline">\(n\)</span> répétitions indépendantes d’une expérience qui peut se conclure par la présence (1) ou l’absence (0) d’un événement, avec une probabilité de présence <span class="math inline">\(p\)</span> pour chaque répétition. Dans ce cas, le nombre de présences <span class="math inline">\(y\)</span> suit une distribution binomiale: <span class="math inline">\(y \sim B(n, p)\)</span>. Selon cette distribution, la probabilité d’obtenir <span class="math inline">\(y\)</span> présences et <span class="math inline">\(n-y\)</span> absences correspond à:</p>
<p><span class="math display">\[ \frac{n!}{y!(n-y)!} p^y (1-p)^{n-y}  = \binom{n}{y} p^y(1-p)^{n-y}\]</span></p>
<p>Pour comprendre cette formule, prenons par exemple la séquence de résultats 01001 (<span class="math inline">\(n = 5\)</span> et <span class="math inline">\(y = 2\)</span>). La probabilité d’obtenir 0 au premier essai est <span class="math inline">\(1-p\)</span>, la probabilité d’obtenir 1 au deuxième essai est <span class="math inline">\(p\)</span>, et ainsi de suite. La probabilité d’une séquence d’essais indépendants est le produit des probabilités de chaque essai, donc <span class="math inline">\(p^2(1-p)^3\)</span>, ce qui correspond à la partie <span class="math inline">\(p^y(1-p)^{n-y}\)</span> de la formule. Toutefois, il s’agit de la probabilité d’une seule séquence de deux 1 et trois 0. Chacune des autres séquences possibles (11000, 00110, etc.) a la même probabilité. Ce nombre de séquences possibles est égal à <span class="math inline">\(n!/y!(n-y)!\)</span>, où <span class="math inline">\(!\)</span> représente l’opération factorielle.</p>
<p>Le <em>nombre</em> moyen de présences est égal à <span class="math inline">\(np\)</span>, avec une variance égale à <span class="math inline">\(np(1-p)\)</span>. La <em>proportion</em> moyenne de présence est égale à <span class="math inline">\(p\)</span> avec une variance de <span class="math inline">\(p(1-p)/n\)</span>.</p>
<p>La distribution binomiale avec un seul essai (<span class="math inline">\(n = 1\)</span>) porte le nom de distribution de Bernoulli, avec une moyenne de <span class="math inline">\(p\)</span> et une variance égale à <span class="math inline">\(p(1-p)\)</span>.</p>
<p>Dans R, les fonctions permettant de calculer des quantités à partir de la distribution binomiale portent le suffixe <code>binom</code>. Par exemple, la probabilité d’obtenir 2 présences sur 5 essais, avec une probabilité de présence de 0.3, est égale à:</p>
<pre class="r"><code>dbinom(2, 5, 0.3)</code></pre>
<pre><code>## [1] 0.3087</code></pre>
<p>Consultez la rubrique d’aide <code>?Binomial</code> pour plus de détails.</p>
</div>
<div id="lien-logit" class="section level2">
<h2>Lien logit</h2>
<p>Pour relier la probabilité de présence <span class="math inline">\(p\)</span> de la distribution binomiale au prédicteur linéaire <span class="math inline">\(\eta = \beta_0 + \sum_{i = 1}^m \beta_i x_i\)</span>, le lien le plus souvent utilisé est la fonction logit.</p>
<p><span class="math display">\[ \eta = \text{logit}(p) = \log \left( \frac{p}{1-p} \right) \]</span></p>
<p>Si on inverse cette équation, on obtient une expression pour <span class="math inline">\(p\)</span> en fonction de <span class="math inline">\(\eta\)</span>.</p>
<p><span class="math display">\[ p = \frac{1}{1 + e^{-\eta}} \]</span></p>
<p>Cette deuxième équation porte souvent le nom de <em>fonction logistique</em>. Voici son graphique:</p>
<p><img src="9-Regression_logistique_files/figure-html/unnamed-chunk-2-1.png" width="384" /></p>
<p>Notons certaines propriétés importantes de cette fonction:</p>
<ul>
<li>Si <span class="math inline">\(\eta = 0\)</span>, <span class="math inline">\(p = 0.5\)</span> (probabilité de 50%).</li>
<li>La fonction est symétrique autour de son point central: <span class="math inline">\(p(-\eta) = 1 - p(\eta)\)</span>.</li>
<li>La probabilité approche 0 lorsque <span class="math inline">\(\eta\)</span> prend des valeurs très négatives et approche 1 lorsque <span class="math inline">\(\eta\)</span> prend des valeurs très positives, sans jamais atteindre ces valeurs extrêmes.</li>
<li>La pente de la courbe est plus prononcée au centre et plus faible (tendant vers 0) aux extrémités. Du point de vue de la régression, cela signifie que l’effet des variables explicatives (contenues dans <span class="math inline">\(\eta\)</span>) sur la probabilité de présence est plus important lorsque celle probabilité est près de 50%.</li>
</ul>
<p>En résumé, la régression logistique est basée sur le modèle suivant:</p>
<ul>
<li><span class="math inline">\(y \sim B(n, p)\)</span> (distribution binomiale de la réponse)</li>
<li><span class="math inline">\(\text{logit}(p) = \beta_0 + \sum_{i = 1}^m \beta_i x_i\)</span> ou de façon équivalente:</li>
</ul>
<p><span class="math display">\[p = \frac{1}{1 + e^{-(\beta_0 + \sum_{i = 1}^m \beta_i x_i)}}\]</span></p>
<p>Contrairement à la régression linéaire, les coefficients <span class="math inline">\(\beta\)</span> d’un modèle linéaire généralisé ne sont pas estimés par la méthode des moindres carrés, mais plutôt par la méthode plus générale du maximum de vraisemblance (<em>maximum likelihood</em>). En bref, cette méthode choisit les valeurs des coefficients <span class="math inline">\(\beta\)</span> qui maximisent la probabilité conjointe de l’ensemble des observations de <span class="math inline">\(y\)</span>.</p>
</div>
<div id="interprétation-des-coefficients" class="section level2">
<h2>Interprétation des coefficients</h2>
<p>Le graphique ci-dessous présente la probabilité <span class="math inline">\(p\)</span> pour un modèle logistique à une variable <span class="math inline">\(x\)</span> avec <span class="math inline">\(\beta_0 = -1\)</span> et <span class="math inline">\(\beta_1\)</span> = 0.4, donc: <span class="math inline">\(\text{logit}(p) = -1 + 0.4x\)</span>.</p>
<p><img src="9-Regression_logistique_files/figure-html/unnamed-chunk-3-1.png" width="384" /></p>
<p>L’ordonnée à l’origine <span class="math inline">\(\beta_0\)</span> est égale au logit de <span class="math inline">\(p\)</span> lorsque <span class="math inline">\(x = 0\)</span>. Pour calculer la valeur de <span class="math inline">\(p\)</span> correspondant à une valeur donnée de <span class="math inline">\(\text{logit}(p)\)</span>, on peut utiliser la fonction <code>plogis</code> dans R (qui correspond donc à la fonction logistique ou “inverse du logit”), tandis que la transformation logit elle-même est obtenue par la fonction <code>qlogis</code>.</p>
<pre class="r"><code>plogis(-1)</code></pre>
<pre><code>## [1] 0.2689414</code></pre>
<pre class="r"><code>qlogis(0.5)</code></pre>
<pre><code>## [1] 0</code></pre>
<p>La valeur de <span class="math inline">\(x\)</span> pour laquelle le prédicteur linéaire est zéro (ici, <span class="math inline">\(-1 + 0.4x = 0\)</span> donne <span class="math inline">\(x = 2.5\)</span>) est associée à une probabilité <span class="math inline">\(p = 0.5\)</span>. C’est à cet endroit de la courbe que la pente de <span class="math inline">\(p\)</span> vs. <span class="math inline">\(x\)</span> est maximale: cette pente maximale, indiquée par un ligne bleue dans le graphique, est égale à <span class="math inline">\(\beta_1/4\)</span>, où <span class="math inline">\(\beta_1\)</span> est le coefficient de <span class="math inline">\(x\)</span> dans le prédicteur linéaire. Ici, puisque <span class="math inline">\(\beta_1 = 0.4\)</span>, on peut dire que lorsque <span class="math inline">\(x\)</span> augmente de 1, la probabilité <span class="math inline">\(p\)</span> augmente au maximum de 0.1 (ou 10%).</p>
</div>
</div>
<div id="régression-logistique-avec-r" class="section level1">
<h1>Régression logistique avec R</h1>
<p>Dans R, la régression logistique peut être utilisée pour deux types de réponses:</p>
<ul>
<li>une variable binaire codée par des valeurs logiques (FALSE, TRUE), des valeurs numériques (0, 1) ou un facteur (le premier niveau du facteur correspond à 0, les autres à 1);</li>
<li>une variable binomiale décrite par deux colonnes (nombre de présences, nombre d’absences).</li>
</ul>
<p>Nous verrons un exemple de chaque cas dans ce cours.</p>
<div id="exemple-concentrations-darsenic-dans-des-puits-au-bangladesh" class="section level2">
<h2>Exemple: Concentrations d’arsenic dans des puits au Bangladesh</h2>
<p>Le tableau de données <code>Wells</code> du package <strong>carData</strong> présente les données d’une étude menée auprès de 3020 ménages au Bangladesh. Les puits utilisés par ces ménages avaient une concentration d’arsenic (variable <code>arsenic</code>, en multiples de 100 <span class="math inline">\(\mu g/L\)</span>) supérieure au niveau jugé sûr. La réponse binaire <code>switch</code> indique si le ménage a changé de puits. En plus de la concentration d’arsenic, le tableau contient d’autres prédicteurs, dont la distance vers le puits sûr le plus près (<code>distance</code> en mètres).</p>
<p>Cet exemple est tiré du manuel de Gelman et Hill, <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>.</p>
<pre class="r"><code>library(carData)
str(Wells)</code></pre>
<pre><code>## &#39;data.frame&#39;:    3020 obs. of  5 variables:
##  $ switch     : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 2 2 2 2 2 2 2 ...
##  $ arsenic    : num  2.36 0.71 2.07 1.15 1.1 3.9 2.97 3.24 3.28 2.52 ...
##  $ distance   : num  16.8 47.3 21 21.5 40.9 ...
##  $ education  : int  0 0 10 12 14 9 4 10 0 0 ...
##  $ association: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 2 2 2 1 2 2 ...</code></pre>
<p>En explorant ce type de données, il peut être utile de comparer la distribution des prédicteurs potentiels lorsque la réponse est positive ou négative.</p>
<p><img src="9-Regression_logistique_files/figure-html/unnamed-chunk-6-1.png" width="480" /></p>
<p>En moyenne, les ménages qui changent de puits ont une concentration d’arsenic plus élevée dans leur puits original et sont situés plus près d’un puits sûr. Toutefois, aucune des deux variables ne semble être associée très fortement avec la réponse.</p>
<p><strong>Note</strong>: Vu la distribution asymétrique des variables <code>arsenic</code> et <code>distance</code>, une transformation logarithmique de ces variables serait justifiable. Pour ce problème particulier, cette transformation affecte peu l’ajustement du modèle, donc nous conservons l’échelle originale des variables qui est plus facile à interpréter.</p>
<p>La fonction <code>glm</code> sert à estimer les paramètres de modèles linéaires généralisés. Comme pour les modèles linéaires, on spécifie d’abord une formule et un tableau de données. Ensuite, il faut indiquer la distribution de la réponse avec l’argument <code>family</code>, ainsi que la fonction de lien (<code>link</code>). Pour la régression logistique, il s’agit d’une distribution binomiale avec un lien logit. Notez que puisque le logit est la fonction de lien par défaut pour une réponse binomiale, sa mention est facultative (on aurait pu seulement écrire <code>family = binomial</code>).</p>
<pre class="r"><code>mod &lt;- glm(switch ~ arsenic + distance, data = Wells,
           family = binomial(link = &quot;logit&quot;))
summary(mod)</code></pre>
<pre><code>## 
## Call:
## glm(formula = switch ~ arsenic + distance, family = binomial(link = &quot;logit&quot;), 
##     data = Wells)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6351  -1.2139   0.7786   1.0702   1.7085  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.002749   0.079448   0.035    0.972    
## arsenic      0.460775   0.041385  11.134   &lt;2e-16 ***
## distance    -0.008966   0.001043  -8.593   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 4118.1  on 3019  degrees of freedom
## Residual deviance: 3930.7  on 3017  degrees of freedom
## AIC: 3936.7
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>L’ordonnée à l’origine est le logit de la probabilité de changer de puits pour une concentration d’arsenic et une distance toutes deux égales à zéro. Cette probabilité est donnée par <code>plogis(0.0027)</code> soit environ 50%. Toutefois, puisque le modèle a été ajusté à un tableau de données où la concentration d’arsenic est toujours supérieure à 0.5, l’ordonnée à l’origine n’a pas vraiment de sens.</p>
<p>Les coefficients des prédicteurs indiquent qu’une augmentation de la concentration d’arsenic d’une unité (100 <span class="math inline">\(\mu g/L\)</span>) résulte au maximum en une augmentation de ~0.12 de la probabilité de changer de puits (0.46/4), tandis qu’une augmentation de 1m de la distance résulte au maximum en une diminution de 0.0022 (0.0087/4) de cette probabilité (équivalent à une diminution de ~0.22 par 100m de distance).</p>
</div>
<div id="visualiser-les-prédictions-du-modèle" class="section level2">
<h2>Visualiser les prédictions du modèle</h2>
<p>Pour mieux comprendre l’effet non-linéaire des prédicteurs, nous pouvons visualiser les prédictions du modèle pour différentes combinaisons de la concentration d’arsenic et de la distance.</p>
<p>Créons d’abord un nouveau tableau de données pour les prédictions, contenant les combinaisons de 10 concentrations d’arsenic (réparties entre 0.5 et 5) et 3 distances au puits sûr le plus près (50 m, 100 m et 200 m). Nous utilisons pour ce faire la fonction <code>expand.grid</code>.</p>
<pre class="r"><code>wells_nouv &lt;- expand.grid(arsenic = seq(0.5, 5, 0.5), distance = c(50, 100, 200))</code></pre>
<p>Regardons ce qui se produit si nous appliquons la fonction <code>predict</code> au modèle avec ces nouvelles données.</p>
<pre class="r"><code>wells_nouv$pred &lt;- predict(mod, wells_nouv)
summary(wells_nouv$pred)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -1.5602 -0.3600  0.2518  0.2238  0.8823  1.8583</code></pre>
<p>Ces valeurs ne se situent pas entre 0 et 1 car par défaut, <code>predict</code> retourne les valeurs du prédicteur linéaire <span class="math inline">\(\eta\)</span> (correspondant à l’option <code>type = "link"</code>). Pour obtenir les prédictions sur l’échelle de la réponse (ici, des probabilités), il faut spécifier <code>type = "response"</code>.</p>
<pre class="r"><code>wells_nouv$prob_pred &lt;- predict(mod, wells_nouv, type = &quot;response&quot;)

ggplot(wells_nouv, aes(x = arsenic, y = prob_pred, color = as.factor(distance))) +
    geom_line() +
    geom_hline(yintercept = 0.5, linetype = &quot;dotted&quot;) +
    scale_color_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="9-Regression_logistique_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Nous pourrions noter par exemple que plus la distance vers un puits sûr augmente, plus la concentration doit être élevée pour qu’un ménage change de puits la majorité du temps (ligne pointillée à <span class="math inline">\(p = 0.5\)</span>).</p>
<p>Pour visualiser l’intervalle de confiance de ces probabilités, nous devons revenir à l’échelle du prédicteur linéaire <span class="math inline">\(\eta\)</span> plutôt que celle de la réponse, obtenir les erreurs-types des valeurs <span class="math inline">\(\eta\)</span> avec l’option <code>se.fit = TRUE</code>, calculer un intervalle de 1.96 erreur-type de part et d’autre de <span class="math inline">\(\eta\)</span>, puis finalement convertir <span class="math inline">\(\eta\)</span> et les bornes de l’intervalle en valeurs de <span class="math inline">\(p\)</span> avec la fonction <code>plogis</code>.</p>
<pre class="r"><code>prob_pred &lt;- predict(mod, wells_nouv, se.fit = TRUE)
wells_nouv$prob_pred &lt;- prob_pred$fit
wells_nouv$prob_se &lt;- prob_pred$se.fit

ggplot(wells_nouv, aes(x = arsenic, y = plogis(prob_pred), color = as.factor(distance),
                       fill = as.factor(distance))) +
    geom_ribbon(aes(ymin = plogis(prob_pred - 1.96*prob_se), 
                    ymax = plogis(prob_pred + 1.96*prob_se)), alpha = 0.3) +
    geom_line() +
    scale_color_brewer(palette = &quot;Dark2&quot;) +
    scale_fill_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="9-Regression_logistique_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>La raison pour laquelle nous devons d’abord calculer l’intervalle de confiance sur l’échelle du prédicteur linéaire est que l’incertitude sur celui-ci suit davantage une distribution normale que l’incertitude sur <span class="math inline">\(p\)</span>.</p>
<p>Notez qu’un intervalle de prédiction des observations individuelles serait moins intéressant ici, car ces observations sont toujours 0 ou 1.</p>
</div>
<div id="vérifier-lajustement-du-modèle" class="section level2">
<h2>Vérifier l’ajustement du modèle</h2>
<p>Les graphiques de diagnostic basés sur les résidus individuels ne sont pas très utiles lorsque la réponse est binaire, comme vous pouvez le constater en appelant la fonction <code>plot(mod)</code>.</p>
<p><img src="9-Regression_logistique_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Pour chaque valeur prédite (ici, les prédictions sont représentées sur l’échelle du prédicteur linéaire, pas celle de <span class="math inline">\(p\)</span>), il n’y a que deux valeurs possibles pour le résidu: l’une positive si la réponse est 1, et l’autre négative si la réponse est 0. C’est pourquoi on observe deux lignes de points sur le graphique ci-dessus.</p>
<p>Une meilleure stratégie serait de regrouper les points avec des probabilités prédites semblables, puis calculer le résidu correspondant à la différence entre (1) la proportion de réponses positives observées parmi les points d’un groupe et (2) la probabilité moyenne pour ces points. Par exemple, pour un groupe de 20 points avec 11 réponses positives et une prédiction moyenne de 0.6, le résidu serait -0.05 (11/20 - 0.6). La fonction <code>binnedplot</code> du package <strong>arm</strong> sert à créer un tel graphique des résidus groupés (<em>binned residual plot</em>).</p>
<pre class="r"><code>library(arm)
binnedplot(fitted(mod), residuals(mod, type = &quot;response&quot;))</code></pre>
<p><img src="9-Regression_logistique_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p><strong>Note</strong>: Il existe différentes définitions des résidus pour un modèle linéaire généralisé, donc il faut spécifier un type dans la fonction <code>residuals</code>. Pour ce graphique-ci, nous utilisons les résidus sur l’échelle de la réponse (<code>type = "response"</code>), soit les différences entre les réponses observées et prédites (<span class="math inline">\(y - \hat{y}\)</span>).</p>
<p>Par défaut, <code>binnedplot</code> choisit le nombre de groupes d’après un compromis visant à avoir suffisamment de points par groupe (pour que chaque proportion moyenne soit précise) et suffisamment de groupes (pour voir la tendance s’il y en a une). Lorsque le nombre d’observations <span class="math inline">\(n &gt; 100\)</span>, le nombre de groupes choisi est environ <span class="math inline">\(\sqrt{n}\)</span>.</p>
<p>Le graphique produit par <code>binnedplot</code> indique aussi un intervalle de prédiction à 95% (lignes grises) pour les résidus moyens. Ainsi, si le modèle binomial est bon, environ 95% des résidus devraient se trouver à l’intérieur de cet intervalle. Ici, c’est le cas de 52 résidus sur 54 ou 96%.</p>
</div>
<div id="coefficient-de-détermination" class="section level2">
<h2>Coefficient de détermination</h2>
<p>L’estimation des paramètres des modèles linéaires généralisés ne fait pas appel à la méthode des moindres carrés. Pour cette raison, le <span class="math inline">\(R^2\)</span> basé sur la somme des écarts carrés ne constitue pas une bonne mesure d’ajustement pour ce type de modèle.</p>
<p>La <strong>déviance</strong> est une mesure d’écart entre les valeurs attendues et les observations calculée à partir de la vraisemblance (<span class="math inline">\(L\)</span>, pour <em>likelihood</em>) du modèle pour les paramètres estimés.</p>
<p><span class="math display">\[ D = -2 \log L \]</span></p>
<p>Cette expression est aussi égale au premier terme de l’AIC. Plus le modèle accorde une grande probabilité aux observations, plus <span class="math inline">\(L\)</span> est grand et plus la déviance est petite. Comme pour l’AIC, la valeur absolue de la déviance n’a pas de sens, mais cette quantité est utile pour comparer l’ajustement de différents modèles.</p>
<p>Dans le sommaire du résultat de <code>glm</code>, la déviance du modèle ajusté est indiquée comme <code>Residual Deviance</code>. Le sommaire inclut aussi une autre valeur, <code>Null Deviance</code>, qui correspond à la déviance du modèle nul ne comptant aucun prédicteur. Ces deux valeurs jouent un rôle semblable à la sommes des écarts carrés résiduels et la somme des écarts carrés totaux dans le modèle linéaire. On peut donc définir le <em>pseudo R<span class="math inline">\(^2\)</span></em> (ou <span class="math inline">\(R^2\)</span> de McFadden) comme la fraction de la déviance du modèle nul expliquée par le modèle incluant les prédicteurs.</p>
<pre class="r"><code>pseudo_R2 &lt;- 1 - mod$deviance/mod$null.deviance
pseudo_R2</code></pre>
<pre><code>## [1] 0.04551395</code></pre>
<p>La concentration d’arsenic et la distance, malgré leur effet significatif, n’expliquent donc pas une grande partie de la décision de changer de puits ou non. Cela est cohérent avec notre exploration initiale des données.</p>
<p>La formule du <span class="math inline">\(R^2\)</span> basé sur la déviance s’applique à tous les modèles ajustés par maximum de vraisemblance. Pour la régression logistique spécifiquement, une autre version du coefficient de détermination a été proposée par Tjur:</p>
<p><span class="math display">\[ {R^2}_{\text{Tjur}} = \bar{\hat{y}}_{(y=1)} - \bar{\hat{y}}_{(y=0)} \]</span></p>
<p>Autrement dit, le <span class="math inline">\(R^2\)</span> de Tjur mesure la différence entre la réponse moyenne prédite pour les cas où la réponse observée est 1 et la réponse moyenne prédite pour les cas où la réponse observée est 0. Il indique donc à quel point le modèle peut “séparer” les deux groupes <span class="math inline">\(y = 1\)</span> et <span class="math inline">\(y = 0\)</span>. À l’extrême, un coefficient de 0 indique que le modèle prédit en moyenne la même réponse pour les deux groupes, tandis qu’un coefficient de 1 indique que le modèle prédit avec certitude la bonne réponse pour toutes les observations.</p>
<p>Pour notre exemple, le <span class="math inline">\(R^2\)</span> de Tjur est d’environ 0.06.</p>
<pre class="r"><code>r2_tjur &lt;- mean(mod$fitted.values[mod$y == 1]) - mean(mod$fitted.values[mod$y == 0])
r2_tjur</code></pre>
<pre><code>## [1] 0.06004311</code></pre>
</div>
</div>
<div id="régression-logistique-binomiale" class="section level1">
<h1>Régression logistique binomiale</h1>
<p>Dans l’exemple précédent, chaque rangée du tableau correspondait à une seule réponse binaire (0 ou 1) et des valeurs des prédicteurs qui lui sont associées. Dans un contexte expérimental, il est possible d’avoir plusieurs réplicats indépendants avec les mêmes valeurs des prédicteurs; le nombre de réponses égales à 1 (présences) parmi ces <span class="math inline">\(N\)</span> réplicats peut être modélisé directement comme une variable binomiale.</p>
<div id="exemple-mortalité-descargots-en-fonction-de-lenvironnement" class="section level2">
<h2>Exemple: Mortalité d’escargots en fonction de l’environnement</h2>
<p>Le tableau de données <code>snails</code> du package <strong>MASS</strong> présente les résultats d’une expérience où des escargots de 2 espèces (<code>Species</code>) ont été exposés à 3 différentes valeurs de température (<code>Temp</code>) et 4 taux d’humidité relative (<code>Rel.Hum</code>) pour 1 à 4 semaines (<code>Exposure</code>). Vingt escargots (<code>N</code>) ont été suivis pour chacune des 96 combinaisons possible de ces quatre variables; la variable <code>Deaths</code> indique le nombre d’entre eux qui sont morts durant l’expérience.</p>
<pre class="r"><code>library(MASS)
str(snails)</code></pre>
<pre><code>## &#39;data.frame&#39;:    96 obs. of  6 variables:
##  $ Species : Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Exposure: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ Rel.Hum : num  60 60 60 65.8 65.8 65.8 70.5 70.5 70.5 75.8 ...
##  $ Temp    : int  10 15 20 10 15 20 10 15 20 10 ...
##  $ Deaths  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ N       : int  20 20 20 20 20 20 20 20 20 20 ...</code></pre>
<p>Dans cet exemple, nous coderons la mortalité par 1 et la survie (absence de mortalité) par 0. Pour appliquer une régression logistique binomiale à ces données, la réponse de notre formule (à gauche du <code>~</code>) doit être composée de deux colonnes (nombre de 1 et nombre de 0) regroupées avec <code>cbind</code>. Ici, la variable représentant le nombre de 0 (les survivants) est obtenu en soustrayant <code>Deaths</code> de <code>N</code>.</p>
<pre class="r"><code>mod_snails &lt;- glm(cbind(Deaths, N - Deaths) ~ Species + Exposure + Rel.Hum + Temp,
           data = snails, family = binomial)
summary(mod_snails)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(Deaths, N - Deaths) ~ Species + Exposure + 
##     Rel.Hum + Temp, family = binomial, data = snails)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7261  -0.7107  -0.3362   0.4231   1.7510  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.40495    0.97070  -1.447    0.148    
## SpeciesB     1.30864    0.16350   8.004 1.20e-15 ***
## Exposure     1.50339    0.10235  14.689  &lt; 2e-16 ***
## Rel.Hum     -0.10684    0.01388  -7.699 1.37e-14 ***
## Temp         0.09404    0.01927   4.881 1.06e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 539.72  on 95  degrees of freedom
## Residual deviance:  55.07  on 91  degrees of freedom
## AIC: 223.93
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Nous remarquons d’abord que le modèle explique une bonne partie de la variation du taux de mortalité entre les groupes (le pseudo-R<span class="math inline">\(^2\)</span> de McFadden est égal à 0.9). Puisque chaque “observation” (rangée) est un sommaire de 20 individus, la variation aléatoire de la réponse est moins grande que lorsqu’on considère les observations individu par individu; il n’est donc pas étonnant que le pseudo-R<span class="math inline">\(^2\)</span> soit plus élevé dans ce cas.</p>
<p>D’après le signe des coefficients, la mortalité est plus grande pour l’espèce B que l’espèce A. Cette mortalité augmente aussi avec le temps d’exposition et la température, mais diminue avec une hausse d’humidité relative (dans la plage de valeurs considérée).</p>
<p>Avec plusieurs observations par rangée, les résidus individuels contiennent davantage d’information, donc les graphiques de diagnostic conventionnels sont plus utiles que dans l’exemple précédent. Cependant, près de la moitié des rangées n’ont aucun escargot mort (<code>Deaths</code> = 0) et ces zéros sont la cause de la “ligne” de résidus dans certains graphiques.</p>
<p><img src="9-Regression_logistique_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Puisque la réponse n’est pas supposée suivre une distribution normale, nous ne nous intéressons pas vraiment au diagramme quantile-quantile. Le graphique <em>Residuals vs Fitted</em> permet de vérifier l’absence de tendance dans les résidus et le graphique <em>Residuals vs Leverage</em> permet de détecter des points avec une grande influence sur la régression. Notez que trois des graphiques utilisent les résidus de Pearson (Std. Pearson resid.) qui correspondent aux résidus divisés par l’écart-type attendu de la réponse (rappelons-nous que cet écart-type dépend de la valeur attendue de la réponse ici). Ces résidus devraient donc avoir une variance plus homogène que les résidus bruts.</p>
</div>
</div>
<div id="problème-de-séparation-parfaite" class="section level1">
<h1>Problème de séparation parfaite</h1>
<p>En utilisant le même jeu de données <code>snails</code>, essayons maintenant d’estimer l’effet de la durée d’exposition comme variable catégorique. Ceci revient à estimer séparément le taux de mortalité après 1, 2, 3 et 4 semaines.</p>
<pre class="r"><code>mod_snails2 &lt;- glm(cbind(Deaths, N - Deaths) ~ as.factor(Exposure), data = snails,
                   family = binomial)
summary(mod_snails2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(Deaths, N - Deaths) ~ as.factor(Exposure), 
##     family = binomial, data = snails)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.9350  -1.0881  -0.0002   0.5037   4.1261  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)            -20.75     888.02  -0.023    0.981
## as.factor(Exposure)2    17.25     888.02   0.019    0.985
## as.factor(Exposure)3    19.33     888.02   0.022    0.983
## as.factor(Exposure)4    20.13     888.02   0.023    0.982
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 539.72  on 95  degrees of freedom
## Residual deviance: 182.38  on 92  degrees of freedom
## AIC: 349.24
## 
## Number of Fisher Scoring iterations: 17</code></pre>
<p>Pourquoi avons-nous des coefficients et des erreurs-types aussi élevés? Regardons quel est le nombre moyen d’escargots morts pour chacune des valeurs de la variable <code>Exposure</code>.</p>
<pre class="r"><code>group_by(snails, Exposure) %&gt;%
    summarize(mean(Deaths))</code></pre>
<pre><code>## # A tibble: 4 x 2
##   Exposure `mean(Deaths)`
##      &lt;int&gt;          &lt;dbl&gt;
## 1        1          0    
## 2        2          0.583
## 3        3          3.88 
## 4        4          7</code></pre>
<p>Dans le modèle ci-dessus, l’ordonnée à l’origine <code>(Intercept)</code> correspond au logit de la probabilité de mortalité pour la valeur de référence du facteur (<code>Exposure</code> = 1). Cependant, il n’y a aucune mort pour ce traitement dans les données. Puisque la fonction logistique ne peut jamais atteindre <span class="math inline">\(p = 0\)</span>, le résultat est un valeur très négative pour ce coefficient, avec une incertitude très élevée: concrètement, cela signifie que le taux de mortalité est très faible, trop faible pour bien être estimé avec des données.</p>
<p>Les autres coefficients ont des valeurs aussi grandes mais positives, simplement car ces effets sont estimés relativement au traitement de référence. Le problème ne se produit plus si on enlève les données avec <code>Exposure</code>= 1.</p>
<pre class="r"><code>mod_snails3 &lt;- glm(cbind(Deaths, N - Deaths) ~ as.factor(Exposure), 
                   data = filter(snails, Exposure &gt; 1), family = binomial)
summary(mod_snails3)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(Deaths, N - Deaths) ~ as.factor(Exposure), 
##     family = binomial, data = filter(snails, Exposure &gt; 1))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.9350  -1.1027  -0.5109   0.5317   4.1261  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)           -3.5051     0.2712 -12.922  &lt; 2e-16 ***
## as.factor(Exposure)3   2.0793     0.2948   7.053 1.75e-12 ***
## as.factor(Exposure)4   2.8861     0.2876  10.034  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 366.69  on 71  degrees of freedom
## Residual deviance: 182.38  on 69  degrees of freedom
## AIC: 347.24
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Cet exemple illustre le problème dit de <em>séparation parfaite</em>. En raison du lien logit, le modèle de régression logistique représente l’effet de paramètres qui modifient la probabilité d’une réponse de façon continue entre 0 et 1. Ce modèle ne peut pas estimer les effets d’un prédicteur s’il y a une séparation parfaite entre les cas où <span class="math inline">\(y = 0\)</span> et <span class="math inline">\(y = 1\)</span>. Pour un prédicteur catégorique, le problème survient si une catégorie contient entièrement des réponses positives ou négatives, comme dans l’exemple précédent. Pour une variable numérique, le problème survient si <span class="math inline">\(y\)</span> est toujours 0 en-dessous d’une certaine valeur de <span class="math inline">\(x\)</span> et toujours 1 au-dessus, ou vice versa.</p>
<p>Il existe des méthodes dites de réduction de biais qui pénalisent les coefficients trop élevés afin d’obtenir des estimés réalistes même dans le cas de séparation parfaite. Nous n’en discuterons pas davantage dans ce cours, mais vous pouvez trouver ces méthodes dans le package <strong>brglm</strong>.</p>
</div>
<div id="résumé" class="section level1">
<h1>Résumé</h1>
<ul>
<li><p>Un modèle linéaire généralisé est composé d’un prédicteur linéaire (combinaison linéaire des variables explicatives), d’une fonction de lien pour la réponse moyenne, et d’une distribution statistique de la réponse en fonction de sa moyenne.</p></li>
<li><p>La régression logistique sert à modéliser des réponses binaires (0 ou 1) ou binomiales (nombre de 0 et de 1). Elle utilise un lien logit et une distribution binomiale de la réponse.</p></li>
<li><p>La fonction logit transforme une probabilité entre 0 et 1 en un nombre réel entre -<span class="math inline">\(\infty\)</span> et +<span class="math inline">\(\infty\)</span>. Un logit négatif correspond à une probabilité en-dessous de 0.5, un logit positif correspond à une probabilité au-dessus de 0.5.</p></li>
<li><p>Dans une régression logistique, l’effet d’un prédicteur sur la réponse est non-linéaire et dépend de la valeur des autres prédicteurs. Il est donc utile de visualiser les prédictions du modèle pour différentes combinaisons des variables.</p></li>
<li><p>Pour une réponse binaire (0 ou 1), les résidus individuels donnent peu d’information, mais on peut vérifier l’ajustement du modèle à partir des moyennes de groupes de résidus (<em>binned residuals</em>).</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
