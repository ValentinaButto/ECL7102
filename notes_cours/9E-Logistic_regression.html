<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />



<meta name="date" content="2021-11-01" />

<title>Logistic regression</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Logistic regression</h1>
<h4 class="date"><br/>November 1, 2021</h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li><p>Know the components of a generalized linear model.</p></li>
<li><p>Use logistic regression to model a binary response (e.g. presence/absence) or a binomial response (e.g. number of presences and absences).</p></li>
<li><p>Interpret the results of a logistic regression and visualize the effects of the predictors.</p></li>
</ul>
</div>
<div id="motivation" class="section level1">
<h1>Motivation</h1>
<p>Suppose we are interested in modeling a binary categorical response, for example:</p>
<ul>
<li>survival or death of individuals within a given time interval;</li>
<li>presence or absence of a species on different sites;</li>
<li>the decision of an animal to stay at the same site or migrate to another site.</li>
</ul>
<p>Numerically, this response is coded by values of 0 or 1. In a regression context, what interests us is the mean of this response, i.e. the proportion of cases where it is equal to 1 (we could also say “the probability that the response is 1”).</p>
<p>We could model the relationship between this mean response and different predictors with a linear regression model, as seen during in the previous classes:</p>
<p><span class="math display">\[ \hat{y} = \beta_0 + \sum_{i = 1}^m \beta_i x_i \]</span></p>
<p>However, this approach poses some problems:</p>
<ul>
<li>For some predictor values, we get a predicted response <span class="math inline">\(\hat{y}\)</span> less than 0 or greater than 1, which makes no sense in this context.</li>
<li>This type of response does not satisfy the homogeneity of variance criterion of the linear regression model. The variance of individual responses is much greater when the probability is close to 0.5 than when it approaches 0 or 1.</li>
</ul>
<p>The <strong>generalized linear models</strong> (GLM) solve these two problems, allowing us to choose different transformations of the linear prediction and different distributions for the individual responses according to the mean response.</p>
<p>To better explain the principle of these models, we reformulate the multiple linear regression model, dividing this model into three components:</p>
<ul>
<li>a linear predictor (a linear combination of the explanatory variables): <span class="math inline">\(\eta = \beta_0 + \sum_{i = 1}^m \beta_i x_i\)</span>;</li>
<li>a link between the mean response and the predictor value (here, simply the equality): <span class="math inline">\(\hat{y} = \eta\)</span>; and</li>
<li>a distribution of the individual responses according to the mean response (here, the normal distribution): <span class="math inline">\(y \sim N(\hat{y}, \sigma)\)</span>.</li>
</ul>
<p>Generalized linear models retain the first element (the prediction depends on a linear combination of the <span class="math inline">\(x_i\)</span> variables), but use different link functions and different distributions for the response. The logistic regression we will see today is designed to model binary response variables, while the Poisson regression covered in the next class is designed for count data (integers <span class="math inline">\(\ge\)</span> 0).</p>
</div>
<div id="logistic-regression-model" class="section level1">
<h1>Logistic regression model</h1>
<p>Logistic regression is an example of a generalized linear model where the response corresponds to one or more observations of a binary result (0 or 1). In this model:</p>
<ul>
<li>the response follows a binomial distribution and</li>
<li>the logit function links the probability of a positive result (<span class="math inline">\(y = 1\)</span>) to the value of the linear predictor.</li>
</ul>
<div id="binomial-distribution" class="section level2">
<h2>Binomial distribution</h2>
<p>Consider <span class="math inline">\(n\)</span> independent repetitions of an experiment that can result in the presence (1) or absence (0) of an event, with the same probability of presence <span class="math inline">\(p\)</span> for each repetition. In this case, the number of presences <span class="math inline">\(y\)</span> follows a binomial distribution: <span class="math inline">\(y \sim B(n, p)\)</span>. According to this distribution, the probability of obtaining <span class="math inline">\(y\)</span> presences and <span class="math inline">\(n-y\)</span> absences is:</p>
<p><span class="math display">\[ \frac{n!}{y!(n-y)!} p^y (1-p)^{n-y}  = \binom{n}{y} p^y(1-p)^{n-y} \]</span></p>
<p>To understand this formula, take for example the result sequence 01001 (<span class="math inline">\(n = 5\)</span> and <span class="math inline">\(y = 2\)</span>). The probability of getting 0 on the first try is <span class="math inline">\(1-p\)</span>, the probability of getting 1 on the second try is <span class="math inline">\(p\)</span>, and so on. The probability of a sequence of independent tries is the product of the probabilities of each try, therefore <span class="math inline">\(p^2 (1-p)^3\)</span>, which corresponds to the <span class="math inline">\(p^y (1-p)^{n-y}\)</span> of the binomial distribution formula. However, this is the probability of a single sequence of two 1s and three 0s. Each of the other possible sequences (11000, 00110, etc.) has the same probability. This number of possible sequences is equal to <span class="math inline">\(n! / y! (n-y)!\)</span>, where <span class="math inline">\(!\)</span> represents the factorial operation.</p>
<p>The mean <em>number</em> of presences is <span class="math inline">\(np\)</span>, with a variance equal to <span class="math inline">\(np(1-p)\)</span>. The mean <em>proportion</em> of presences is <span class="math inline">\(p\)</span> with a variance of <span class="math inline">\(p(1-p)/n\)</span>.</p>
<p>The binomial distribution with a single trial (<span class="math inline">\(n = 1\)</span>) is called the Bernoulli distribution, with a mean of <span class="math inline">\(p\)</span> and a variance equal to <span class="math inline">\(p(1-p)\)</span>.</p>
<p>In R, the functions for calculating quantities from the binomial distribution have the <code>binom</code> suffix. For example, the probability of obtaining 2 presences over 5 trials, with a probability of presence of 0.3, is equal to:</p>
<pre class="r"><code>dbinom(2, 5, 0.3)</code></pre>
<pre><code>## [1] 0.3087</code></pre>
<p>Check the help topic <code>?Binomial</code> for more details.</p>
</div>
<div id="logit-link" class="section level2">
<h2>Logit link</h2>
<p>To link the probability of presence <span class="math inline">\(p\)</span> of the binomial distribution to a linear predictor <span class="math inline">\(\eta = \beta_0 + \sum_{i = 1}^m \beta_i x_i\)</span>, the most commonly used link is the logit function.</p>
<p><span class="math display">\[ \eta = \text{logit}(p) = \log \left( \frac{p}{1-p} \right) \]</span></p>
<p>By inverting this equation, we obtain an expression for <span class="math inline">\(p\)</span> as a function of <span class="math inline">\(\eta\)</span>.</p>
<p><span class="math display">\[ p = \frac{1}{1 + e^{-\eta}} \]</span></p>
<p>This last equation is often called the <em>logistic function</em>. Here is its graphical representation:</p>
<p><img src="9E-Logistic_regression_files/figure-html/unnamed-chunk-2-1.png" width="384" /></p>
<p>Note some important properties of this function:</p>
<ul>
<li>If <span class="math inline">\(\eta = 0\)</span>, <span class="math inline">\(p = 0.5\)</span> (50% probability).</li>
<li>The function is symmetrical about its central point: <span class="math inline">\(p(-\eta) = 1 - p(\eta)\)</span>.</li>
<li>The probability approaches 0 when <span class="math inline">\(\eta\)</span> takes very negative values and approaches 1 when <span class="math inline">\(\eta\)</span> takes very positive values, without ever reaching those extremes.</li>
<li>The slope of the curve is more pronounced in the center and flatter (tending towards 0) at the ends. From a regression point of view, this means that the effect of the explanatory variables (contained in <span class="math inline">\(\eta\)</span>) on the probability of presence is greater when that probability is close to 50%.</li>
</ul>
<p>In summary, the logistic regression is based on the following model:</p>
<ul>
<li><span class="math inline">\(y \sim B(n, p)\)</span> (binomial distribution of the response)</li>
<li><span class="math inline">\(\text{logit}(p) = \beta_0 + \sum_{i = 1}^m \beta_i x_i\)</span> or equivalently:</li>
</ul>
<p><span class="math display">\[p = \frac{1}{1 + e^{-(\beta_0 + \sum_{i = 1}^m \beta_i x_i)}}\]</span></p>
<p>Unlike linear regression, the <span class="math inline">\(\beta\)</span> coefficients of a generalized linear model are not estimated by the least squares method, but rather by the more general <em>maximum likelihood</em> method. In short, this method chooses the values of the <span class="math inline">\(\beta\)</span> coefficients that maximize the joint probability of all the observations of <span class="math inline">\(y\)</span>.</p>
</div>
<div id="interpreting-the-coefficients" class="section level2">
<h2>Interpreting the coefficients</h2>
<p>The graph below shows the probability <span class="math inline">\(p\)</span> for a logistic regression model with one predictor <span class="math inline">\(x\)</span>, with coefficients <span class="math inline">\(\beta_0 = -1\)</span> and <span class="math inline">\(\beta_1\)</span> = 0.4, i.e. <span class="math inline">\(\text{logit}(p) = -1 + 0.4x\)</span>.</p>
<p><img src="9E-Logistic_regression_files/figure-html/unnamed-chunk-3-1.png" width="384" /></p>
<p>The intercept <span class="math inline">\(\beta_0\)</span> is equal to logit(<span class="math inline">\(p\)</span>) when <span class="math inline">\(x = 0\)</span>. To calculate the value of <span class="math inline">\(p\)</span> corresponding to a given value of logit(<span class="math inline">\(p\)</span>), we can use the <code>plogis</code> function in R (which corresponds to the logistic function, also called “inverse logit”), whereas the logit transform itself is done by the <code>qlogis</code> function.</p>
<pre class="r"><code>plogis(-1)</code></pre>
<pre><code>## [1] 0.2689414</code></pre>
<pre class="r"><code>qlogis(0.5)</code></pre>
<pre><code>## [1] 0</code></pre>
<p>The value of <span class="math inline">\(x\)</span> for which the linear predictor is zero (here, <span class="math inline">\(-1 + 0.4x = 0\)</span> gives <span class="math inline">\(x = 2.5\)</span>) is associated with a probability <span class="math inline">\(p = 0.5\)</span>. It is at this point of the curve that the slope of <span class="math inline">\(p\)</span> vs. <span class="math inline">\(x\)</span> is maximal: this maximum slope, indicated by a blue line in the graph, is equal to <span class="math inline">\(\beta_1 / 4\)</span>, where <span class="math inline">\(\beta_1\)</span> is the coefficient of <span class="math inline">\(x\)</span> in the linear predictor. Here, since <span class="math inline">\(\beta_1 = 0.4\)</span>, we can say that when <span class="math inline">\(x\)</span> increases by 1, the probability <span class="math inline">\(p\)</span> increases by a maximum of 0.1 (or 10%).</p>
</div>
</div>
<div id="logistic-regression-with-r" class="section level1">
<h1>Logistic regression with R</h1>
<p>In R, logistic regression can be used for two types of responses:</p>
<ul>
<li>a binary variable encoded by logical values (FALSE, TRUE), numerical values (0, 1) or a factor (the first level of the factor corresponds to 0, the others to 1);</li>
<li>a binomial variable described by two columns (number of presences, number of absences).</li>
</ul>
<p>We will see an example of each case in this class.</p>
<div id="example-arsenic-concentrations-in-wells-in-bangladesh" class="section level2">
<h2>Example: Arsenic concentrations in wells in Bangladesh</h2>
<p>The <code>Wells</code> data frame from the <strong>carData</strong> package contains data from a survey of 3020 households in Bangladesh. The wells used by these households had an arsenic concentration (<code>arsenic</code>, in multiples of 100 <span class="math inline">\(\mu g / L\)</span>) higher than the level considered safe. The <code>switch</code> binary response indicates whether the household has changed wells. In addition to the arsenic concentration, the table contains other predictors, including the distance to the nearest safe well (<code>distance</code> in meters).</p>
<p>This example is from the textbook by Gelman and Hill, <em>Data Analysis Using Regression and Multilevel / Hierarchical Models</em>.</p>
<pre class="r"><code>library(carData)
str(Wells)</code></pre>
<pre><code>## &#39;data.frame&#39;:    3020 obs. of  5 variables:
##  $ switch     : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 2 2 2 2 2 2 2 ...
##  $ arsenic    : num  2.36 0.71 2.07 1.15 1.1 3.9 2.97 3.24 3.28 2.52 ...
##  $ distance   : num  16.8 47.3 21 21.5 40.9 ...
##  $ education  : int  0 0 10 12 14 9 4 10 0 0 ...
##  $ association: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 2 2 2 1 2 2 ...</code></pre>
<p>When exploring that type of dataset, it can be useful to compare the distribution of potential explanatory variables for positive and negative values of the response.</p>
<p><img src="9E-Logistic_regression_files/figure-html/unnamed-chunk-6-1.png" width="480" /></p>
<p>On average, households that change wells have higher arsenic concentrations in their original wells and are closer to a safe well. However, neither variable seems to be strongly associated with the response.</p>
<p><strong>Note</strong>: Given the asymmetric distribution of the arsenic and distance variables, a logarithmic transformation of these variables would be justifiable. For this particular problem, this transformation does not really improve the fit of the model, so we keep the original scale of variables, which is easier to interpret.</p>
<p>The <code>glm</code> function is used to estimate generalized linear model parameters. As for linear models, we first specify a formula and a data frame. Then, we must indicate the distribution of the response with the argument <code>family</code>, as well as the link function (<code>link</code>). For logistic regression, it is a binomial distribution with a logit link. Note that since the logit is the default link function for a binomial response, specifying it is optional (<code>family = binomial</code> would be sufficient).</p>
<pre class="r"><code>mod &lt;- glm(switch ~ arsenic + distance, data = Wells,
           family = binomial(link = &quot;logit&quot;))
summary(mod)</code></pre>
<pre><code>## 
## Call:
## glm(formula = switch ~ arsenic + distance, family = binomial(link = &quot;logit&quot;), 
##     data = Wells)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6351  -1.2139   0.7786   1.0702   1.7085  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.002749   0.079448   0.035    0.972    
## arsenic      0.460775   0.041385  11.134   &lt;2e-16 ***
## distance    -0.008966   0.001043  -8.593   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 4118.1  on 3019  degrees of freedom
## Residual deviance: 3930.7  on 3017  degrees of freedom
## AIC: 3936.7
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The intercept is the logit of the probability of changing wells for an arsenic concentration and a distance both equal to zero. This probability is given by <code>invlogit (0.0027)</code> which is about 50%. However, since the model has been fitted to data where the arsenic concentration is always greater than 0.5, the intercept does not really have a meaning.</p>
<p>The predictor coefficients indicate that an increase in arsenic concentration of one unit (100 <span class="math inline">\(\mu g / L\)</span>) results in an increase of ~0.12 in the probability of changing wells (0.46 / 4), while an increase of 1 m in the distance results in a maximum decrease of 0.0022 (0.0087 / 4) for this probability (equivalent to a decrease of ~0.22 per 100m of distance).</p>
</div>
<div id="visualize-model-predictions" class="section level2">
<h2>Visualize model predictions</h2>
<p>To better understand the non-linear effect of predictors, we can visualize model predictions for different combinations of arsenic concentration and distance.</p>
<p>Let’s first create a new prediction data frame, containing combinations of 10 arsenic concentrations (distributed between 0.5 and 5) and 3 distances to the nearest safe well (50 m, 100 m and 200 m). We use the <code>expand.grid</code> function to do this.</p>
<pre class="r"><code>wells_nouv &lt;- expand.grid(arsenic = seq(0.5, 5, 0.5), distance = c(50, 100, 200))</code></pre>
<p>Let’s see what happens if we apply the <code>predict</code> function to the model with these new data.</p>
<pre class="r"><code>wells_nouv$pred &lt;- predict(mod, wells_nouv)
summary(wells_nouv$pred)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -1.5602 -0.3600  0.2518  0.2238  0.8823  1.8583</code></pre>
<p>These values are not between 0 and 1 because by default, <code>predict</code> returns the values of the linear predictor <span class="math inline">\(\eta\)</span> (corresponding to the <code>type = "link"</code> argument). To obtain the predictions on the scale of the response (here, probabilities), we must specify <code>type = "response"</code>.</p>
<pre class="r"><code>wells_nouv$prob_pred &lt;- predict(mod, wells_nouv, type = &quot;response&quot;)

ggplot(wells_nouv, aes(x = arsenic, y = prob_pred, color = as.factor(distance))) +
    geom_line() +
    geom_hline(yintercept = 0.5, linetype = &quot;dotted&quot;) +
    scale_color_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="9E-Logistic_regression_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>We could note, for example, that the more the distance to a safe well increases, the higher the concentration must be for a household to change wells a majority of the time (dashed line at <span class="math inline">\(p = 0.5\)</span>).</p>
<p>To visualize the confidence interval of these probabilities, we need to go back to the scale of the linear predictor <span class="math inline">\(\eta\)</span> rather than that of the response, get the standard errors of the <span class="math inline">\(\eta\)</span> values with the <code>se.fit = TRUE</code> option, compute an interval of 1.96 standard errors on each side of <span class="math inline">\(\eta\)</span>, and finally convert <span class="math inline">\(\eta\)</span> and the bounds of the interval into values of <span class="math inline">\(p\)</span> with the function <code>plogis</code>.</p>
<pre class="r"><code>prob_pred &lt;- predict(mod, wells_nouv, se.fit = TRUE)
wells_nouv$prob_pred &lt;- prob_pred$fit
wells_nouv$prob_se &lt;- prob_pred$se.fit

ggplot(wells_nouv, aes(x = arsenic, y = plogis(prob_pred), color = as.factor(distance),
                       fill = as.factor(distance))) +
    geom_ribbon(aes(ymin = plogis(prob_pred - 1.96*prob_se), 
                    ymax = plogis(prob_pred + 1.96*prob_se)), alpha = 0.3) +
    geom_line() +
    scale_color_brewer(palette = &quot;Dark2&quot;) +
    scale_fill_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="9E-Logistic_regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The reason we must first calculate the confidence interval on the scale of the linear predictor is that its uncertainty is closer to being normally distributed, whereas the uncertainty on <span class="math inline">\(p\)</span> is not.</p>
<p>Note that a prediction interval for individual observations would be less interesting here, since these observations are always 0 or 1.</p>
</div>
<div id="checking-the-model-fit" class="section level2">
<h2>Checking the model fit</h2>
<p>Diagnostic graphs based on individual residuals are not very useful when the response is binary, as you can see by calling the <code>plot(mod)</code> function.</p>
<p><img src="9E-Logistic_regression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>For each predicted value (here, the predictions are represented on the scale of the linear predictor, not that of <span class="math inline">\(p\)</span>), there are only two possible values for the residual: one is positive for points with <span class="math inline">\(y = 1\)</span>, the other is negative if <span class="math inline">\(y = 0\)</span>. This is why we see two lines of points on the graph above.</p>
<p>A better strategy would be to group the points with similar predicted probabilities, then calculate the residual corresponding to the difference between (1) the proportion of positive responses observed among the points of a group and (2) the mean predicted value for those points. For example, for a group of 20 points with 11 positive responses and a mean prediction of 0.6, the residual would be -0.05 (11/20 - 0.6). The <code>binnedplot</code> function of the <strong>arm</strong> package is used to create such a binned residual plot.</p>
<pre class="r"><code>library(arm)
binnedplot(fitted(mod), residuals(mod, type = &quot;response&quot;))</code></pre>
<p><img src="9E-Logistic_regression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p><strong>Note</strong>: There are different definitions of residuals for a generalized linear model, so you need to specify which type to use in the <code>residuals</code> function. For this graph, we use the residuals on the response scale (<code>type = "response"</code>), which are the differences between observed and predicted responses (<span class="math inline">\(y - \hat{y}\)</span>).</p>
<p>By default, <code>binnedplot</code> chooses the number of groups based on a compromise to have enough points per group (so that the mean response is precise) and enough groups (to see the trend if there is one). When the number of cases <span class="math inline">\(n &gt; 100\)</span>, the number of groups chosen is about <span class="math inline">\(\sqrt{n}\)</span>.</p>
<p>The graph produced by <code>binnedplot</code> also indicates a 95% prediction interval (gray lines) for the averaged residuals. Thus, if the binomial model is good, about 95% of the residuals should be within this range. Here, this is the case of 52 residuals out of 54, or 96%.</p>
</div>
<div id="coefficient-of-determination" class="section level2">
<h2>Coefficient of determination</h2>
<p>The estimation of generalized linear model parameters is not based on the least squares method. For this reason, the <span class="math inline">\(R^2\)</span> based on the sum of the square deviations is not a good measure of fit for this type of model.</p>
<p>The <strong>deviance</strong> is a measure of the deviation of observed values from the expected values, which is calculated from the likelihood <span class="math inline">\(L\)</span> of the fitted model.</p>
<p><span class="math display">\[ D = -2 \log L \]</span></p>
<p>This expression is also equal to the first term in the AIC. The higher the probability of the model, the larger <span class="math inline">\(L\)</span> and the smaller the deviance. As with AIC, the absolute value of the deviance is meaningless, but this metric is useful for comparing the fit of different models.</p>
<p>In the summary result of <code>glm</code>, the deviance of the fitted model is indicated as <code>Residual Deviance</code>. The summary also includes another value, <code>Null Deviance</code>, which corresponds to the deviance of the null model with no predictor. These two values play a role similar to the sum of the residual square deviations and the sum of the total square deviations in the linear model. We can therefore define the <em>pseudo-R<span class="math inline">\(^2\)</span></em> (or McFadden’s <span class="math inline">\(R^2\)</span>) as the fraction of the deviance of the null model explained by the fitted model.</p>
<pre class="r"><code>pseudo_R2 &lt;- 1 - mod$deviance/mod$null.deviance
pseudo_R2</code></pre>
<pre><code>## [1] 0.04551395</code></pre>
<p>Arsenic concentration and distance, despite their significant effect, therefore do not explain much of the decision to change wells or not. This is consistent with our initial exploration of the data.</p>
<p>The deviance-based <span class="math inline">\(R^2\)</span> applies to all models fitted with maximum likelihood. For logistic regression specifically, another version of the coefficient of determination was proposed by Tjur:</p>
<p><span class="math display">\[ {R^2}_{\text{Tjur}} = \bar{\hat{y}}_{(y=1)} - \bar{\hat{y}}_{(y=0)} \]</span></p>
<p>In other words, Tjur’s <span class="math inline">\(R^2\)</span> measures the difference between the predicted mean response for the cases where the observed response is 1 and the predicted mean response for the cases where the observed response is 0. It therefore indicates how the model can “separate” the two groups <span class="math inline">\(y = 1\)</span> and <span class="math inline">\(y = 0\)</span>. In the extreme cases, a coefficient of 0 indicates that the model predicts the same mean response for both groups, while a coefficient of 1 indicates that the model predicts with certainty the correct response for all observations.</p>
<p>For our example, Tjur’s <span class="math inline">\(R^2\)</span> is about 0.06.</p>
<pre class="r"><code>r2_tjur &lt;- mean(mod$fitted.values[mod$y == 1]) - mean(mod$fitted.values[mod$y == 0])
r2_tjur</code></pre>
<pre><code>## [1] 0.06004311</code></pre>
</div>
</div>
<div id="binomial-logistic-regression" class="section level1">
<h1>Binomial logistic regression</h1>
<p>In the previous example, each row in the table corresponded to a single binary response (0 or 1) and the associated values of the predictors. In an experimental context, it is possible to have several independent replicates with the same values of the predictors; the number of responses equal to 1 among these <span class="math inline">\(N\)</span> replicates can be modeled directly as a binomial variable.</p>
<div id="example-mortality-of-snails-depending-on-the-environment" class="section level2">
<h2>Example: Mortality of snails depending on the environment</h2>
<p>The <code>snails</code> data table in the <strong>MASS</strong> package presents the results of an experiment where snails of 2 species (<code>Species</code>) were exposed to 3 different temperature values (<code>Temp</code>) and 4 relative humidity values (<code>Rel.Hum</code>) for 1 to 4 weeks (<code>Exposure</code>). Twenty snails (<code>N</code>) were followed for each of the 96 possible combinations of these four variables; the <code>Deaths</code> variable indicates how many of them died during the experiment.</p>
<pre class="r"><code>library(MASS)
str(snails)</code></pre>
<pre><code>## &#39;data.frame&#39;:    96 obs. of  6 variables:
##  $ Species : Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Exposure: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ Rel.Hum : num  60 60 60 65.8 65.8 65.8 70.5 70.5 70.5 75.8 ...
##  $ Temp    : int  10 15 20 10 15 20 10 15 20 10 ...
##  $ Deaths  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ N       : int  20 20 20 20 20 20 20 20 20 20 ...</code></pre>
<p>In this example, we will code mortality as 1 and its absence (survival) as 0. To apply a binomial logistic regression to these data, the response in the formula (to the left of the <code>~</code>) must be composed of two columns (number of 1 and number of 0) grouped with <code>cbind</code>. Here, the variable representing the number of 0 (the survivors) is obtained by subtracting <code>Deaths</code> from <code>N</code>.</p>
<pre class="r"><code>mod_snails &lt;- glm(cbind(Deaths, N - Deaths) ~ Species + Exposure + Rel.Hum + Temp,
           data = snails, family = binomial)
summary(mod_snails)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(Deaths, N - Deaths) ~ Species + Exposure + 
##     Rel.Hum + Temp, family = binomial, data = snails)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7261  -0.7107  -0.3362   0.4231   1.7510  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.40495    0.97070  -1.447    0.148    
## SpeciesB     1.30864    0.16350   8.004 1.20e-15 ***
## Exposure     1.50339    0.10235  14.689  &lt; 2e-16 ***
## Rel.Hum     -0.10684    0.01388  -7.699 1.37e-14 ***
## Temp         0.09404    0.01927   4.881 1.06e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 539.72  on 95  degrees of freedom
## Residual deviance:  55.07  on 91  degrees of freedom
## AIC: 223.93
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>We first note that the model accounts for much of the variation in the mortality rate between groups (McFadden’s pseudo-R<span class="math inline">\(^2\)</span> equals 0.9). Since each “observation” (row) is a summary of 20 individuals, the random variation of the response is less than when considering individual observations; it is thus not surprising that the pseudo-R<span class="math inline">\(^2\)</span> is higher in this case.</p>
<p>According to the sign of the coefficients, mortality is greater for species B than species A. This mortality also increases with time of exposure and temperature, but decreases with a rise in relative humidity (in the range of values considered).</p>
<p>With several observations per row, individual residuals contain more information, so conventional diagnostic charts are more useful than in the previous example. However, almost half of the rows have no dead snail (<code>Deaths</code> = 0) and these zeros are the cause of the “line” of residuals in some graphs.</p>
<p><img src="9E-Logistic_regression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Since the response is not supposed to follow a normal distribution, we are not really interested in the quantile-quantile plot. The <em>Residuals vs Fitted</em> plot allows us to check for a trend in the residuals and the <em>Residuals vs Leverage</em> plot allows us to detect points with a great influence on the regression. Note that three of the graphs use Pearson residuals (Std. Pearson resid.), which correspond to the residuals divided by the expected standard deviation of the response (remember that this standard deviation depends on the expected value of the response here). These standardized residuals should therefore have a more homogeneous variance than the raw residuals.</p>
</div>
</div>
<div id="perfect-separation-problem" class="section level1">
<h1>Perfect separation problem</h1>
<p>Using the same <code>snails</code> data set, let’s now try to estimate the effect of exposure time as a categorical variable. This amounts to separately estimating the mortality rate after 1, 2, 3 and 4 weeks.</p>
<pre class="r"><code>mod_snails2 &lt;- glm(cbind(Deaths, N - Deaths) ~ as.factor(Exposure), data = snails,
                   family = binomial)
summary(mod_snails2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(Deaths, N - Deaths) ~ as.factor(Exposure), 
##     family = binomial, data = snails)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.9350  -1.0881  -0.0002   0.5037   4.1261  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)            -20.75     888.02  -0.023    0.981
## as.factor(Exposure)2    17.25     888.02   0.019    0.985
## as.factor(Exposure)3    19.33     888.02   0.022    0.983
## as.factor(Exposure)4    20.13     888.02   0.023    0.982
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 539.72  on 95  degrees of freedom
## Residual deviance: 182.38  on 92  degrees of freedom
## AIC: 349.24
## 
## Number of Fisher Scoring iterations: 17</code></pre>
<p>Why do we have such high coefficients and standard errors? Let’s look at the average number of dead snails for each of the values of the <code>Exposure</code> variable.</p>
<pre class="r"><code>group_by(snails, Exposure) %&gt;%
    summarize(mean(Deaths))</code></pre>
<pre><code>## # A tibble: 4 x 2
##   Exposure `mean(Deaths)`
##      &lt;int&gt;          &lt;dbl&gt;
## 1        1          0    
## 2        2          0.583
## 3        3          3.88 
## 4        4          7</code></pre>
<p>In the model above, the intercept corresponds to the logit of the mortality rate for the reference factor value (<code>Exposure</code> = 1). However, there are no deaths for this treatment in the data. Since the logistic function can never reach <span class="math inline">\(p = 0\)</span>, the result is a very negative value for this coefficient, with a very high uncertainty: concretely, this means that the mortality rate is very low, too low to be well estimated with data.</p>
<p>The other coefficients have such large but positive values, simply because these effects are estimated relative to the reference treatment. The problem no longer occurs if you remove the data with <code>Exposure</code> = 1.</p>
<pre class="r"><code>mod_snails3 &lt;- glm(cbind(Deaths, N - Deaths) ~ as.factor(Exposure), 
                   data = filter(snails, Exposure &gt; 1), family = binomial)
summary(mod_snails3)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(Deaths, N - Deaths) ~ as.factor(Exposure), 
##     family = binomial, data = filter(snails, Exposure &gt; 1))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.9350  -1.1027  -0.5109   0.5317   4.1261  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)           -3.5051     0.2712 -12.922  &lt; 2e-16 ***
## as.factor(Exposure)3   2.0793     0.2948   7.053 1.75e-12 ***
## as.factor(Exposure)4   2.8861     0.2876  10.034  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 366.69  on 71  degrees of freedom
## Residual deviance: 182.38  on 69  degrees of freedom
## AIC: 347.24
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>This example illustrates the so-called “perfect separation” problem. Due to the logit link, the logistic regression model represents the effect of parameters that change the probability of a response gradually between 0 and 1. This model cannot estimate the effects of a predictor if there is a perfect separation between the cases where <span class="math inline">\(y = 0\)</span> and <span class="math inline">\(y = 1\)</span>. For a categorical predictor, the problem occurs if a category contains entirely positive or negative responses, as in the previous example. For a numeric variable, the problem occurs if <span class="math inline">\(y\)</span> is always 0 below a certain value of <span class="math inline">\(x\)</span> and always 1 above, or vice versa.</p>
<p>There are so-called bias reduction methods that penalize coefficients that are too high in order to obtain realistic estimates even in the case of perfect separation. We will not discuss these further in this course, but you can find these methods in the <strong>brglm</strong> package.</p>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>A generalized linear model consists of a linear predictor (linear combination of explanatory variables), a link function for the mean response, and a statistical distribution of the response as a function of its mean.</p></li>
<li><p>Logistic regression is used to model a binary (0 or 1) or binomial (number of 0s and 1s) response variable. It uses a logit link and a binomial distribution of the response.</p></li>
<li><p>The logit function transforms a probability between 0 and 1 into a real number between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(+\infty\)</span>. A negative logit corresponds to a probability below 0.5, a positive logit corresponds to a probability above 0.5.</p></li>
<li><p>In a logistic regression, the effect of a predictor on the response is non-linear and depends on the value of the other predictors. It is therefore useful to visualize model predictions for different combinations of variables.</p></li>
<li><p>For a binary response (0 or 1), individual residuals give little information, but we can check model fit with a binned residual plot.</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
