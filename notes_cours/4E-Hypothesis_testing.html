<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="date" content="2019-09-16" />

<title>Hypothesis testing</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Hypothesis testing</h1>
<h4 class="date"><br/>September 16, 2019</h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Describe the general operation of a statistical hypothesis test.</li>
<li>Define concepts related to the accuracy of a test: significance threshold, power, type I and II errors.</li>
<li>Use the <span class="math inline">\(t\)</span>-test to compare the means of two independent or paired samples.</li>
</ul>
<div id="comparing-a-sample-mean-to-a-reference-value" class="section level2">
<h2>Comparing a sample mean to a reference value</h2>
<p>In this example, we want to verify the absence of bias of a soil moisture sensor, by measuring the relative humidity of the soil at 9 points in a 1-m<span class="math inline">\(^2\)</span> plot. With a more accurate sensor, we have determined that the mean humidity in this plot is 50%. Here are the 9 values obtained with the device to be tested, their mean and the standard error of that mean.</p>
<pre class="r"><code>humidite &lt;- c(47, 50, 48, 50, 54, 49, 56, 52, 51)
humid_moy &lt;- mean(humidite)
humid_et &lt;- sd(humidite) / sqrt(length(humidite))
paste(&quot;Mean of&quot;, round(humid_moy, 2), &quot;and standard error of&quot;, round(humid_et, 2))</code></pre>
<pre><code>## [1] &quot;Mean of 50.78 and standard error of 0.95&quot;</code></pre>
<p>Suppose these measures follow a normal distribution. If the sensor were not biased (<span class="math inline">\(\mu\)</span> = 50), what would be the probability that the sample mean <span class="math inline">\(\bar{x}\)</span> would be this far from the reference value?</p>
<p>During the class on confidence intervals, we saw that the difference between <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\mu\)</span>, divided by the standard error, follows a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n - 1\)</span> degrees of freedom:</p>
<p><span class="math display">\[ t_{n-1} = \frac{\bar{x} - \mu}{s / \sqrt{n}} \]</span></p>
<p>Recall that the <span class="math inline">\(t\)</span> distribution replaces the standard normal distribution when the standard deviation <span class="math inline">\(s\)</span> is estimated from the sample.</p>
<p>The <span class="math inline">\(t\)</span> statistic for this sample, if <span class="math inline">\(\mu\)</span> = 50, is equal to:</p>
<pre class="r"><code>humid_t &lt;- (humid_moy - 50) / humid_et
humid_t</code></pre>
<pre><code>## [1] 0.8151115</code></pre>
<p>The cumulative <span class="math inline">\(t\)</span> distribution (<code>pt</code> function in R) gives us the probability of observing a value smaller or equal to a given value. In this case, the probability of obtaining a value of the <span class="math inline">\(t\)</span> statistic greater than the one observed for our sample, if <span class="math inline">\(\mu\)</span> = 50, is calculated as:</p>
<pre class="r"><code>1 - pt(humid_t, df = 8)</code></pre>
<pre><code>## [1] 0.2192996</code></pre>
<p>This probability (21.9%) matches the filled fraction of the area under the curve in the graph below:</p>
<p><img src="4E-Hypothesis_testing_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Since our question was: “What is the probability of the sample mean being that far from the reference value?”, we must also consider the probability of obtaining a negative difference greater than the observed positive difference, i.e. a value of the <span class="math inline">\(t\)</span> statistic less than -0.815. Since the <span class="math inline">\(t\)</span> distribution is symmetrical, this probability is also 21.9%, so the probability of the mean being that far from the reference point is 43.8%, as shown in the graph below.</p>
<p><img src="4E-Hypothesis_testing_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Thus, the difference observed between the sample mean and the expected value (<span class="math inline">\(\mu\)</span> = 50) is very likely given the variance of the measurements. In the context of hypothesis testing, the probability of obtaining a more extreme result than the one observed is called the <strong><em>p</em>-value</strong>. In this case, the <em>p</em>-value associated with the hypothesis <span class="math inline">\(\mu\)</span> = 50 is 0.438.</p>
<p>In the next sections, we will see more formally the elements of a statistical hypothesis test.</p>
</div>
</div>
<div id="statistical-hypothesis-tests" class="section level1">
<h1>Statistical hypothesis tests</h1>
<p>Generally, a statistical hypothesis test aims to determine whether the variation observed in a sample of data is consistent with a “default” model (the null hypothesis), or whether observations are so improbable under this null hypothesis that it must be rejected in favor of an alternative hypothesis.</p>
<div id="null-hypothesis-and-alternative-hypothesis" class="section level2">
<h2>Null hypothesis and alternative hypothesis</h2>
<p>The null hypothesis takes its name from the fact that it often corresponds to a lack of effect: no difference between two treatments, no correlation between two variables, etc.</p>
<p>In our previous example, the null hypothesis (<span class="math inline">\(H_0\)</span>) corresponded to the absence of bias of the sensor (<span class="math inline">\(\mu = 50\)</span>). The alternative hypothesis (<span class="math inline">\(H_a\)</span>) is the logical negation of the null hypothesis, so <span class="math inline">\(\mu \neq 50\)</span>.</p>
<div id="exercise" class="section level3">
<h3>Exercise</h3>
<p>What is the null hypothesis corresponding to each of the following alternative hypotheses?</p>
<ul>
<li><p>The density of fir seedlings varies according to the slope in a plot.</p></li>
<li><p>This new insecticide is more effective than the existing treatment for emerald ash borer.</p></li>
</ul>
</div>
</div>
<div id="one-sided-or-two-sided-test" class="section level2">
<h2>One-sided or two-sided test</h2>
<p>The second example in the exercise is a one-sided test. If <span class="math inline">\(\mu_T\)</span> is the effect of the new product and <span class="math inline">\(\mu_R\)</span> that of the reference product, then the null hypothesis is <span class="math inline">\(\mu_T \leq \mu_R\)</span> and the alternative hypothesis is <span class="math inline">\(\mu_T &gt; \mu_R\)</span>.</p>
<p>In our first example, we wanted to test the presence of a positive or negative bias in the measurements of the soil moisture sensor. This is a two-sided test, where the alternative hypothesis <span class="math inline">\(\mu \neq 50\)</span> is equivalent to the union of two one-sided hypotheses (<span class="math inline">\(\mu &lt; 50\)</span> or <span class="math inline">\(\mu &gt; 50\)</span>).</p>
<p>The choice of a one-sided or two-sided test must be made in advance and depends on the question of interest.</p>
<div id="exercise-1" class="section level3">
<h3>Exercise</h3>
<p>What would be an example of a null hypothesis and alternative hypothesis in your field of research?</p>
</div>
</div>
<div id="scientific-hypothesis-and-statistical-hypothesis" class="section level2">
<h2>Scientific hypothesis and statistical hypothesis</h2>
<p>In the context of the evaluation of experimental treatments (in medicine, agronomy or forestry, etc.), the null hypothesis generally corresponds to the absence of an effect of the studied treatment compared to a reference treatment or control group. In this case, the alternative hypothesis corresponds to the scientific hypothesis which really interests the researchers. It is hypothesized that the treatment has an effect, so we check if this effect can be detected (that is, if the null hypothesis is rejected by the experiment).</p>
<p>In other contexts, the null hypothesis is based on the predictions of a model that one wishes to test. For example, do the rainy days observed in a year correspond to the precipitation probabilities predicted by the meteorological models? In this case, rejection of the null hypothesis means that the observations are incompatible with the model and the model needs to be improved.</p>
</div>
<div id="elements-of-a-hypothesis-test" class="section level2">
<h2>Elements of a hypothesis test</h2>
<p>From a given null hypothesis, the construction of a statistical test requires three main elements:</p>
<ul>
<li>a statistic that measures the deviation of observations from the null hypothesis;</li>
<li>the distribution of that statistic under the null hypothesis; and</li>
<li>a significance threshold.</li>
</ul>
<p>In the example we saw at the beginning of this class, we calculated the <span class="math inline">\(t\)</span> statistic for which we know the theoretical distribution, which allowed us to determine a <span class="math inline">\(p\)</span> value, which is the probability of getting a deviation as large or greater than the one observed, if the null hypothesis was true.</p>
<p>The significance threshold (<span class="math inline">\(\alpha\)</span>) is a probability that is considered small enough to reject the null hypothesis if <span class="math inline">\(p \leq \alpha\)</span>. For historical reasons, the threshold most often used in is <span class="math inline">\(\alpha = 0.05\)</span>. This corresponds to a 5% probability of erroneous rejection of the null hypothesis.</p>
<p>The <span class="math inline">\(\alpha\)</span> threshold must be chosen before data analysis.</p>
</div>
<div id="one-sided-or-two-sided-test-1" class="section level2">
<h2>One-sided or two-sided test</h2>
<p>For a two-sided test, we reject a fraction <span class="math inline">\(\alpha / 2\)</span> from each end of the distribution (as for a confidence interval). For a one-sided test, we reject a fraction <span class="math inline">\(\alpha\)</span> from one end of the distribution. Here is an illustration of the two cases with <span class="math inline">\(\alpha = 0.05\)</span>.</p>
<p><img src="4E-Hypothesis_testing_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
<div id="types-of-errors-and-power-of-a-test" class="section level1">
<h1>Types of errors and power of a test</h1>
<div id="type-i-and-type-ii-errors" class="section level2">
<h2>Type I and Type II errors</h2>
<p>Here are the four possible scenarios depending if <span class="math inline">\(H_0\)</span> is true or false and if we reject it or not:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(H_0\)</span> is not rejected</th>
<th><span class="math inline">\(H_0\)</span> is rejected</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(H_0\)</span> is true</td>
<td>correct decision</td>
<td>type I error</td>
</tr>
<tr class="even">
<td><span class="math inline">\(H_0\)</span> is false</td>
<td>type II error</td>
<td>correct decision</td>
</tr>
</tbody>
</table>
<p><em>Note</em>: The truth of <span class="math inline">\(H_0\)</span> for a null hypothesis of the type <span class="math inline">\(\mu = 0\)</span> is somewhat of an abstraction. In practice, the difference between two treatments can be very small, but will never be perfectly zero. Nevertheless, one can conceive a null hypothesis that is true up to a certain precision.</p>
<p>A <strong>type I</strong> error occurs when <span class="math inline">\(H_0\)</span> is rejected even though it is true. The significance threshold <span class="math inline">\(\alpha\)</span> is the probability of this type of error if <span class="math inline">\(H_0\)</span> is true.</p>
<p>A <strong>type II</strong> error occurs when <span class="math inline">\(H_0\)</span> is not rejected even if it is false. The probability of this type of error is denoted by <span class="math inline">\(\beta\)</span>. More often, we are interested in (<span class="math inline">\(1 - \beta\)</span>), the probability of rejecting <span class="math inline">\(H_0\)</span> when it is false (of detecting a significant difference when there is one). This probability is called the <strong>power</strong> of the test.</p>
<div id="question" class="section level3">
<h3>Question</h3>
<p>In our example from the beginning of the class, could we calculate the power of the test, i.e. the probability of detecting a bias in the sensor with the sample of 9 measurements? Which additional information do we need?</p>
</div>
</div>
<div id="power-of-a-test" class="section level2">
<h2>Power of a test</h2>
<p>Unlike the significance threshold <span class="math inline">\(\alpha\)</span> that is chosen by the analyst, the power of a test depends (among other things) on the actual value of the effect. In our example, for a fixed <span class="math inline">\(\alpha\)</span> and a specific experiment design, it is easier to detect a large bias than a smaller bias.</p>
<p>Calculating the power of a <span class="math inline">\(t\)</span>-test is a fairly complex problem; in R, you can use the <strong>pwr</strong> package functions to perform this calculation.</p>
<p>Here, we will simplify the calculation by assuming that the test statistic follows a normal distribution (which is approximately correct when the sample size <span class="math inline">\(n\)</span> is high).</p>
<p>For example, let’s calculate the power of the test using the soil moisture data (null hypothesis: <span class="math inline">\(\mu = 50\)</span>), if the true bias is 2, the standard error is 1 and <span class="math inline">\(\alpha = 0.05\)</span>. In this case, the true bias corresponds to a standard normal value of <span class="math inline">\(z = 2\)</span>, or 2 standard errors above the null hypothesis mean.</p>
<p>Since <span class="math inline">\(\alpha = 0.05\)</span> and our test is two-sided, the null hypothesis will be rejected for values of <span class="math inline">\(z\)</span> corresponding to a cumulative probability &lt; 0.025 and &gt; 0.975.</p>
<p><img src="4E-Hypothesis_testing_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>For now, let’s focus on the upper limit. The <strong>critical value</strong> of <span class="math inline">\(z\)</span> beyond which we would reject the null hypothesis can be found with <code>qnorm</code>.</p>
<pre class="r"><code>qnorm(0.975)</code></pre>
<pre><code>## [1] 1.959964</code></pre>
<p>Due to the bias, the sample mean will follow a normal distribution with the same standard deviation, but centered on <span class="math inline">\(z = 2\)</span>. This distribution is shown in blue in the graph below.</p>
<p><img src="4E-Hypothesis_testing_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>The filled blue section corresponds to the probability that the null hypothesis will be rejected, if the real mean is 2 standard errors above the reference value. This probability, the power of the test, is about 50%. The exact value can be calculated as follows: 1 - (Probability that the obtained mean is smaller than the critical value of <span class="math inline">\(z\)</span> according to the null hypothesis).</p>
<pre class="r"><code>1 - pnorm(qnorm(0.975), mean = 2)</code></pre>
<pre><code>## [1] 0.5159678</code></pre>
<div id="question-1" class="section level3">
<h3>Question</h3>
<ul>
<li><p>For the same <span class="math inline">\(\alpha\)</span>, is the power of a one-sided test (alternative hypothesis: <span class="math inline">\(\mu &gt; 50\)</span>) smaller, equal or greater than that of a two-sided test?</p></li>
<li><p>If we perform the test illustrated in the graph and we obtain a significant result (rejection of the null hypothesis), would the measured bias be a good estimate of the actual bias of the device? Why?</p></li>
</ul>
<p>If we consider only the filled blue section of the graph above, the mean of <span class="math inline">\(z\)</span> is 2.77. Therefore, in cases where a significant effect is detected, this effect is overestimated.</p>
<p>Here is a more extreme case, when the true bias is at <span class="math inline">\(z = 0.5\)</span> (the actual effect is 1/2 the standard error).</p>
<p><img src="4E-Hypothesis_testing_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>In this case, we have:</p>
<ul>
<li>a 92% probability of not detecting any effect;</li>
<li>a 7% probability of detecting an effect, but it will be strongly over-estimated;</li>
<li>a 1% probability of detecting an effect of the wrong sign.</li>
</ul>
<p>Gelman and Carlin (2014) describe the latter two cases as type M (error on the magnitude of the effect) and type S (error on the sign) errors.</p>
<p><small>Gelman, A. and Carlin, J. (2014) Beyond power calculations: Assessing type S (sign) and type M (magnitude) errors.</small></p>
</div>
</div>
<div id="increasing-the-power-of-a-test" class="section level2">
<h2>Increasing the power of a test</h2>
<ul>
<li><p>A higher <span class="math inline">\(\alpha\)</span> decreases the number of Type II errors (better power), but increases the number of Type I (and Type S) errors.</p></li>
<li><p>The only way to reduce all these types of errors is to increase the sample size. The appropriate sample size depends both on the magnitude of the effect being measured and the variability of the data.</p></li>
<li><p>It is incorrect to calculate the power after the test based on the measured effect. As we have seen, this effect can be strongly overestimated if the real power is low.</p></li>
</ul>
</div>
</div>
<div id="applications-of-the-t-test" class="section level1">
<h1>Applications of the t-test</h1>
<div id="comparing-the-sample-mean-to-a-reference-value" class="section level2">
<h2>Comparing the sample mean to a reference value</h2>
<p>The first application of the <span class="math inline">\(t\)</span>-test consists in comparing the mean of a sample to a fixed value (for example, a theoretical prediction or a very precise reference value).</p>
<p>Let’s repeat our first example, a comparison of a sample of 9 humidity values to a reference mean of 50, this time using the <code>t.test</code> function in R.</p>
<pre class="r"><code>humidite &lt;- c(47, 50, 48, 50, 54, 49, 56, 52, 51)
t.test(humidite, mu = 50)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  humidite
## t = 0.81511, df = 8, p-value = 0.4386
## alternative hypothesis: true mean is not equal to 50
## 95 percent confidence interval:
##  48.57739 52.97816
## sample estimates:
## mean of x 
##  50.77778</code></pre>
<div id="question-2" class="section level3">
<h3>Question</h3>
<ol style="list-style-type: decimal">
<li><p>What does each element of this result of the <code>t.test</code> function mean?</p></li>
<li><p>What is the relationship between a confidence interval and a hypothesis test? What does the 95% confidence interval of <span class="math inline">\(\bar{x}\)</span> tell us about the test result of the null hypothesis <span class="math inline">\(\mu = 50\)</span> with a threshold <span class="math inline">\(\alpha = 0.05\)</span>?</p></li>
</ol>
</div>
</div>
<div id="comparing-the-means-of-two-independent-samples" class="section level2">
<h2>Comparing the means of two independent samples</h2>
<p>The <code>InsectSprays</code> data frame included with R contains the data from an experiment of Geoffrey Beall (1942) on the number of insects (<code>count</code>) on plots treated with different products (<code>spray</code>), with 12 independent measurements by product type.</p>
<pre class="r"><code>ggplot(InsectSprays, aes(x = spray, y = count)) + 
    geom_boxplot()</code></pre>
<p><img src="4E-Hypothesis_testing_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Let’s consider a subset of the data consisting of plots treated with products A and B, and test the null hypothesis that the two products have the same efficiency: <span class="math inline">\(\mu_A = \mu_B\)</span>, or equivalently, <span class="math inline">\(\mu_A - \mu_B = 0\)</span>.</p>
<p>For two independent samples, each with a normal distribution, the difference of the means <span class="math inline">\(\bar{x}_A - \bar{x}_B\)</span> divided by its standard error also follows a <span class="math inline">\(t\)</span> distribution:</p>
<p><span class="math display">\[t = \frac{\bar{x}_A - \bar{x}_B}{\sigma_{\bar{x}_A - \bar{x}_B}}\]</span> We still need to determine the standard error of the difference between the two means and the number of degrees of freedom of this <span class="math inline">\(t\)</span> distribution.</p>
<p>The variance of a difference between two independent random variables is equal to the sum of the variances of the variables taken separately. Thus, the standard error of the difference can be related to the variance and size of each of the two samples.</p>
<p><span class="math display">\[\sigma_{\bar{x}_A - \bar{x}_B}^2 = \sigma_{\bar{x}_A}^2 + \sigma_{\bar{x}_B}^2\]</span> <span class="math display">\[\sigma_{\bar{x}_A - \bar{x}_B}^2 = \frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}\]</span> <span class="math display">\[\sigma_{\bar{x}_A - \bar{x}_B} = \sqrt{\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}}\]</span></p>
<p>The calculation of the degrees of freedom is more complex. According to Welch’s approximation:</p>
<p><span class="math display">\[df = \frac{\left(s_A^2 / n_A + s_B^2 / n_B \right)^2}{\frac{\left( s_A^2 / n_A \right) ^2}{n_A - 1} + \frac{\left( s_B^2/n_B \right)^2}{n_B - 1}}\]</span></p>
<p>Here is the R code to compare the means of the samples treated by products A and B.</p>
<pre class="r"><code>library(dplyr)
insectesAB &lt;- filter(InsectSprays, spray %in% c(&quot;A&quot;, &quot;B&quot;))
t.test(count ~ spray, data = insectesAB)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  count by spray
## t = -0.45352, df = 21.784, p-value = 0.6547
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -4.646182  2.979515
## sample estimates:
## mean in group A mean in group B 
##        14.50000        15.33333</code></pre>
<p>The first argument of <code>t.test</code> is the formula <code>count ~ spray</code>. These formulas are used in several R functions to define statistical models. The variable preceding the symbol <code>~</code> is the dependent variable (response), while the variable following this symbol is the independent variable (predictor, treatment).</p>
<p>If we know that the variance of each group is equal, we can specify <code>var.equal = TRUE</code>.</p>
<pre class="r"><code>t.test(count ~ spray, data = insectesAB, var.equal = TRUE)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  count by spray
## t = -0.45352, df = 22, p-value = 0.6546
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -4.643994  2.977327
## sample estimates:
## mean in group A mean in group B 
##        14.50000        15.33333</code></pre>
<p>In this case, the result is almost identical, except for a slight increase in the number of degrees of freedom, and therefore a slight reduction in the width of the confidence interval. When the groups are the same size and their variances are similar, both versions of the test give similar results.</p>
<p>The <code>t.test</code> function chooses <code>var.equal = FALSE</code> by default and when in doubt, it is better to assume that the variances are different. There are tests to determine if the variances of two samples are equal, but these tests are less reliable than tests comparing means, especially when the distribution of the variable is not exactly normal.</p>
<div id="question-3" class="section level3">
<h3>Question</h3>
<p>If we are interested only in the case where spray B is more effective than A, what is the null hypothesis and the alternative hypothesis on the value of <span class="math inline">\(\bar{x}_A - \bar{x}_B\)</span>?</p>
<p>To do a one-sided test, you need to set the <code>alternative</code> argument of <code>t.test</code> to <code>less</code> or<code>greater</code>. In the case where our alternative hypothesis is <span class="math inline">\(\bar{x}_A - \bar{x}_B &gt; 0\)</span>, we specify <code>alternative = "greater"</code>.</p>
<pre class="r"><code>t.test(count ~ spray, data = insectesAB, alternative = &quot;greater&quot;)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  count by spray
## t = -0.45352, df = 21.784, p-value = 0.6727
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -3.989891       Inf
## sample estimates:
## mean in group A mean in group B 
##        14.50000        15.33333</code></pre>
</div>
</div>
<div id="comparing-the-mean-of-two-paired-samples" class="section level2">
<h2>Comparing the mean of two paired samples</h2>
<p>Suppose that we wish to compare soil moisture measurements taken by two sensors at the same 9 points in a plot. Our null hypothesis is that the mean is the same for both sensors.</p>
<pre class="r"><code>humi &lt;- data.frame(
    point = 1:9,
    mesureA = c(50.0, 51.1, 48.0, 50.0, 51.1, 55.7, 54.3, 46.0, 50.7),
    mesureB = c(49.6, 52.2, 48.3, 50.2, 52.0, 56.1, 54.5, 46.8, 51.7)
)
humi</code></pre>
<pre><code>##   point mesureA mesureB
## 1     1    50.0    49.6
## 2     2    51.1    52.2
## 3     3    48.0    48.3
## 4     4    50.0    50.2
## 5     5    51.1    52.0
## 6     6    55.7    56.1
## 7     7    54.3    54.5
## 8     8    46.0    46.8
## 9     9    50.7    51.7</code></pre>
<p>These measures are paired (one pair of observations per point), so we set the argument <code>paired = TRUE</code> in <code>t.test</code>.</p>
<pre class="r"><code>t.test(humi$mesureA, humi$mesureB, paired = TRUE)</code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  humi$mesureA and humi$mesureB
## t = -3.0779, df = 8, p-value = 0.01516
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.8746025 -0.1253975
## sample estimates:
## mean of the differences 
##                    -0.5</code></pre>
<p>Note that this test is equivalent to a one-sample <span class="math inline">\(t\)</span>-test, which compares the mean of the 9 differences (one per pair) to a reference value of 0.</p>
<pre class="r"><code>humi &lt;- mutate(humi, diff = mesureA - mesureB)
t.test(humi$diff)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  humi$diff
## t = -3.0779, df = 8, p-value = 0.01516
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -0.8746025 -0.1253975
## sample estimates:
## mean of x 
##      -0.5</code></pre>
<div id="exercise-2" class="section level3">
<h3>Exercise</h3>
<p>Interpret the result of the paired test above and compare it to the following test which assumes that the samples are independent.</p>
<pre class="r"><code>t.test(humi$mesureA, humi$mesureB)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  humi$mesureA and humi$mesureB
## t = -0.3629, df = 16, p-value = 0.7214
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.420809  2.420809
## sample estimates:
## mean of x mean of y 
##  50.76667  51.26667</code></pre>
<p>When two groups are paired, the number of degrees of freedom of the <span class="math inline">\(t\)</span>-test is smaller, since we have 9 independent pairs rather than 18 independent points. Despite this, the power of the test increases if the use of pairs makes it possible to separate the effect of the treatment from other sources of variation that influence the response between the pairs. Here, we want to separate the difference in soil moisture measurements between the two devices from the variation in moisture between different points of the plot.</p>
</div>
</div>
</div>
<div id="validity-of-t-test-results" class="section level1">
<h1>Validity of t-test results</h1>
<div id="assumptions-of-the-t-test" class="section level2">
<h2>Assumptions of the t-test</h2>
<p>The <span class="math inline">\(t\)</span>-test requires that:</p>
<ul>
<li><p>the observations are independent of each other (for paired groups, pairs of observations must be independent); and</p></li>
<li><p>the observations come from a normal distribution.</p></li>
</ul>
<p>The independence of observations depends on sampling or experimental design (random sampling or random assignment of treatments).</p>
</div>
<div id="normality-of-the-data" class="section level2">
<h2>Normality of the data</h2>
<ul>
<li><p>The <span class="math inline">\(t\)</span>-test is rather robust, i.e. its conclusions are little affected by small to moderate deviations from the assumption of normality.</p></li>
<li>There are tests of the normality of a sample (e.g. Shapiro-Wilk test). However, these are rarely necessary.
<ul>
<li>With a large sample, the distribution of the sample mean is almost normal (central limit theorem) even if the data is not.</li>
<li>With a small sample, the power of the normality test is low.</li>
</ul></li>
<li>The <span class="math inline">\(t\)</span>-test is less reliable when the distribution is strongly asymmetrical or has outliers.
<ul>
<li>In the first case, a transformation (e.g. logarithmic) can produce a more symmetric distribution that is suitable for the <span class="math inline">\(t\)</span>-test.</li>
<li>In the second case, methods less sensitive to extreme values (more robust) are used.</li>
</ul></li>
</ul>
</div>
<div id="alternatives-to-the-t-test" class="section level2">
<h2>Alternatives to the t-test</h2>
<p>The <strong>Wilcoxon-Mann-Whitney</strong> test is based on the rank of the observations. For the two-sided test with two independent samples A and B, the null hypothesis is that if we drew one element from each group <span class="math inline">\(x_A\)</span> and <span class="math inline">\(x_B\)</span> at random, the probabilities <span class="math inline">\(P(x_A &gt; x_B)\)</span> and <span class="math inline">\(P(x_A &lt; x_B)\)</span> are equal. Usually, this means the median is the same for both groups.</p>
<p>This test is performed by the <code>wilcox.test</code> function in R, whose structure is similar to <code>t.test</code>.</p>
<p>Since the test is based on the order of observations rather than their value, it is less sensitive to extreme values, just like the median is less sensitive to extreme values than the mean.</p>
<p>However, being a non-parametric test (which does not depend on a specific distribution of observations), the Wilcoxon-Mann-Whitney test provides only a <span class="math inline">\(p\)</span> value, without estimating the size of the effect or its confidence interval. In addition, it is not designed to compare two samples of unequal variance.</p>
<p>Other non-parametric methods are based on re-sampling observations to obtain a confidence interval. This type of method, including the <em>bootstrap</em>, will be part of the advanced statistics course (ECL 8202, offered at the winter semester).</p>
</div>
</div>
<div id="recall" class="section level1">
<h1>Recall</h1>
<ul>
<li>General concepts of hypothesis testing
<ul>
<li>Null and alternative hypothesis</li>
<li>One-sided and two-sided test</li>
<li>Test statistic, reference distribution and significance threshold</li>
<li>Power of a test</li>
</ul></li>
<li>Use of the <span class="math inline">\(t\)</span>-test
<ul>
<li>Compare the average of a sample to a reference value</li>
<li>Compare the average of two independent or paired samples</li>
<li>Assumptions: independence of observations, normal distribution of the mean</li>
</ul></li>
</ul>
</div>
<div id="presentation-and-interpretation-of-hypothesis-tests" class="section level1">
<h1>Presentation and interpretation of hypothesis tests</h1>
<p>Hypothesis tests are often misused in the scientific literature. This last section therefore presents some points to keep in mind relative to the use and interpretation of these tests.</p>
<div id="avoid-unnecessary-tests" class="section level2">
<h2>Avoid unnecessary tests</h2>
<p>To justify the presentation of a hypothesis test, the null hypothesis must be plausible. For example, if there is no doubt that a variable has an effect on the measured response, it is sufficient to estimate this effect and indicate its confidence interval.</p>
</div>
<div id="the-p-value-is-only-one-part-of-the-result" class="section level2">
<h2>The p-value is only one part of the result</h2>
<p>The graph below shows the estimation of two effects with their 95% confidence interval. Both effects are significantly different from zero with a <span class="math inline">\(p\)</span>-value of 0.01.</p>
<p><img src="4E-Hypothesis_testing_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>A small <span class="math inline">\(p\)</span>-value indicates that observed effect size is very unlikely if the null hypothesis is true. It does not indicate the magnitude of the effect on the measured variable. That is why it is important to not only report the result of the hypothesis test with its <span class="math inline">\(p\)</span>-value, but also report an estimate of the size of the effect with a confidence interval.</p>
<p>In the following graph, both effects have the same confidence interval. However, the distribution of the observed values (points) is different.</p>
<p><img src="4E-Hypothesis_testing_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>In summary, at least three results of a statistical test must be reported:</p>
<ul>
<li>the probability that the measured effect is due to chance (<span class="math inline">\(p\)</span>-value);</li>
<li>the estimated effect size with its confidence interval; and</li>
<li>the effect size compared to the variance of the individual data points.</li>
</ul>
</div>
<div id="statistically-significant-does-not-equal-important" class="section level2">
<h2>(Statistically) significant does not equal important</h2>
<p>It is rare that the effect of a treatment is exactly zero. For this reason, with a large enough sample and a constant <span class="math inline">\(\alpha\)</span> threshold, we can always detect a significant effect.</p>
<p>For example, in 2014, a controversial study produced by Facebook statisticians showed that experimental manipulation of negative and positive topics appearing on the news feed of subscribers to the site could affect the number of positive and negative words in their own posts. The sample size is huge (<span class="math inline">\(n\)</span> about 700,000) and the measured effects are tiny.</p>
<p><img src="../images/fb1.PNG" width="200" /><img src="../images/fb2.PNG" width="200" /></p>
<p>Also, note that a bar chart must <em>always</em> extend to zero on the axis. Otherwise, as seen here, the difference between the length of the bars overestimates the size of the effect.</p>
<p>If a statistical test demonstrates that an effect is not due to sampling variance, it is up to the researchers to determine whether the estimated effect is important in the context of the system studied.</p>
</div>
<div id="watch-out-for-multiple-comparisons" class="section level2">
<h2>Watch out for multiple comparisons</h2>
<p>By definition, a hypothesis test done with <span class="math inline">\(\alpha\)</span> = 0.05 will commit a type I error 5% of the time. Thus, when performing multiple tests in a single study, the probability that one of the tests will detect a effect due only to sampling variance increases. We will see some solutions to the problem of multiple comparisons in the next classes.</p>
<p>Also, keep in mind that a type I error rate of 5% is not negligible, especially when we consider the number of studies publishing hypothesis tests every year. The publication of a study with a <span class="math inline">\(p &lt; 0.05\)</span> result does not mean that the null hypothesis is definitely rejected. Moreover, as we saw above, when the statistical power is low, results that exceed the significance threshold tend to strongly overestimate the real effect. It is therefore prudent to be skeptical of a study showing a larger effect than expected if the sample size is small. Replicating the significant result at another site is a good way to confirm the existence of an effect.</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
