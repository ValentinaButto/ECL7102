<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="date" content="2019-10-07" />

<title>Linear regression</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Linear regression</h1>
<h4 class="date"><br/>October 7, 2019</h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li><p>Estimate and interpret the parameters of a simple linear regression and those of a multiple linear regression, with or without interactions.</p></li>
<li><p>Check the assumptions of a regression model from the diagnostic graphs.</p></li>
<li><p>Differentiate the confidence interval of a regression line and the prediction interval for new observations.</p></li>
<li><p>Identify problems due to the collinearity of several predictors.</p></li>
</ul>
</div>
<div id="regression-overview" class="section level1">
<h1>Regression: Overview</h1>
<p>The next six courses will focus on regression models. These models represent the mathematical relationship between a <em>response</em> variable and one or more variables named <em>predictors</em>.</p>
<p>Regression analysis is particularly useful in the following cases:</p>
<ul>
<li><p>Analyzing the results of an experiment when one or more treatment variables are numeric (e.g. temperature, dose).</p></li>
<li><p>Separating the effect of discrete treatments (categorical variables) from that of other experimental conditions represented by numerical variables. In this context, it is often called <strong>analysis of covariance</strong>.</p></li>
<li><p>Determining the importance of associations between variables measured in nature (without assuming a causal link).</p></li>
<li><p>Using the associations between predictors and response to predict the value of the latter for new observations.</p></li>
</ul>
<p>The mathematical model remains the same for all these situations, so they differ in the interpretation and use of the results.</p>
</div>
<div id="simple-linear-regression" class="section level1">
<h1>Simple linear regression</h1>
<p>Simple linear regression refers to the case where there is only one numerical predictor (<span class="math inline">\(x\)</span>).</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x + \epsilon \]</span></p>
<p>Recall that <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the <em>coefficients</em> of the regression that will be estimated from the data, while <span class="math inline">\(\epsilon\)</span> is the random <em>residual</em> that follows a normal distribution centered on zero: <span class="math inline">\(N(0,\sigma)\)</span>.</p>
<p>This model means that for a given value of <span class="math inline">\(x\)</span>, the response <span class="math inline">\(y\)</span> follows a normal distribution with mean <span class="math inline">\(\mu = \beta_0 + \beta_1 x\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. The <em>intercept</em> <span class="math inline">\(\beta_0\)</span> is the mean value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span>, while the <em>slope</em> <span class="math inline">\(\beta_1\)</span> is the mean difference of <span class="math inline">\(y\)</span> between two observations that differ by 1 unit of <span class="math inline">\(x\)</span>.</p>
<div id="method-of-least-squares" class="section level2">
<h2>Method of least squares</h2>
<div id="example" class="section level3">
<h3>Example</h3>
<p>The <a href="../donnees/plant_growth_rate.csv">plant_growth_rate.csv</a> dataset (from the textbook of Beckerman, Childs and Petchey, <em>Getting Started with R, An Introduction for Biologists</em>) contains data on the growth of a plant as a function of soil moisture content.</p>
<pre class="r"><code>pgr &lt;- read.csv(&quot;../donnees/plant_growth_rate.csv&quot;)
str(pgr)</code></pre>
<pre><code>## &#39;data.frame&#39;:    50 obs. of  2 variables:
##  $ soil.moisture.content: num  0.47 0.541 1.698 0.826 0.857 ...
##  $ plant.growth.rate    : num  21.3 27 39 30.2 37.1 ...</code></pre>
<p>Estimating the coefficients of the linear regression is equivalent to finding the line that is “closest” to the points in the graph of <span class="math inline">\(y\)</span> vs. <span class="math inline">\(x\)</span>.</p>
<pre class="r"><code>ggplot(pgr, aes(x = soil.moisture.content, y = plant.growth.rate)) +
    geom_point() +
    geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>More precisely, it is possible to show that the best unbiased estimators of linear regression parameters are those that minimize the sum of the squared residuals. This is the <strong>method of least squares</strong>.</p>
<p>For a series of <span class="math inline">\(n\)</span> observations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, this sum of squared residuals corresponds to:</p>
<p><span class="math display">\[ \sum_{k = 1}^n \epsilon_k^2 = \sum_{k = 1}^n (y_k - (\beta_0 + \beta_1 x_k))^2 \]</span></p>
<p>The estimates <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> that minimize this sum are obtained by differential calculus. (They are the values for which the partial derivatives of the sum as a function of each coefficient are equal to zero.)</p>
<p>The estimator for the slope <span class="math inline">\(\beta_1\)</span> is equal to the covariance of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> divided by the variance of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\hat{\beta_1} = \frac{\sum_{k = 1}^n (x_k - \bar{x})(y_k - \bar{y})}{\sum_{k = 1}^n (x_k - \bar{x})^2}\]</span></p>
<p>Here, <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> represent the means of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, respectively, over all observations.</p>
<p>The estimator for the intercept <span class="math inline">\(\beta_0\)</span> is equal to:</p>
<p><span class="math display">\[\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}\]</span></p>
<p>By rearranging this last equation:</p>
<p><span class="math display">\[\bar{y} = \hat{\beta_0} + \hat{\beta_1} \bar{x}\]</span></p>
<p>We see that the estimated regression line goes through the point <span class="math inline">\((\bar{x}, \bar{y})\)</span>, the “center of mass” for the scatterplot of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
</div>
</div>
<div id="interpreting-the-results" class="section level2">
<h2>Interpreting the results</h2>
<p>By fitting a regression model to the data above, we obtain the following result:</p>
<pre class="r"><code>mod &lt;- lm(plant.growth.rate ~ soil.moisture.content, data = pgr)
summary(mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = plant.growth.rate ~ soil.moisture.content, data = pgr)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.9089 -3.0747  0.2261  2.6567  8.9406 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             19.348      1.283   15.08   &lt;2e-16 ***
## soil.moisture.content   12.750      1.021   12.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.019 on 48 degrees of freedom
## Multiple R-squared:  0.7648, Adjusted R-squared:  0.7599 
## F-statistic: 156.1 on 1 and 48 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The coefficient table includes the intercept and the effect of each predictor. If the assumptions of the model are respected (see section below), each estimated <span class="math inline">\(\hat{\beta}\)</span> follows a normal distribution with a mean equal to the value of the parameter <span class="math inline">\(\beta\)</span>, and a standard error as shown in the table. This allows us to run a <span class="math inline">\(t\)</span>-test for the null hypothesis <span class="math inline">\(\beta = 0\)</span>, with a value <span class="math inline">\(p\)</span> indicated in the last column.</p>
<p>Under that table, <code>Residual standard error</code> is the standard deviation of model residuals calculated with 48 degrees of freedom (50 observations - 2 estimated parameters).</p>
<pre class="r"><code>sqrt(sum(mod$residuals^2) / 48)</code></pre>
<pre><code>## [1] 4.019094</code></pre>
<p>As in ANOVA, the coefficient of determination (<span class="math inline">\(R^2\)</span>) represents the fraction of total variance explained by the model.</p>
<p><span class="math display">\[R^2 = 1 - \frac{\sum_{k=1}^n (y_k - \hat{y_k})^2}{\sum_{k=1}^n (y_k - \bar{y})^2}\]</span></p>
<p>The numerator is the sum of squared residuals, since <span class="math inline">\(\hat{y_k} = \hat{\beta_0} + \hat{\beta_1} x_k\)</span> is the expected value (mean) <span class="math inline">\(y_k\)</span> according to the model.</p>
<pre class="r"><code>r2 &lt;- 1 - sum(mod$residuals^2) / sum((pgr$plant.growth.rate - mean(pgr$plant.growth.rate))^2)
r2</code></pre>
<pre><code>## [1] 0.764796</code></pre>
<p>The values of <span class="math inline">\(\hat{y_k}\)</span> for each point are recorded in the <code>fitted.values</code> component of the result of <code>lm</code> (e.g. <code>mod$fitted.values</code>).</p>
<p>For a simple linear regression, the square root of <span class="math inline">\(R^2\)</span> is equal to the correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<pre class="r"><code>cor_pgr &lt;- cor(pgr$soil.moisture.content, pgr$plant.growth.rate) 
all.equal(cor_pgr, sqrt(r2))</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p><strong>Note</strong>: When comparing real numbers in R, it is better to use <code>all.equal</code> than <code>==</code>. Due to the limited accuracy of the calculations, the two values are in fact <em>almost</em> equal, so the exact equality <code>==</code> would return <code>FALSE</code>.</p>
<pre class="r"><code>cor_pgr - sqrt(r2)</code></pre>
<pre><code>## [1] 1.110223e-16</code></pre>
<p>The last line of the summary is an <span class="math inline">\(F\)</span>-test similar to ANOVA. When we have only one predictor, this test gives the same information as the <span class="math inline">\(t\)</span>-test for that predictor: the probability of obtaining an estimated effect that far from 0 if the actual effect of the predictor is 0.</p>
</div>
<div id="confidence-interval-and-prediction-interval" class="section level2">
<h2>Confidence interval and prediction interval</h2>
<p>To display the regression line on a scatter plot of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, along with its confidence interval, we use the <code>geom_smooth</code> function of the <em>ggplot2</em> package, with the <code>lm</code> (linear model) method.</p>
<pre class="r"><code>ggplot(pgr, aes(x = soil.moisture.content, y = plant.growth.rate)) +
    geom_point() + 
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>For each value of <span class="math inline">\(x\)</span>, the gray area gives a confidence interval for the average value of <span class="math inline">\(y\)</span> according to the linear model. By default, this is a 95% interval, which can be changed with the <code>level</code> argument of <code>geom_smooth</code>.</p>
<p>Note that the confidence interval becomes wider at the ends of the graph. Remember that the regression line must pass through the point <span class="math inline">\((\bar{x}, \bar {y})\)</span>, so the uncertainty on the slope “rotates” the line slightly around this point, which generates greater uncertainty at the ends.</p>
<p>Suppose that in addition to estimating the average trend between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, we want to predict the value of <span class="math inline">\(y\)</span> for new observations, knowing only the value of <span class="math inline">\(x\)</span>. In the code below, we create a new <code>pgr_nouv</code> data array with 101 soil moisture values, and then we call the <code>predict</code> function to get growth predictions from the model, with a prediction interval. We then attach these columns to <code>pgr_nouv</code> with <code>cbind</code>.</p>
<pre class="r"><code>pgr_nouv &lt;- data.frame(soil.moisture.content = seq(0, 2, 0.02))
pgr_pred &lt;- predict(mod, pgr_nouv, interval = &quot;prediction&quot;)
pgr_nouv &lt;- cbind(pgr_nouv, pgr_pred)
str(pgr_nouv)</code></pre>
<pre><code>## &#39;data.frame&#39;:    101 obs. of  4 variables:
##  $ soil.moisture.content: num  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 ...
##  $ fit                  : num  19.3 19.6 19.9 20.1 20.4 ...
##  $ lwr                  : num  10.9 11.1 11.4 11.7 11.9 ...
##  $ upr                  : num  27.8 28.1 28.3 28.6 28.8 ...</code></pre>
<p>The <code>fit</code> column contains the predicted values (which correspond to the points on the regression line) while <code>lwr</code> and <code>upr</code> are the lower and upper limits of the 95% prediction interval.</p>
<p>Now let’s superimpose the regression line, the prediction interval (with <code>geom_ribbon</code>) and the scatter plot:</p>
<pre class="r"><code>ggplot(pgr_nouv, aes(x = soil.moisture.content)) +
    labs(x = &quot;Soil moisture&quot;, y = &quot;Growth&quot;) +
    geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3) +
    geom_line(aes(y = fit), color = &quot;blue&quot;) +
    geom_point(data = pgr, aes(y = plant.growth.rate))</code></pre>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>In contrast to the confidence interval, which represents the uncertainty in the mean value of the response for a certain predictor value, the prediction interval represents the uncertainty in the value of the response for an individual observation. Thus, it is expected that about 95% of the points are within the prediction interval, which is the case here (48/50).</p>
<p><strong>Note</strong>: In general, it is not prudent to use the result of a regression to predict the response for predictor values outside the range of values with which the model was estimated (in this example, for soil moisture values &gt; 2). These extrapolations are less reliable than predictions within the range of observed values (interpolation). In particular, an approximately linear relation on a restricted scale of values of <span class="math inline">\(x\)</span> can become strongly non-linear at a different scale.</p>
</div>
</div>
<div id="assumptions-of-the-linear-regression-model" class="section level1">
<h1>Assumptions of the linear regression model</h1>
<p>As for ANOVA, the residuals are:</p>
<ul>
<li>independent and</li>
<li>normally distributed</li>
<li>with the same variance.</li>
</ul>
<p>Moreover:</p>
<ul>
<li>the relationship between the average response and the predictors is linear, and</li>
<li>the predictors are measured without error (or this error is negligible compared to the other errors of the model).</li>
</ul>
<div id="linearity" class="section level2">
<h2>Linearity</h2>
<p>The linearity requirement is less restrictive than it seems. Variable transformations make it possible to convert a non-linear relationship into a linear relationship. For example, if <span class="math inline">\(y\)</span> is a function of a certain power of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[ y = a x^b \]</span></p>
<p>then by applying a logarithm to each side, we obtain a linear model:</p>
<p><span class="math display">\[ \log(y) = \log(a) + b \log(x) \]</span></p>
<p>In general, the equation linking <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> can contain nonlinear functions of <span class="math inline">\(x\)</span>, as long as it is a linear function of the <em>coefficients</em>. For example, the quadratric equation:</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x + \beta_2 x^2 \]</span> is an example of a linear model; it is a multiple linear regression, since there are two predictors, <span class="math inline">\(x\)</span> and <span class="math inline">\(x^2\)</span>.</p>
</div>
<div id="independence-of-residuals" class="section level2">
<h2>Independence of residuals</h2>
<p>The independence of residuals means that the portion of the response <span class="math inline">\(y\)</span> not explained by the predictors <span class="math inline">\(x\)</span> is independent from one observation to another.</p>
<p>In ecology, the non-independence of residuals is often due to a proximity of certain observations in space and time. For example, if the observations are spread over several days, observations that are closer in time may be more similar. Factors that can cause this temporal dependence (e.g., weather) can be included in the model to obtain the most independent residuals possible.</p>
<p>The non-independence of residuals does not bias the estimates of the model coefficients, so these remain valid, but their uncertainty will be underestimated. (We could say that a sample of non-independent observations is equivalent to a smaller independent sample.) Thus, the confidence intervals and hypothesis tests on the significance of the coefficients will not be valid.</p>
</div>
<div id="diagnostic-graphs" class="section level2">
<h2>Diagnostic graphs</h2>
<p>Here are the four diagnostic graphs obtained with the <code>plot</code> function applied to the result of <code>lm</code>.</p>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The first two graphs correspond to those already seen in ANOVA. For the graph of residuals vs. fitted values, here are important points to check:</p>
<ul>
<li><p>The residuals must be randomly dispersed around zero. The presence of a trend (linear or not) indicates systematic effects ignored by the model. In this case, we may have a slight non-linear (quadratic) trend in the residuals.</p></li>
<li><p>The variance of the residuals must be approximately constant (homoscedasticity). A common type of heteroscedasticity occurs when the variance increases with the mean. In this case, the graph of the residuals vs. fitted values produces a funnel shape (dispersion of points increases along the <span class="math inline">\(x\)</span> axis).</p></li>
</ul>
<p>The quantile-quantile plot makes it possible to detect systematic deviations from the normality of the residuals.</p>
<p>The third graph shows the scale of the residuals (in absolute value) according to the fitted values of <span class="math inline">\(y\)</span>. This graph will also show a trend if the variance is not constant.</p>
</div>
<div id="leverage" class="section level2">
<h2>Leverage</h2>
<p>The last chart shows the <strong>leverage</strong> of observations relative to the value of the residuals. An observation with high leverage has a greater influence on the regression coefficient estimates; this occurs most often in the case of isolated observations and far from the mean values of the predictors. An observation far from the mean (significant positive or negative residual) that also has a strong leverage effect may move the regression line away from the general trend indicated by the other data.</p>
<p>Cook’s distance <span class="math inline">\(D\)</span> is a metric that combines leverage with the magnitude of the residual. The dashed lines on the fourth graph identify problematic points that exceed a certain value of <span class="math inline">\(D\)</span>, usually <span class="math inline">\(D &gt; 1\)</span>. In our previous example, no point has a large influence, so these dotted lines are outside the visible part of the graph.</p>
</div>
</div>
<div id="multiple-linear-regression" class="section level1">
<h1>Multiple linear regression</h1>
<p>The multiple linear regression model represents the relationship between a response variable and <span class="math inline">\(m\)</span> predictors <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_m\)</span>.</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_m x_m + \epsilon = \beta_0 + \sum_{i = 1}^m \beta_i x_i + \epsilon \]</span> As in the case of simple linear regression, the <span class="math inline">\(\beta\)</span> coefficients can be computed from the method of least squares. In this model, each <span class="math inline">\(\beta_i\)</span> coefficient (except <span class="math inline">\(\beta_0\)</span>) is the partial derivative of <span class="math inline">\(y\)</span> with respect to a predictor <span class="math inline">\(x_i\)</span>. In other words, this coefficient represents the mean change in <span class="math inline">\(y\)</span> if <span class="math inline">\(x_i\)</span> increases by 1 <em>and all other predictors remain constant</em>.</p>
</div>
<div id="analysis-of-covariance" class="section level1">
<h1>Analysis of covariance</h1>
<p>We will first consider a model including a categorical predictor and a numerical predictor. In an experimental context, this type of model is called an analysis of covariance (ANCOVA).</p>
<p>The data frame <a href="../donnees/compensation.csv">compensation.csv</a> is taken from Crawley’s book, <em>Statistics: An introduction using R</em>. It contains data on seed mass produced by a plant species (<em>Fruit</em>) based on root size (<em>Root</em>) and whether or not the plant is grazed (<em>Grazing</em>).</p>
<pre class="r"><code>comp &lt;- read.csv(&quot;../donnees/compensation.csv&quot;)
str(comp)</code></pre>
<pre><code>## &#39;data.frame&#39;:    40 obs. of  3 variables:
##  $ Root   : num  6.22 6.49 4.92 5.13 5.42 ...
##  $ Fruit  : num  59.8 61 14.7 19.3 34.2 ...
##  $ Grazing: Factor w/ 2 levels &quot;Grazed&quot;,&quot;Ungrazed&quot;: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<p>Let’s first inspect the data.</p>
<pre class="r"><code>ggplot(comp, aes(x = Root, y = Fruit, color = Grazing)) +
    geom_point() +
    scale_color_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>The graph shows the existence of a linear relationship between root size and seed production, as well as the effect of treatment: for the same root size, grazing reduces seed production. Note that if we had not measured the roots, we might think that grazing has a positive effect.</p>
<pre class="r"><code>ggplot(comp, aes(x = Grazing, y = Fruit)) +
    geom_boxplot()</code></pre>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>This is because the plants undergoing grazing had (on average) larger roots initially. Root size is therefore a <em>confounding</em> variable that must be taken into account in order to properly assess the effect of grazing.</p>
<p>Here is a linear model where the effect of the two predictors is additive:</p>
<pre class="r"><code>mod_comp &lt;- lm(Fruit ~ Grazing + Root, data = comp)
summary(mod_comp)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Fruit ~ Grazing + Root, data = comp)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.1920  -2.8224   0.3223   3.9144  17.3290 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -127.829      9.664  -13.23 1.35e-15 ***
## GrazingUngrazed   36.103      3.357   10.75 6.11e-13 ***
## Root              23.560      1.149   20.51  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.747 on 37 degrees of freedom
## Multiple R-squared:  0.9291, Adjusted R-squared:  0.9252 
## F-statistic: 242.3 on 2 and 37 DF,  p-value: &lt; 2.2e-16</code></pre>
<div id="results-interpretation" class="section level2">
<h2>Results interpretation</h2>
<p>If <span class="math inline">\(x_1\)</span> is the grazing variable (0 = Grazed, 1 = Ungrazed according to the default encoding in R) and <span class="math inline">\(x_2\)</span> is the root size, the mathematical expression of this model is:</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon \]</span></p>
<p>To simplify the interpretation of the coefficients, we separate the case with grazing (<span class="math inline">\(x_1 = 0\)</span>):</p>
<p><span class="math display">\[ y = \beta_0 + \beta_2 x_2 + \epsilon \]</span></p>
<p>and the case without grazing (<span class="math inline">\(x_1 = 1\)</span>):</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 + \beta_2 x_2 + \epsilon \]</span></p>
<p>The coefficients can now be interpreted as follows:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> (<code>Intercept</code> in the summary table) is the intercept of the <em>Fruit</em> vs. <em>Root</em> regression line with grazing.</li>
<li><span class="math inline">\(\beta_1\)</span> (<code>GrazingUngrazed</code>) is the effect of the absence of grazing on the intercept of the <em>Fruit</em> vs. <em>Root</em> line.</li>
<li><span class="math inline">\(\beta_2\)</span> (<code>Root</code>) is the slope of the <em>Fruit</em> vs. <em>Root</em> regression line with or without grazing.</li>
</ul>
<p>Since the slope is the same with or without grazing, the coefficient <span class="math inline">\(\beta_1\)</span> corresponds to a translation on the <span class="math inline">\(y\)</span> axis of the regression line. This model of the <em>additive</em> effects of a treatment and a numerical variable thus results in two parallel lines, which corresponds quite well to our visualization of the data. In addition, the value of <span class="math inline">\(R^2\)</span> (0.93) indicates that the model accounts for much of the observed variation in the data.</p>
<p>Even a large value of <span class="math inline">\(R^2\)</span> does not necessarily mean that the model is appropriate. Always look at the diagnostic charts.</p>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Note that some extreme points are labelled with the corresponding row number in the data frame, to facilitate the identification of problematic points.</p>
<p>The <span class="math inline">\(F\)</span>-test reported at the bottom of the results summary of <code>lm</code> corresponds to the null hypothesis of no effect for all predictors.</p>
<p>We can also obtain a conventional ANOVA table by applying the <code>anova</code> function to the <code>lm</code> result.</p>
<pre class="r"><code>anova(mod_comp)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Fruit
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Grazing    1  2910.4  2910.4  63.929 1.397e-09 ***
## Root       1 19148.9 19148.9 420.616 &lt; 2.2e-16 ***
## Residuals 37  1684.5    45.5                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="model-with-interaction" class="section level2">
<h2>Model with interaction</h2>
<p>As in ANOVA, we can include the interaction between two variables with the multiplication symbol <code>*</code>:</p>
<pre class="r"><code>mod_comp_inter &lt;- lm(Fruit ~ Grazing * Root, data = comp)
summary(mod_comp_inter)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Fruit ~ Grazing * Root, data = comp)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.3177  -2.8320   0.1247   3.8511  17.1313 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          -125.173     12.811  -9.771 1.15e-11 ***
## GrazingUngrazed        30.806     16.842   1.829   0.0757 .  
## Root                   23.240      1.531  15.182  &lt; 2e-16 ***
## GrazingUngrazed:Root    0.756      2.354   0.321   0.7500    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.831 on 36 degrees of freedom
## Multiple R-squared:  0.9293, Adjusted R-squared:  0.9234 
## F-statistic: 157.6 on 3 and 36 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>If <span class="math inline">\(x_1\)</span> is the grazing variable (0 = Grazed, 1 = Ungrazed according to the default encoding in R) and <span class="math inline">\(x_2\)</span> is the root size, the mathematical expression of this model is:</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \epsilon \]</span></p>
<p>The interaction is thus equivalent to adding a new predictor to the model, equal to the product of the two interacting variables. Let’s separate again into two equations according to the treatment:</p>
<p>With grazing (<span class="math inline">\(x_1 = 0\)</span>):</p>
<p><span class="math display">\[ y = \beta_0 + \beta_2 x_2 \]</span></p>
<p>Without grazing (<span class="math inline">\(x_1 = 1\)</span>):</p>
<p><span class="math display">\[ y = (\beta_0 + \beta_1) + (\beta_2 + \beta_{12}) x_2 \]</span></p>
<p>For this model with interaction, the interpretation of the coefficients changes a bit:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> (<code>Intercept</code> in the summary table) is the intercept of the <em>Fruit</em> vs. <em>Root</em> line without grazing.</li>
<li><span class="math inline">\(\beta_1\)</span> (<code>GrazingUngrazed</code>) is the effect of the absence of grazing on the intercept of <em>Fruit</em> vs. <em>Root</em>.</li>
<li><span class="math inline">\(\beta_2\)</span> (<code>Root</code>) is the slope of the <em>Fruit</em> vs. <em>Root</em> line with grazing.</li>
<li><span class="math inline">\(\beta_{12}\)</span> (<code>GrazingUngrazed:Root</code>) is the effect of the absence of grazing on the slope of the <em>Fruit</em> vs. <em>Root</em> line.</li>
</ul>
<p>The interaction model is therefore equivalent to separately estimating the regression line (intercept and slope) for each of the two treatments.</p>
<p>Compared to the additive model, note that the effect of no grazing (<code>GrazingUngrazed</code>) now has a much higher standard error and a larger <span class="math inline">\(p\)</span> value.</p>
<pre class="r"><code>summary(mod_comp)$coefficients</code></pre>
<pre><code>##                   Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)     -127.82936   9.664095 -13.22725 1.349804e-15
## GrazingUngrazed   36.10325   3.357396  10.75335 6.107286e-13
## Root              23.56005   1.148771  20.50892 8.408231e-22</code></pre>
<pre class="r"><code>summary(mod_comp_inter)$coefficients</code></pre>
<pre><code>##                          Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)          -125.1730569  12.811165 -9.7706222 1.150540e-11
## GrazingUngrazed        30.8057049  16.841823  1.8291194 7.567489e-02
## Root                   23.2403732   1.530771 15.1821314 3.173208e-17
## GrazingUngrazed:Root    0.7560338   2.354111  0.3211547 7.499503e-01</code></pre>
<p>This is because the intercept, corresponding to <em>Root</em> = 0, is far from the data range (<em>Root</em> values are all between 4 and 11). Therefore, a small change of slope in the middle of the graph can lead to a significant change in intercept and the uncertainty of the interaction coefficient (difference in slope) also affects the estimate of the difference in intercept.</p>
<p>By consulting the ANOVA table, we can confirm that the interaction is not significant, the additive model is therefore preferable.</p>
<pre class="r"><code>anova(mod_comp_inter)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Fruit
##              Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
## Grazing       1  2910.4  2910.4  62.3795 2.262e-09 ***
## Root          1 19148.9 19148.9 410.4201 &lt; 2.2e-16 ***
## Grazing:Root  1     4.8     4.8   0.1031      0.75    
## Residuals    36  1679.6    46.7                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Why is the effect of grazing significant here? In the ANOVA table, we test whether there is a significant difference in the mean of <em>Fruit</em> between plants that are grazed or not, rather than whether there is a significant difference in intercept between two lines (which is what <code>GrazingUngrazed</code> measures in the linear model).</p>
</div>
</div>
<div id="regression-with-multiple-numerical-predictors" class="section level1">
<h1>Regression with multiple numerical predictors</h1>
<p>The <code>hills</code> data frame in the <em>MASS</em> package (included by default with R) contains the record times (<em>time</em>, in minutes) for Scottish bike races based on the horizontal distance (<em>dist</em>, in miles) and the total change in altitude (<em>climb</em>, in feet).</p>
<pre class="r"><code>library(MASS)
str(hills)</code></pre>
<pre><code>## &#39;data.frame&#39;:    35 obs. of  3 variables:
##  $ dist : num  2.5 6 6 7.5 8 8 16 6 5 6 ...
##  $ climb: int  650 2500 900 800 3070 2866 7500 800 800 650 ...
##  $ time : num  16.1 48.4 33.6 45.6 62.3 ...</code></pre>
<p>For a data frame with multiple numeric variables, the <code>plot</code> function displays a matrix of scatter plots for each pair of variables.</p>
<pre class="r"><code>plot(hills)</code></pre>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>The record times seem to depend linearly on the distance and the climb. (The distance and climb also seem to be correlated, we will come back to this later.) We therefore apply a linear model to these data.</p>
<pre class="r"><code>mod_hills &lt;- lm(time ~ dist + climb, hills)</code></pre>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Since the rows in this data frame are labelled (<code>rownames</code>) in R, those labels appear next to the extreme values in the diagnostic graphs.</p>
<p>According to these graphs, two races (Knock Hill and Bens of Jura) have a record time much longer than expected (significant positive residual). These same points also have a great influence on the regression coefficients (according to the fourth graph). In this case, it would be recommended to check if these routes have peculiarities which explain this large difference compared to the model.</p>
<div id="standardization-of-variables" class="section level2">
<h2>Standardization of variables</h2>
<p>Let’s look at the summary results:</p>
<pre class="r"><code>summary(mod_hills)</code></pre>
<pre><code>## 
## Call:
## lm(formula = time ~ dist + climb, data = hills)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.215  -7.129  -1.186   2.371  65.121 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -8.992039   4.302734  -2.090   0.0447 *  
## dist         6.217956   0.601148  10.343 9.86e-12 ***
## climb        0.011048   0.002051   5.387 6.45e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.68 on 32 degrees of freedom
## Multiple R-squared:  0.9191, Adjusted R-squared:  0.914 
## F-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The values of the coefficients mean that on average, each mile of distance adds 6.22 minutes to the record time while each foot of elevation adds 0.01 minute. Since the predictors do not have the same units, the value of the coefficients is not indicative of the importance of each variable. In this case, <em>dist</em> varies between 2 and 28 miles whereas <em>climb</em> varies between 300 and 7500 feet.</p>
<p>Also, the intercept is not meaningful, since a route cannot have a length of 0.</p>
<p>In order to compare the influence of different predictors, it may be useful to standardize these, that is, to transform each value by subtracting the mean and dividing by the standard deviation. In R, the <code>scale</code> function automatically performs this transformation.</p>
<pre class="r"><code>hills_scl &lt;- hills
hills_scl[, -3] &lt;- scale(hills_scl[, -3]) # we don&#39;t standardize the response
mod_hills_scl &lt;- lm(time ~ dist + climb, data = hills_scl)
summary(mod_hills_scl)</code></pre>
<pre><code>## 
## Call:
## lm(formula = time ~ dist + climb, data = hills_scl)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.215  -7.129  -1.186   2.371  65.121 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   57.876      2.481  23.331  &lt; 2e-16 ***
## dist          34.348      3.321  10.343 9.86e-12 ***
## climb         17.888      3.321   5.387 6.45e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.68 on 32 degrees of freedom
## Multiple R-squared:  0.9191, Adjusted R-squared:  0.914 
## F-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>For each point, the standardized variable represents the deviation of the original variable from its mean, expressed as a multiple of the standard deviation of the original variable. For example, in this version of the model, the coefficient of <em>dist</em> indicates the difference in record time associated with an increase of one standard deviation in the horizontal distance. The standardized coefficients thus represent the effect of a variable relative to the typical differences observed for that variable.</p>
<p>Another advantage of this representation is that, since the standardized predictors take a value of 0 at their mean, the value of the intercept is the overall mean of the response (here the mean record time is about 58 minutes).</p>
<p>The standardization of predictors only changes the scale of the estimated effects. The significance of the effect of each predictor and the predictions of the model remain the same.</p>
</div>
<div id="interaction-between-continuous-variables" class="section level2">
<h2>Interaction between continuous variables</h2>
<p>How to interpret the interaction between two continuous variables? For example:</p>
<pre class="r"><code>mod_hills_inter &lt;- lm(time ~ dist * climb, hills_scl)
summary(mod_hills_inter)</code></pre>
<pre><code>## 
## Call:
## lm(formula = time ~ dist * climb, data = hills_scl)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -25.994  -4.968  -2.220   2.381  56.115 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   52.304      2.793  18.728  &lt; 2e-16 ***
## dist          32.776      2.965  11.053 2.78e-12 ***
## climb         10.411      3.742   2.782  0.00911 ** 
## dist:climb     8.793      2.745   3.203  0.00314 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.92 on 31 degrees of freedom
## Multiple R-squared:  0.9392, Adjusted R-squared:  0.9333 
## F-statistic: 159.6 on 3 and 31 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>As we saw earlier, the equation for a model with two interacting predictors is:</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \epsilon \]</span></p>
<p>We can rewrite this equation in two ways:</p>
<p><span class="math display">\[ y = \beta_0 + (\beta_1 + \beta_{12} x_2) x_1 + \beta_2 x_2 \]</span></p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + (\beta_2 + \beta_{12} x_1) x_2 \]</span></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the value of <span class="math inline">\(y\)</span> if <span class="math inline">\(x_1 = 0\)</span> and <span class="math inline">\(x_2 = 0\)</span>;</li>
<li><span class="math inline">\(\beta_1\)</span> is the effect on <span class="math inline">\(y\)</span> of a unit increase in <span class="math inline">\(x_1\)</span> if <span class="math inline">\(x_2 = 0\)</span>;</li>
<li><span class="math inline">\(\beta_2\)</span> is the effect on <span class="math inline">\(y\)</span> of a unit increase in <span class="math inline">\(x_2\)</span> if <span class="math inline">\(x_1 = 0\)</span>;</li>
<li><span class="math inline">\(\beta_{12}\)</span> is both the change in slope of <span class="math inline">\(y\)</span> vs. <span class="math inline">\(x_1\)</span> if <span class="math inline">\(x_2\)</span> increases by 1, and the change in slope of <span class="math inline">\(y\)</span> vs. <span class="math inline">\(x_2\)</span> if <span class="math inline">\(x_1\)</span> increases by 1.</li>
</ul>
</div>
<div id="collinearity" class="section level2">
<h2>Collinearity</h2>
<p>For the <code>hills</code> data set, the two predictors (<em>dist</em> and <em>climb</em>) are themselves correlated.</p>
<pre class="r"><code>cor(hills$dist, hills$climb)</code></pre>
<pre><code>## [1] 0.6523461</code></pre>
<p>The correlation between two predictors complicates the estimation of the effects of each predictor. Since the model coefficients represent the effect of one predictor when the others remain constant, when several of them vary together, it becomes difficult to isolate the effect of each. This problem is generalized to models with more than two predictors, if one of the predictors is correlated with a linear combination of the other predictors: this is called <strong>collinearity</strong>.</p>
<p>The <code>vif</code> function of the <em>car</em> package calculates the variance inflation factor (VIF) of each predictor.</p>
<pre class="r"><code>library(car)
vif(mod_hills)</code></pre>
<pre><code>##     dist    climb 
## 1.740812 1.740812</code></pre>
<p>The VIF is equal to <span class="math inline">\(1 - 1/R^2\)</span>, where <span class="math inline">\(R^2\)</span> is the coefficient of determination of a linear model of the predictor considered as a function of all the others. For example, if one of the predictors can be determined from the value of the others with a $R^2 $ of 0.9, VIF = 10. When the VIF of some predictors exceeds 10, it is recommended to eliminate one of the redundant predictors.</p>
</div>
<div id="example-1" class="section level2">
<h2>Example</h2>
<p>The <code>msleep</code> data frame included with the <em>ggplot</em> package contains sleep data for different mammal species. We choose three columns corresponding to total sleep time, body weight and brain weight.</p>
<pre class="r"><code>msleep2 &lt;- dplyr::select(msleep, sleep_total, bodywt, brainwt)
summary(msleep2)</code></pre>
<pre><code>##   sleep_total        bodywt            brainwt       
##  Min.   : 1.90   Min.   :   0.005   Min.   :0.00014  
##  1st Qu.: 7.85   1st Qu.:   0.174   1st Qu.:0.00290  
##  Median :10.10   Median :   1.670   Median :0.01240  
##  Mean   :10.43   Mean   : 166.136   Mean   :0.28158  
##  3rd Qu.:13.75   3rd Qu.:  41.750   3rd Qu.:0.12550  
##  Max.   :19.90   Max.   :6654.000   Max.   :5.71200  
##                                     NA&#39;s   :27</code></pre>
<p>The <em>bodywt</em> and <em>brainwt</em> variables are very asymmetrical and vary over several orders of magnitude, thus we perform a logarithmic transformation of the three variables.</p>
<pre class="r"><code>msleep2 &lt;- log(msleep2)</code></pre>
<p>On a logarithmic scale, <em>bodywt</em> and <em>brainwt</em> are strongly correlated.</p>
<pre class="r"><code>plot(msleep2)</code></pre>
<p><img src="7E-Linear_regression_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Here is what happens if we fit a linear model using one or both of the predictors.</p>
<pre class="r"><code>summary(lm(sleep_total ~ bodywt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ bodywt, data = msleep2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9757 -0.2712 -0.0096  0.2727  1.0004 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.31575    0.04647  49.830  &lt; 2e-16 ***
## bodywt      -0.10265    0.01389  -7.388 1.19e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4097 on 81 degrees of freedom
## Multiple R-squared:  0.4026, Adjusted R-squared:  0.3952 
## F-statistic: 54.58 on 1 and 81 DF,  p-value: 1.193e-10</code></pre>
<pre class="r"><code>summary(lm(sleep_total ~ brainwt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ brainwt, data = msleep2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.89025 -0.27043 -0.01841  0.30664  0.88271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.69551    0.10037  16.892  &lt; 2e-16 ***
## brainwt     -0.12640    0.02103  -6.011 1.64e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3943 on 54 degrees of freedom
##   (27 observations deleted due to missingness)
## Multiple R-squared:  0.4009, Adjusted R-squared:  0.3898 
## F-statistic: 36.13 on 1 and 54 DF,  p-value: 1.637e-07</code></pre>
<pre class="r"><code>summary(lm(sleep_total ~ bodywt + brainwt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.92817 -0.25572 -0.01949  0.28150  1.01779 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.317881   0.382108   6.066 1.42e-07 ***
## bodywt      -0.105861   0.062787  -1.686   0.0977 .  
## brainwt      0.002517   0.079212   0.032   0.9748    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3877 on 53 degrees of freedom
##   (27 observations deleted due to missingness)
## Multiple R-squared:  0.4314, Adjusted R-squared:  0.4099 
## F-statistic:  20.1 on 2 and 53 DF,  p-value: 3.181e-07</code></pre>
<p>Compare the standard errors of the coefficients of the third model with those for each of the predictors considered separately. We can confirm that these variables are collinear with <code>vif</code>.</p>
<pre class="r"><code>vif(lm(sleep_total ~ bodywt + brainwt, data = msleep2))</code></pre>
<pre><code>##   bodywt  brainwt 
## 14.67386 14.67386</code></pre>
<p>If you need to keep only one of the two variables of the model, which one to choose? We will discuss this problem during the class on model selection.</p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>The <code>lm</code> function fits a linear regression model in R.</p></li>
<li><p>In a multiple linear regression (without interaction), the coefficient associated with a predictor measures the effect of a unit variation of the predictor on the response, if the other predictors remain constant.</p></li>
<li><p>For a model without interaction, the effect of a categorical variable can be represented by a translation of the regression line between the treatments.</p></li>
<li><p>The variance inflation factor (VIF) indicates whether the value of a predictor is strongly correlated with that of the other predictors. A high VIF makes it difficult to estimate coefficients for correlated predictors.</p></li>
<li><p>The confidence interval of a regression line represents the uncertainty on the average value of <span class="math inline">\(y\)</span> for given values of the predictors. The prediction interval represents the uncertainty about the value of a future observation of <span class="math inline">\(y\)</span>, knowing the value of the predictors.</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
