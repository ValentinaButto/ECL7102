<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />



<meta name="date" content="2020-10-26" />

<title>Model evaluation and selection</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Model evaluation and selection</h1>
<h4 class="date"><br/>October 26, 2020</h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li><p>Identify problems due to the collinearity of several predictors.</p></li>
<li><p>Identify problems related to underfitting and overfitting models.</p></li>
<li><p>Know the advantages and disadvantages of different methods to evaluate the fit of models.</p></li>
<li><p>Use AIC to compare different models.</p></li>
<li><p>Combine the predictions of several models.</p></li>
</ul>
</div>
<div id="collinearity" class="section level1">
<h1>Collinearity</h1>
<p>In the previous class, we used the <code>hills</code> dataset, which presents record times (<em>time</em>, in minutes) for Scottish bike races based on the horizontal distance (<em>dist</em>, in miles) and the total change in altitude (<em>climb</em>, in feet).</p>
<pre class="r"><code>library(MASS)
str(hills)</code></pre>
<pre><code>## &#39;data.frame&#39;:    35 obs. of  3 variables:
##  $ dist : num  2.5 6 6 7.5 8 8 16 6 5 6 ...
##  $ climb: int  650 2500 900 800 3070 2866 7500 800 800 650 ...
##  $ time : num  16.1 48.4 33.6 45.6 62.3 ...</code></pre>
<p>We note that the two predictors (<em>dist</em> and <em>climb</em>) are themselves correlated.</p>
<pre class="r"><code>cor(hills$dist, hills$climb)</code></pre>
<pre><code>## [1] 0.6523461</code></pre>
<p>The correlation between two predictors complicates the estimation of the effects of each predictor. Since the model coefficients represent the effect of one predictor when the others remain constant, when several of them vary together, it becomes difficult to isolate the effect of each. This problem is generalized to models with more than two predictors, if one of the predictors is correlated with a linear combination of the other predictors: this is called <strong>collinearity</strong>.</p>
<p>The <code>vif</code> function of the <em>car</em> package calculates the variance inflation factor (VIF) of each predictor.</p>
<pre class="r"><code>library(car)
mod_hills &lt;- lm(time ~ dist + climb, hills)
vif(mod_hills)</code></pre>
<pre><code>##     dist    climb 
## 1.740812 1.740812</code></pre>
<p>The VIF is equal to <span class="math inline">\(1 / (1 - R^2)\)</span>, where <span class="math inline">\(R^2\)</span> is the coefficient of determination of a linear model of the predictor considered as a function of all the others. For example, if one of the predictors can be determined from the value of the others with a $R^2 $ of 0.9, VIF = 10. When the VIF of some predictors exceeds 10, it is recommended to eliminate one of the redundant predictors.</p>
<div id="example" class="section level2">
<h2>Example</h2>
<p>The <code>msleep</code> data frame included with the <em>ggplot</em> package contains sleep data for different mammal species. We choose three columns corresponding to total sleep time, body weight and brain weight.</p>
<pre class="r"><code>msleep2 &lt;- dplyr::select(msleep, sleep_total, bodywt, brainwt)
summary(msleep2)</code></pre>
<pre><code>##   sleep_total        bodywt            brainwt       
##  Min.   : 1.90   Min.   :   0.005   Min.   :0.00014  
##  1st Qu.: 7.85   1st Qu.:   0.174   1st Qu.:0.00290  
##  Median :10.10   Median :   1.670   Median :0.01240  
##  Mean   :10.43   Mean   : 166.136   Mean   :0.28158  
##  3rd Qu.:13.75   3rd Qu.:  41.750   3rd Qu.:0.12550  
##  Max.   :19.90   Max.   :6654.000   Max.   :5.71200  
##                                     NA&#39;s   :27</code></pre>
<p>The <em>bodywt</em> and <em>brainwt</em> variables are very asymmetrical and vary over several orders of magnitude, thus we perform a logarithmic transformation of the three variables.</p>
<pre class="r"><code>msleep2 &lt;- log(msleep2)</code></pre>
<p>On a logarithmic scale, <em>bodywt</em> and <em>brainwt</em> are strongly correlated.</p>
<pre class="r"><code>plot(msleep2)</code></pre>
<p><img src="8E-Model_selection_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Here is what happens if we fit a linear model using one or both of the predictors.</p>
<pre class="r"><code>summary(lm(sleep_total ~ bodywt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ bodywt, data = msleep2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9757 -0.2712 -0.0096  0.2727  1.0004 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.31575    0.04647  49.830  &lt; 2e-16 ***
## bodywt      -0.10265    0.01389  -7.388 1.19e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4097 on 81 degrees of freedom
## Multiple R-squared:  0.4026, Adjusted R-squared:  0.3952 
## F-statistic: 54.58 on 1 and 81 DF,  p-value: 1.193e-10</code></pre>
<pre class="r"><code>summary(lm(sleep_total ~ brainwt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ brainwt, data = msleep2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.89025 -0.27043 -0.01841  0.30664  0.88271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.69551    0.10037  16.892  &lt; 2e-16 ***
## brainwt     -0.12640    0.02103  -6.011 1.64e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3943 on 54 degrees of freedom
##   (27 observations deleted due to missingness)
## Multiple R-squared:  0.4009, Adjusted R-squared:  0.3898 
## F-statistic: 36.13 on 1 and 54 DF,  p-value: 1.637e-07</code></pre>
<pre class="r"><code>summary(lm(sleep_total ~ bodywt + brainwt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.92817 -0.25572 -0.01949  0.28150  1.01779 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.317881   0.382108   6.066 1.42e-07 ***
## bodywt      -0.105861   0.062787  -1.686   0.0977 .  
## brainwt      0.002517   0.079212   0.032   0.9748    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3877 on 53 degrees of freedom
##   (27 observations deleted due to missingness)
## Multiple R-squared:  0.4314, Adjusted R-squared:  0.4099 
## F-statistic:  20.1 on 2 and 53 DF,  p-value: 3.181e-07</code></pre>
<p>Compare the standard errors of the coefficients of the third model with those for each of the predictors considered separately. We can confirm that these variables are collinear with <code>vif</code>.</p>
<pre class="r"><code>vif(lm(sleep_total ~ bodywt + brainwt, data = msleep2))</code></pre>
<pre><code>##   bodywt  brainwt 
## 14.67386 14.67386</code></pre>
<p>If you need to keep only one of the two variables of the model, which one should we choose?</p>
<ul>
<li><p>We can use our prior knowledge or hypotheses about how the system works to determine which of the predictors is most relevant.</p></li>
<li><p>We can compare the fit of models using one predictor or the other.</p></li>
</ul>
<p>The methods presented in the rest of this class address the need to compare different models explaining the same response variable. They apply to any case where we have several candidate models for the same problem.</p>
</div>
</div>
<div id="types-of-model-errors" class="section level1">
<h1>Types of model errors</h1>
<div id="example-1" class="section level2">
<h2>Example</h2>
<p><img src="../images/polatouche.jpg" /></p>
<p>Suppose we want to estimate the population density of the northern flying squirrel (<em>Glaucomys sabrinus</em>) according to different variables, such as:</p>
<ul>
<li>the type of stand (hardwood, coniferous, mixed);</li>
<li>the age of the stand;</li>
<li>average temperatures of the coldest and hottest month;</li>
<li>the continuous forest area around the stand;</li>
<li>the presence or absence of other small mammal species.</li>
</ul>
<p>To do this, we have data from a limited number of sites (e.g. 40 stands). What are the advantages and disadvantages: (1) of a model including the maximum number of predictors and (2) a model including a reduced number (eg 1 or 2) predictors? What is the difference between the types of errors made in each case?</p>
</div>
<div id="model-too-simple-underfitting-and-bias" class="section level2">
<h2>Model too simple: underfitting and bias</h2>
<p>With a model that is too simple, we risk omitting predictors that affect the variable of interest. This is referred to as <em>underfitting</em>. For example, assume that the stand type is not included in the estimated model, but that the flying squirrel is more associated with conifers. In this case, model predictions will underestimate its population density in coniferous stands and overestimate its population density in hardwood stands.</p>
<p>Since this problem creates a systematic error independent of the number of observations, it is often called a <em>bias</em>.</p>
</div>
<div id="model-too-complex-overfitting-and-variance" class="section level2">
<h2>Model too complex: overfitting and variance</h2>
<p>In return, a model with too many parameters may measure associations between variables that are not generalizable effects, but rather coincidences of the dataset used. In that case, the model <em>overfits</em> the data. In general, the more predictors a model includes, the larger the sample needed to estimate effects with the same accuracy. From a graphical point of view, the combined range of <span class="math inline">\(m\)</span> predictors creates a “surface” in <span class="math inline">\(m\)</span> dimensions; the larger the number of dimensions, the more points it takes to fully cover the range of possible values (see illustration below).</p>
<p><img src="../images/densite_pred_en.png" /></p>
<p>The error due to overfitting is random (depends on sampling coincidences) and decreases by increasing the sample size, so it is often called a <em>variance</em>.</p>
</div>
<div id="another-example-fitting-a-polynomial-function" class="section level2">
<h2>Another example: fitting a polynomial function</h2>
<p>During the last class on linear regression, we saw the example of the growth of a plant species as a function of soil moisture. The graphs below show the fit of these data by regressions using polynomials of degree 1 (straight line), 2 and 4.</p>
<p><img src="8E-Model_selection_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>If the linear model may underfit the data here (there appears to be a curvature in the scatter plot), the model of degree 4 clearly overfits if we look slightly past the range of the data.</p>
</div>
<div id="summary-sources-of-error-and-complexity-of-models" class="section level2">
<h2>Summary: Sources of error and complexity of models</h2>
<ul>
<li><p>The exact function linking the response to the predictors is unknown.</p></li>
<li><p>A model that has more adjustable parameters approximates this function with less bias (systematic error), but more variance (sensitivity to random variations in data). The total error is minimized as some number of parameters.</p></li>
<li><p>With a larger sample, the bias remains the same, but the variance decreases. Not only does the total error decrease for all models, but the optimal number of parameters (which minimizes the total error) increases, as shown in the graph below.</p></li>
</ul>
<p><img src="../images/biais_variance_en.png" /></p>
</div>
<div id="prediction-and-explanation" class="section level2">
<h2>Prediction and explanation</h2>
<p>The criteria presented in this class are based on the predictive ability of the models. In other words, we are looking for the model for which the predictions will come closest to the observed values for new samples (different from those used to estimate model parameters).</p>
<p>It often happens, particularly in an experimental context, that the purpose of a model is not necessarily to predict the response, but rather to identify the causes (explanation). In this case, the desire to limit the complexity of the models reflects the principle of parsimony, or “Occam’s razor”, according to which it is preferable to use the minimum of causes necessary to explain a phenomenon.</p>
</div>
</div>
<div id="criteria-for-comparing-models" class="section level1">
<h1>Criteria for comparing models</h1>
<p>In this section, we will see different statistical criteria for comparing the fit of different models to estimate the same response variable.</p>
<div id="coefficient-of-determination-r2" class="section level2">
<h2>Coefficient of determination (<span class="math inline">\(R^2\)</span>)</h2>
<p>The coefficient of determination indicates the fraction of the total variance of the response that is explained by the model. In R, the result of <code>lm</code> reports two versions of <span class="math inline">\(R^2\)</span>, <em>Multiple R-squared</em> and <em>Adjusted R-squared</em>. The first is the <span class="math inline">\(R^2\)</span> we have already seen, based on the residual and total sum of squares:</p>
<p><span class="math display">\[ R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y_i})^2}{\sum_{i=1}^n (y_i - \bar{y})^2} \]</span></p>
<p>The “multiple” <span class="math inline">\(R^2\)</span> always increases when adding a new predictor to the model, since each new predictor has a correlation (even if minimal and random) with the response. That measure is biased towards more complex models.</p>
<p>The adjusted <span class="math inline">\(R^2\)</span> divides each sum of squares by the appropriate number of degrees of freedom. Since a model with more parameters has fewer residual degrees of freedom, the adjusted <span class="math inline">\(R^2\)</span> may decrease if we add a parameter that does not contribute enough to explain the response. It is therefore a less biased estimator of the population <span class="math inline">\(R^2\)</span>.</p>
<p>Either version of <span class="math inline">\(R^2\)</span> is suitable for comparing two linear models with the same number of adjustable parameters. However, the adjusted <span class="math inline">\(R^2\)</span> does not penalize more complex models as much as the AIC, which will be presented later.</p>
</div>
<div id="significance-tests" class="section level2">
<h2>Significance tests</h2>
<p>If two models only differ by one predictor, e.g.: <code>y ~ x1 + x2</code> and <code>y ~ x1 + x2 + x3</code>, we can choose the more complex model if the effect of the new predictor <code>x3</code> is significant. We have already used this technique to determine whether or not to include the interaction term in a two-factor ANOVA model.</p>
<p>There are sequential methods (<em>stepwise</em>) that choose between more than two models by adding or removing one variable at a time. These methods will be presented in more detail later. For the moment, note that the selection of models based on significance tests has several disadvantages.</p>
<ul>
<li><p>The <span class="math inline">\(p\)</span>-value measures the probability of obtaining the observed results according to the null hypothesis, not the predictive power of the model. Therefore, it helps us answer questions like: “Is the effect of <span class="math inline">\(x_3\)</span> significantly different from zero?”, but not: "What is the best model between <span class="math inline">\(y \sim x_1 + x_2\)</span> and <span class="math inline">\(y \sim x_3\)</span>?</p></li>
<li><p>The significance threshold for the inclusion of a variable (e.g. <span class="math inline">\(\alpha = 0.05\)</span>) is arbitrary.</p></li>
<li><p>With sequential methods, we must consider the increase in the probability of type I error (problem of multiple comparisons).</p></li>
</ul>
</div>
<div id="validation-set" class="section level2">
<h2>Validation set</h2>
<p>As we saw earlier, a model with several parameters can overfit the data, that is, it detects fortuitous associations between variables found in one specific sample, which cannot be generalized to other samples.</p>
<p>From this perspective, the best way to evaluate the predictive power of a model is to measure its prediction error (for example, the residual sum of squares: <span class="math inline">\(\sum (y_i - \hat{y_i})^2\)</span>) for a <em>different</em> dataset from the one used to estimate the model parameters. For example, with measurements taken at 40 sites, we can estimate the parameters from the data of 30 sites (randomly selected), then evaluate the prediction error of the model on the 10 remaining sites.</p>
<p>This separation of the data into a <em>training set</em> and a <em>validation set</em> is used quite frequently when a very large sample is available. It is best to put most of the data in the training set, for example a 70%/30% or 80%/20% split.</p>
<p>For smaller datasets, this type of validation is not efficient. On the one hand, we can not afford to leave aside some of our data for validation, and on the other hand, a small validation set (e.g. 10 cases) is not very representative.</p>
</div>
<div id="cross-validation" class="section level2">
<h2>Cross-validation</h2>
<p>Cross-validation consists in randomly dividing the observations into groups and measuring the prediction error of the observations of a group according to a model adjusted to the remaining observations.</p>
<p>For example, if each group has only one observation (leave-one-out cross-validation), the sum of the square prediction errors is equal to <span class="math inline">\(\sum (y_i - \hat{y_i})^2\)</span>, where each predicted value <span class="math inline">\(\hat{y_i}\)</span> comes from the model fitted without observation <span class="math inline">\(i\)</span>. If we use this method to compare the prediction error of different models from <span class="math inline">\(n\)</span> observations, each of the models must be fitted <span class="math inline">\(n\)</span> times, leaving aside one observation each time.</p>
<p>If the number of observations <span class="math inline">\(n\)</span> is large, it may be more practical to divide the observations into <span class="math inline">\(k\)</span> groups (k-fold cross-validation), for example <span class="math inline">\(k\)</span> = 10, and to fit each model <span class="math inline">\(k\)</span> times, leaving one group aside each time.</p>
<p>Cross-validation can be coded in R using a loop, but there are also packages where these functions are already coded and ready to be applied to various models, such as the <strong>cvTools</strong> package.</p>
</div>
</div>
<div id="comparison-of-models-with-aic" class="section level1">
<h1>Comparison of models with AIC</h1>
<div id="origin" class="section level2">
<h2>Origin</h2>
<p>The Akaike Information Criterion (AIC), proposed by Hirotugu Akaike in 1973, compares the fit of different models.</p>
<p>AIC is based on information theory. Imagine a function <span class="math inline">\(f\)</span> that associates to each individual in the population a value the exact value of the response variable <span class="math inline">\(y\)</span>, while <span class="math inline">\(g\)</span> is a model that tries to approximate <span class="math inline">\(f\)</span>. The AIC approximates the loss of information incurred by estimating the true distribution <span class="math inline">\(f\)</span> by the model <span class="math inline">\(g\)</span>.</p>
<p>How can we estimate the “distance” between our model and an ideal model <span class="math inline">\(f\)</span> which in unknown? In fact, the AIC measures this distance up to a constant, which is sufficient to compare different models according to their relative level of fit. The absolute value of the AIC has no meaning, all that matters is the difference in AIC between models.</p>
</div>
<div id="definition-of-aic" class="section level2">
<h2>Definition of AIC</h2>
<p>AIC is defined as:</p>
<p><span class="math display">\[ AIC = -2 \log(L) + 2 K \]</span></p>
<p>where <span class="math inline">\(L\)</span> is the <em>likelihood</em> function and <span class="math inline">\(K\)</span> is the number of parameters estimated by the model. According to this criterion, a smaller AIC corresponds to a better model.</p>
<p>The likelihood <span class="math inline">\(L(\theta | y)\)</span> is equal to the probability of the observed values of <span class="math inline">\(y\)</span> according to the value of the model parameters <span class="math inline">\(\theta\)</span>. The <strong>maximum likelihood</strong> method estimates model parameters as those that maximize the likelihood. This is a general method for estimating the parameters of a model; for a linear model, the maximum likelihood gives the same estimates as the least squares method. The value of <span class="math inline">\(L\)</span> at its maximum, corresponding to the estimated parameters of the model, is therefore used in the AIC equation above.</p>
<p>Like the <span class="math inline">\(R^2\)</span>, the likelihood tends to increase with each parameter added to the model. The second term of the AIC, proportional to the number of estimated parameters <span class="math inline">\(K\)</span>, therefore serves to penalize more complex models.</p>
<p><strong>Note</strong></p>
<ol style="list-style-type: decimal">
<li>For a linear model, <span class="math inline">\(K\)</span> must count the intercept and the estimated residual variance (<span class="math inline">\(\sigma^2\)</span>).</li>
</ol>
<p>E.g.: For the model <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon\)</span>, <span class="math inline">\(K = 4\)</span> since we estimate three <span class="math inline">\(\beta\)</span> coefficients as well as the variance of the <span class="math inline">\(\epsilon\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>The AIC could have been defined as <span class="math inline">\(-\log(L) + K\)</span>. The factor of 2 has been added for historical reasons (to link the AIC with other existing measures of fit).</li>
</ol>
</div>
<div id="aic-and-cross-validation" class="section level2">
<h2>AIC and cross-validation</h2>
<p>It has been shown that the comparison of models with the AIC is equivalent to a leave-one-out cross-validation. Thus, the AIC allows us to estimate the predictive power of our models on new data, without having to repeatedly fit each model several times, as in the cross-validation method.</p>
</div>
<div id="aicc-for-small-samples" class="section level2">
<h2>AICc for small samples</h2>
<p>The AIC formula presented above is a good approximation when the number of observations <span class="math inline">\(n\)</span> is sufficiently large. If <span class="math inline">\(n\)</span> is small relative to the number of estimated parameters (if <span class="math inline">\(n/K &lt; 40\)</span>, according to the Burnham and Anderson manual), this formula does not penalize complex models enough. In this case, the AIC is replaced by the AICc, which includes an additional correction:</p>
<p><span class="math display">\[ AICc = -2 \log(L) + 2 K \left( \frac{n}{n-K-1} \right) \]</span> The fraction <span class="math inline">\(n/(n-K-1)\)</span> is greater than 1, so the second term of the AICc is slightly larger than <span class="math inline">\(2K\)</span>, but the difference between the two decreases as <span class="math inline">\(n\)</span> increases.</p>
<p>When comparing several models, the same metric must be used for each one. The AICc is therefore used for all models as soon as one of the models has a small <span class="math inline">\(n/K\)</span> ratio.</p>
</div>
<div id="ranking-models-with-aic" class="section level2">
<h2>Ranking models with AIC</h2>
<p>As mentioned above, the absolute value of the AIC is not important, only differences in AIC between models matter. After calculating the AIC for different models, we rank them according to the difference between their AIC and the minimum AIC (which corresponds to the best model): <span class="math inline">\(\Delta AIC = AIC - \min AIC\)</span>. The best model has a <span class="math inline">\(\Delta AIC = 0\)</span>.</p>
<p>Of course, since our data are based on random sampling, it is not certain that the best model identified by the AIC would still be the same with a different sample.</p>
<p>The expression:</p>
<p><span class="math display">\[ e^{-\frac{\Delta AIC}{2} } \]</span></p>
<p>corresponds to the <em>evidence ratio</em> of each model vs. the one having the minimum AIC. For example, <span class="math inline">\(\Delta AIC = 2\)</span> corresponds to a ratio of 0.37 (~3 times less likely), while <span class="math inline">\(\Delta AIC = 10\)</span> corresponds to a ratio of 0.0067 (~150 times less likely).</p>
<p>If we have <span class="math inline">\(m\)</span> candidate models, we can normalize the evidence ratios by their sum to obtain the Akaike weight <span class="math inline">\(w_i\)</span> of model <span class="math inline">\(i\)</span>.</p>
<p><span class="math display">\[ w_i = \frac{e^{\frac{-\Delta AIC_i}{2}}}{\sum_{j=1}^{m} e^{\frac{-\Delta AIC_j}{2}}}\]</span></p>
<p>Since the sum of the $ w_i $ equals 1, these weights represent the probability that each model would be identified as the best model by the AIC, if the sampling was repeated several times.</p>
</div>
<div id="example-2" class="section level2">
<h2>Example</h2>
<p>We use Johnson and Simberloff’s dataset of the number of vascular plant species from different British Isles, which was already introduced in a previous lab.</p>
<pre class="r"><code>iles &lt;- read.csv(&quot;../donnees/britain_species.csv&quot;)
str(iles)</code></pre>
<pre><code>## &#39;data.frame&#39;:    42 obs. of  7 variables:
##  $ island      : chr  &quot;Ailsa&quot; &quot;Anglesey&quot; &quot;Arran&quot; &quot;Barra&quot; ...
##  $ area        : num  0.8 712.5 429.4 18.4 31.1 ...
##  $ elevation   : int  340 127 874 384 226 1343 210 103 143 393 ...
##  $ soil_types  : int  1 3 4 2 1 16 1 3 1 1 ...
##  $ latitude    : num  55.3 53.3 55.6 57 60.1 54.3 57.1 56.6 56.1 56.9 ...
##  $ dist_britain: num  14 0.2 5.2 77.4 201.6 ...
##  $ species     : int  75 855 577 409 177 1666 300 443 482 453 ...</code></pre>
<p>First, we modify the dataset to exclude the island of Great Britain and to apply a logarithmic transformation to the size of the islands, their distance from Great Britain and their number of species.</p>
<pre class="r"><code>iles2 &lt;- filter(iles, island != &quot;Britain&quot;) %&gt;%
    mutate(log_area = log(area), log_dist = log(dist_britain), 
           log_sp = log(species))</code></pre>
<p>The most complex model that we consider is the following, where the number of species depends on the size of the island, its distance from Great Britain and its latitude.</p>
<pre class="r"><code>mod_comp &lt;- lm(log(species) ~ log(area) + log(dist_britain) + latitude, data = iles2)
summary(mod_comp)</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(species) ~ log(area) + log(dist_britain) + latitude, 
##     data = iles2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.19638 -0.09484  0.04124  0.22668  0.52043 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       14.36786    1.94448   7.389 8.65e-09 ***
## log(area)          0.21112    0.03165   6.670 7.83e-08 ***
## log(dist_britain)  0.02452    0.05251   0.467    0.643    
## latitude          -0.16775    0.03624  -4.629 4.42e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3883 on 37 degrees of freedom
## Multiple R-squared:  0.6941, Adjusted R-squared:  0.6693 
## F-statistic: 27.99 on 3 and 37 DF,  p-value: 1.267e-09</code></pre>
<p>Let’s first check the diagnostic plots for the full model:</p>
<p><img src="8E-Model_selection_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Three islands have large negative residuals (fewer species than expected) and are far from the normal Q-Q line, but we consider the overall fit acceptable.</p>
<p>Next, we create a list of models that include only one or two predictors from the full model, as well as a null model: <code>log_sp ~ 1</code>, which represents a constant function (independent of the predictors).</p>
<pre class="r"><code>liste_mod &lt;- list(
    nul = lm(log_sp ~ 1, iles2),
    area = lm(log_sp ~ log_area, iles2),
    dist = lm(log_sp ~ log_dist, iles2),
    lat = lm(log_sp ~ latitude, iles2),
    area_dist = lm(log_sp ~ log_area + log_dist, iles2),
    area_lat = lm(log_sp ~ log_area + latitude, iles2),
    dist_lat = lm(log_sp ~ log_dist + latitude, iles2),
    comp = lm(log_sp ~ log_area + log_dist + latitude, iles2)
)</code></pre>
<p>We are now ready to compare these models with the AIC. We will use the <strong>AICcmodavg</strong> package, which has several functions for model comparison and multimodel inference.</p>
<p>The full model here has 5 estimated parameters and 41 observations, so it is better to use the AICc. The <code>AICc</code> function calculates the AICc for a single model.</p>
<pre class="r"><code>library(AICcmodavg)
AICc(mod_comp)</code></pre>
<pre><code>## [1] 46.29878</code></pre>
<p>Instead of calling <code>AICc</code> separately for each model, we will use the<code>aictab</code> function to produce an AICc comparison table from our list of models.</p>
<pre class="r"><code>aictab(liste_mod)</code></pre>
<pre><code>## 
## Model selection based on AICc:
## 
##           K  AICc Delta_AICc AICcWt Cum.Wt     LL
## area_lat  4 43.94       0.00   0.77   0.77 -17.41
## comp      5 46.30       2.36   0.23   1.00 -17.29
## area_dist 4 62.42      18.49   0.00   1.00 -26.66
## area      3 69.16      25.23   0.00   1.00 -31.26
## lat       3 74.93      31.00   0.00   1.00 -34.14
## dist_lat  4 76.07      32.13   0.00   1.00 -33.48
## dist      3 78.85      34.92   0.00   1.00 -36.10
## nul       2 87.47      43.53   0.00   1.00 -41.58</code></pre>
<p>The table presents the models in ascending order of AICc, with the following additional information:</p>
<ul>
<li>The values of <span class="math inline">\(\log(L)\)</span> (<code>LL</code>) and <span class="math inline">\(K\)</span> used in the calculation of the AICc.</li>
<li>The <span class="math inline">\(\Delta AICc\)</span> (<code>Delta_AICc</code>), the Akaike weights (<code>AICcWt</code>) and the cumulative weights (<code>Cum.Wt</code>).</li>
</ul>
<p>We see that the model including the area and latitude receives 77% of the weight, the complete model receives 23% of the weight, and the rest of the models have a negligible weight.</p>
</div>
<div id="tips-and-things-to-watch-for" class="section level2">
<h2>Tips and things to watch for</h2>
<ul>
<li><p>For the AIC values (or AICc) to be comparable, all models must have the same response variable. As an example, we cannot compare models where the response is <code>species</code> with others where the response is <code>log(species)</code>.</p></li>
<li><p>Each model must also be based on the same set of observations. Particular attention should be paid to missing <code>NA</code> values in R. R automatically excludes from a model any row that has a missing value for one of the variables included in the model. Before estimating a series of models, be sure to eliminate all rows that have at least one missing value for one of the variables of interest.</p></li>
<li><p>In the previous example, all models were <em>nested</em>, that is, each model contained a subset of the predictors of the full model. The AIC can also compare non-nested models.</p></li>
<li><p>The AIC compares models with each other, but does not tell us if the “best” model has a good fit. For a case where all models are nested, it is sufficient to check the fit of the most complex model, as we did above. In this case, any simpler model with a better AIC will also have a good fit.</p></li>
<li><p>In the example, the two predictors of the best model according to the AIC (<code>log_area</code> and <code>lat</code>) were also the two predictors with a significant effect in the full model. This is not always the case.</p></li>
<li><p>Avoid relying on the <span class="math inline">\(p\)</span>-values and confidence intervals of the parameters of the best model chosen by the AIC. The results from a single model do not take into account the fact that we compared several models to choose the best one in the end; like all multiple comparison problems, the <span class="math inline">\(p\)</span>-values and confidence intervals of the chosen model will be too optimistic.</p></li>
</ul>
</div>
</div>
<div id="how-to-choose-the-set-of-models-to-compare" class="section level1">
<h1>How to choose the set of models to compare?</h1>
<p>In their textbook on model selection with AIC (see the references section at the end), Burnham and Anderson recommend choosing a restricted set of models based on subject knowledge and interesting scientific hypotheses.</p>
<p>It is important to note that the number of possible models increases rapidly with the number of predictors, even if we limit ourselves to linear models without interactions. For <span class="math inline">\(k\)</span> predictors, there are <span class="math inline">\(2^k\)</span> possible models. In the previous example, we had 8 possible models for 3 predictors; with 7 predictors, that number increases to 128!</p>
<div id="stepwise-selection-algorithms" class="section level2">
<h2>Stepwise selection algorithms</h2>
<p>There are forward, backward and bidirectional stepwise selection algorithms, like the <code>stepAIC</code> function in R, which aim to find the best model according to the AIC without having to evaluate all the possible models.</p>
<p>For example, a forward selection process works like this:</p>
<ol style="list-style-type: decimal">
<li>Calculate the AIC of the null model (no predictors);</li>
<li>Evaluate all models with 1 predictor (<code>y ~ x1</code>, <code>y ~ x2</code>, etc.), keep the best one if its AIC is less than that of the null model.</li>
<li>Evaluate all models with 2 predictors, including the 1st predictor chosen in Step 2, keep the best one if its AIC is better than the model from Step 2.</li>
<li>Continue this way until none of the more complex models get a lower AIC than the model chosen in the previous step.</li>
</ol>
<p>Stepwise methods have several disadvantages:</p>
<ul>
<li>It is not guaranteed that they will find the best model.</li>
<li>These methods allow model selection, but without being able to calculate Akaike weights, we cannot know if several other models were almost as good as the selected one.</li>
<li>They encourage the comparison of a very large number of models, which is not advised for the reasons mentioned above.</li>
</ul>
</div>
</div>
<div id="multimodel-predictions" class="section level1">
<h1>Multimodel predictions</h1>
<p>As in the last lab, suppose that we wanted to predict the number of species for new islands with known predictor values.</p>
<pre class="r"><code>iles_nouv &lt;- data.frame(area = c(1, 40), dist_britain = c(5, 20), 
                        latitude = c(60, 55)) %&gt;%
    mutate(log_area = log(area), log_dist = log(dist_britain))
iles_nouv</code></pre>
<pre><code>##   area dist_britain latitude log_area log_dist
## 1    1            5       60 0.000000 1.609438
## 2   40           20       55 3.688879 2.995732</code></pre>
<p>We could make the predictions from the best model identified by the AIC. However, when several models are quite plausible, it is possible to improve the predictions by using the <em>weighted average</em> of the values predicted by the different models, based on their Akaike weights:</p>
<p><span class="math display">\[ \hat{y} = \sum_{i = 1}^m w_i \hat{y_i} \]</span></p>
<p>In this equation, <span class="math inline">\(\hat{y_i}\)</span> is the prediction from model <span class="math inline">\(i\)</span> and <span class="math inline">\(\hat{y}\)</span> is the multimodel average prediction.</p>
<p>In the <em>AICcmodavg</em> package, the <code>modavgPred</code> function calculates multimodel average predictions.</p>
<pre class="r"><code>modavgPred(liste_mod, newdata = iles_nouv)</code></pre>
<pre><code>## 
## Model-averaged predictions on the response scale
## based on entire model set and 95% confidence interval:
## 
##   mod.avg.pred uncond.se lower.CL upper.CL
## 1        4.411     0.183    4.053    4.769
## 2        5.979     0.089    5.803    6.154</code></pre>
<p>Note that the interval provided is a confidence interval, not a prediction interval. In other words, it tells us about the uncertainty of the average value of <code>log_sp</code> for these predictor values, not the uncertainty of <code>log_sp</code> for a single island.</p>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>The variance inflation factor (VIF) indicates whether the value of a predictor is strongly correlated with that of the other predictors. A high VIF makes it difficult to estimate coefficients for correlated predictors.</p></li>
<li><p>The selection between several models with different levels of complexity requires a compromise between bias (underfitting) and variance (overfitting).</p></li>
<li><p>To estimate the goodness of fit of a model, it is useful to compare its predictions to new observations, different from those used to estimate the parameters of the model.</p></li>
<li><p>The AIC (or AICc, for small samples) is based on the likelihood of the model, with a penalty for models with a larger number of parameters.</p></li>
<li><p>AIC approximates the relative predictive ability of different models and can therefore be used to rank them.</p></li>
<li><p>The absolute value of AIC has no meaning. Only AIC differences between models based on the same observations can be interpreted.</p></li>
<li><p>Akaike weights, calculated from AIC differences between models, estimate the probability that each model would be identified as the best model if sampling was repeated. These weights are also used to average the predictions of different models.</p></li>
</ul>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Burnham, K.P and Anderson, D.R. (2002) <em>Model selection and multimodel inference : a practical information-theoretic approach</em>, 2nd ed. Springer-Verlag, New York.</p>
<p>Anderson, D.R. and Burnham, K.P. (2002) Avoiding pitfalls when using information-theoretic methods. <em>The Journal of Wildlife Management</em> 66: 912-918.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
