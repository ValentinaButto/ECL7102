<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="date" content="2019-09-30" />

<title>Two-way ANOVA and randomized complete blocks</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Two-way ANOVA and randomized complete blocks</h1>
<h4 class="date"><br/>September 30, 2019</h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<p>In the last class, we saw examples of one-way ANOVA. In this course, we will use ANOVA to estimate the additive effect of two factors (two categorical variables) as well as their interaction. We will also study the analysis of experimental plans in blocks (randomized complete blocks).</p>
<p>Finally, we will reformulate the ANOVA model as a linear regression model and we will consider different coding options for categorical variables (factors).</p>
</div>
<div id="reminder-one-way-anova" class="section level1">
<h1>Reminder: One-way ANOVA</h1>
<p>In the one-way ANOVA model, the response <span class="math inline">\(y_{ik}\)</span> of the individual <span class="math inline">\(k\)</span> in group <span class="math inline">\(i\)</span> is the sum of three effects: the grand mean of the population (<span class="math inline">\(\mu\)</span>), the difference between the mean of the group <span class="math inline">\(i\)</span> and the grand mean (<span class="math inline">\(\alpha_i\)</span>) and the residual (<span class="math inline">\(\epsilon_{ik}\)</span>) or “error” which is the part of the response not explained by the model.</p>
<p><span class="math display">\[ y_{ik} = \mu + \alpha_i + \epsilon_{ik} \]</span> The model also assumes that the residuals are independent and all follow the same normal distribution.</p>
<p><span class="math display">\[ \epsilon_{ik} \sim N(0, \sigma) \]</span></p>
</div>
<div id="two-way-anova-without-interaction" class="section level1">
<h1>Two-way ANOVA without interaction</h1>
<p>To illustrate two-way ANOVA, we will first use the <a href="../donnees/growth.csv">growth.csv</a> dataset from the textbook <em>Statistics: An Introduction Using R</em>. The experiment compares the weight gain of 48 animals following three types of diet with four types of supplements. There are 12 groups (all combinations of the 3 diets and 4 supplements) of 4 individuals each.</p>
<pre class="r"><code>growth &lt;- read.csv(&quot;../donnees/growth.csv&quot;)
str(growth)</code></pre>
<pre><code>## &#39;data.frame&#39;:    48 obs. of  3 variables:
##  $ supplement: Factor w/ 4 levels &quot;agrimore&quot;,&quot;control&quot;,..: 3 3 3 3 2 2 2 2 4 4 ...
##  $ diet      : Factor w/ 3 levels &quot;barley&quot;,&quot;oats&quot;,..: 3 3 3 3 3 3 3 3 3 3 ...
##  $ gain      : num  17.4 16.8 18.1 15.8 17.7 ...</code></pre>
<pre class="r"><code>ggplot(growth, aes(x = supplement, y = gain, color = diet)) +
    # position_dodge puts horizontal space between points of different colors
    geom_point(position = position_dodge(width = 0.3)) + 
    scale_color_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="6E-Two_way_ANOVA_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We can represent this experiment with the following model, where <span class="math inline">\(y_{ijk}\)</span> is the weight gain for individual <span class="math inline">\(k\)</span> having diet <span class="math inline">\(i\)</span> and supplement <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[ y_{ijk} = \mu + \alpha_i + \beta_j + \epsilon_{ijk} \]</span></p>
<p>In this equation, <span class="math inline">\(\mu\)</span> is the grand mean, <span class="math inline">\(\alpha_i\)</span> represents the effect of treatment <span class="math inline">\(i\)</span> in the first factor (factor A) and <span class="math inline">\(\beta_j\)</span> represents the effect of treatment <span class="math inline">\(j\)</span> in the second factor (factor B).</p>
<p>Note that this model is <strong>additive</strong>. In our example, the combined effect of a diet and a supplement is the sum of the two effects taken separately. We will see later how to model non-additive effects.</p>
<p>The different coefficients of the model are estimated as follows (the hat indicates it is an estimate):</p>
<ul>
<li>Grand mean: <span class="math inline">\(\hat{\mu} = \bar{y}\)</span>.</li>
<li>Effect of treatment <span class="math inline">\(i\)</span> of factor A: <span class="math inline">\(\hat{\alpha_i} = \bar{y_i} - \bar{y}\)</span> (difference between the mean of individuals receiving the treatment and the grand mean).</li>
<li>Effect of treatment <span class="math inline">\(j\)</span> for factor B: <span class="math inline">\(\hat{\beta_j} = \bar{y_j} - \bar{y}\)</span>.</li>
</ul>
<p>The residuals are equal to <span class="math inline">\(y_{ijk} - \bar{y} - (\bar{y_i} - \bar{y}) - (\bar{y_j} - \bar{y})\)</span>, or by simplifying: <span class="math inline">\(y_{ijk} - \bar{y_i} - \bar{y_j} + \bar{y}\)</span>.</p>
<div id="two-way-anova-table-no-interaction" class="section level2">
<h2>Two-way ANOVA table (no interaction)</h2>
<p>Here is the ANOVA table corresponding to this model if we have <span class="math inline">\(l\)</span> treatments for factor A, <span class="math inline">\(m\)</span> treatments for factor B and <span class="math inline">\(n\)</span> replicates for each combination of treatments (thus <span class="math inline">\(lmn\)</span> total observations).</p>
<table style="width:100%;">
<colgroup>
<col width="5%" />
<col width="31%" />
<col width="31%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Sum of squares (SS)</th>
<th>Degrees of freedom (df)</th>
<th>Mean square (MS)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Factor A</td>
<td><span class="math inline">\(SSA = \sum_{i = 1}^l mn (\bar{y_i} - \bar{y})^2\)</span></td>
<td><span class="math inline">\(l - 1\)</span></td>
<td><span class="math inline">\(MSA = \frac{SSA}{l - 1}\)</span></td>
</tr>
<tr class="even">
<td>Factor B</td>
<td><span class="math inline">\(SSB = \sum_{j = 1}^m ln (\bar{y_j} - \bar{y})^2\)</span></td>
<td><span class="math inline">\(m - 1\)</span></td>
<td><span class="math inline">\(MSB = \frac{SSB}{m - 1}\)</span></td>
</tr>
<tr class="odd">
<td>Residual</td>
<td><span class="math inline">\(SSE = \sum_{i = 1}^l \sum_{j = 1}^m \sum_{k = i}^n (y_{ijk} - \bar{y_i} - \bar{y_j} + \bar{y})^2\)</span></td>
<td><span class="math inline">\(lmn - l - m + 1\)</span></td>
<td><span class="math inline">\(MSE = \frac{SSE}{lmn - l - m + 1}\)</span></td>
</tr>
<tr class="even">
<td>Total</td>
<td><span class="math inline">\(SST = \sum_{i = 1}^l \sum_{j = 1}^m \sum_{k = i}^n (y_{ijk} - \bar{y})^2\)</span></td>
<td><span class="math inline">\(lmn - 1\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<p>The number of degrees of freedom for mean square residual (MSE) is always the total number of observations, minus 1 degree of freedom per estimated mean (1 for the grand mean, <span class="math inline">\(l - 1\)</span> for the effects of factor A and <span class="math inline">\(m - 1\)</span> for the effects of factor B).</p>
<p>In one-way ANOVA, the ratio <span class="math inline">\(MSA / MSE\)</span> was compared to the <span class="math inline">\(F\)</span> distribution to determine if factor A had a significant effect. Here, two independent <span class="math inline">\(F\)</span> tests are conducted based on <span class="math inline">\(MSA / MSE\)</span> and <span class="math inline">\(MSB / MSE\)</span>, so we get a <span class="math inline">\(p\)</span>-value for each of the two factors.</p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>In R, a model with two additive factors is represented by the equation <code>response ~ factorA + factorB</code>.</p>
<pre class="r"><code>aov_growth_add &lt;- aov(gain ~ diet + supplement, data = growth)</code></pre>
<p>Let’s first check the diagnostic plots, then the summary results of the ANOVA.</p>
<p><img src="6E-Two_way_ANOVA_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>summary(aov_growth_add)</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## diet         2 287.17  143.59   92.36 4.20e-16 ***
## supplement   3  91.88   30.63   19.70 3.98e-08 ***
## Residuals   42  65.30    1.55                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Both factors have a very significant effect. By consulting the results from Tukey’s range test, we see that all three diets have a different effect (wheat &lt; oats &lt; barley). Among the supplements, <em>agrimore</em> and <em>supersupp</em> have a greater effect than <em>supergain</em> and <em>control</em>.</p>
<pre class="r"><code>TukeyHSD(aov_growth_add)</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = gain ~ diet + supplement, data = growth)
## 
## $diet
##                   diff       lwr       upr p adj
## oats-barley  -3.092817 -4.163817 -2.021817 0e+00
## wheat-barley -5.990298 -7.061298 -4.919297 0e+00
## wheat-oats   -2.897481 -3.968481 -1.826481 2e-07
## 
## $supplement
##                           diff        lwr        upr     p adj
## control-agrimore    -2.6967005 -4.0583332 -1.3350677 0.0000234
## supergain-agrimore  -3.3814586 -4.7430914 -2.0198259 0.0000003
## supersupp-agrimore  -0.7273521 -2.0889849  0.6342806 0.4888738
## supergain-control   -0.6847581 -2.0463909  0.6768746 0.5400389
## supersupp-control    1.9693484  0.6077156  3.3309811 0.0020484
## supersupp-supergain  2.6541065  1.2924737  4.0157392 0.0000307</code></pre>
</div>
<div id="coefficient-of-determination" class="section level2">
<h2>Coefficient of determination</h2>
<p>As we saw in the previous class, the sum of the squared differences between each observation and the grand mean can be broken down by the sum of the differences due to treatments and residual effects. For two-factor ANOVA, we have the relationship: <span class="math inline">\(SST = SSA + SSB + SSE\)</span>. The ratios <span class="math inline">\(SSA / SST\)</span> and <span class="math inline">\(SSB / SST\)</span> can thus be interpreted as the fractions of the total variance in the response explained by factor A and factor B, respectively. The ratio <span class="math inline">\(SSE / SST\)</span> is the fraction of the variance unexplained by the model (residual error).</p>
<p>In our previous example, <span class="math inline">\(SST = 287.17 + 91.88 + 65.30 = 444.35\)</span>.</p>
<pre class="r"><code>summary(aov_growth_add)</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## diet         2 287.17  143.59   92.36 4.20e-16 ***
## supplement   3  91.88   30.63   19.70 3.98e-08 ***
## Residuals   42  65.30    1.55                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We calculate a ratio of 0.646 for factor A, 0.207 for factor B and 0.147 for residuals. In this experiment, most of the variation in weight gain is associated with the change in diet.</p>
<p>Since <span class="math inline">\(SSE / SST\)</span> is the fraction of the variance unexplained by the model, the fraction of the variance explained by the factors included in the model is equal to:</p>
<p><span class="math display">\[ R^2 = 1 - \frac{SSE}{SST} \]</span></p>
<p>The value <span class="math inline">\(R^2\)</span> is called the <strong>coefficient of determination</strong>. In the previous example, <span class="math inline">\(R^2 = 0.853\)</span>.</p>
<p>During the class on the hypothesis testing, it was recommended to present three types of results following a test:</p>
<ul>
<li>the probability that the measured effect is due to chance (<span class="math inline">\(p\)</span>-value);</li>
<li>the estimate and the confidence interval of the measured effect; and</li>
<li>the magnitude of the effect compared to the variance of the individual data points.</li>
</ul>
<p>The coefficient of determination <span class="math inline">\(R^2\)</span> answers the third question: Which part of the observed variation is due to the effect of the treatments or predictors measured?</p>
<p>Finally, a reminder: when we speak of the <em>effect</em> of a predictor or the fraction of the variance <em>explained</em>, this does not always mean that there is a cause-and-effect relationship between the predictor and the response. Our ability to interpret a statistical association (or correlation) as a cause-and-effect relationship does not depend on the magnitude of the effect, but rather on the controls established in the experimental design: independent variation of factors, use of control group, random assignment of treatments, etc.</p>
</div>
</div>
<div id="two-way-anova-with-interaction" class="section level1">
<h1>Two-way ANOVA with interaction</h1>
<div id="example-1" class="section level2">
<h2>Example</h2>
<p>The <a href="../donnees/antibiot.csv">antibiot.csv</a> dataset contains measures of bacterial spread (surface covered in mm<span class="math inline">\(^2\)</span>) as a function of the humidity (“sec” = dry, “humide” = humid) and the concentration of antibiotic (“faible” = low, “modérée” = moderate, “élevée” = high).</p>
<pre class="r"><code># fileEncoding = &quot;UTF-8&quot; is to read the French accents correctly
antibiot &lt;- read.csv(&quot;../donnees/antibiot.csv&quot;, fileEncoding = &quot;UTF-8&quot;)
str(antibiot)</code></pre>
<pre><code>## &#39;data.frame&#39;:    30 obs. of  3 variables:
##  $ Surface      : num  2.1 2.73 1.86 2.36 2.2 ...
##  $ Humidité     : Factor w/ 2 levels &quot;humide&quot;,&quot;sec&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ Concentration: Factor w/ 3 levels &quot;élevée&quot;,&quot;faible&quot;,..: 2 2 2 2 2 3 3 3 3 3 ...</code></pre>
<p>When R improts a dataset with <code>read.csv</code>, non-numerical columns are imported as factors with the order of categories (<code>levels</code>) set by alphabetical order.</p>
<pre class="r"><code>levels(antibiot$Concentration)</code></pre>
<pre><code>## [1] &quot;élevée&quot;  &quot;faible&quot;  &quot;modérée&quot;</code></pre>
<p>We can reorder the levels with the <code>factor</code> function.</p>
<pre class="r"><code>antibiot$Concentration &lt;- factor(antibiot$Concentration, 
                                 levels = c(&quot;faible&quot;, &quot;modérée&quot;, &quot;élevée&quot;))
levels(antibiot$Concentration)</code></pre>
<pre><code>## [1] &quot;faible&quot;  &quot;modérée&quot; &quot;élevée&quot;</code></pre>
<p>Here is the graph of these data. Would a model with additive effects of antibiotic concentration and humidity level be appropriate here?</p>
<pre class="r"><code>ggplot(antibiot, aes(x = Concentration, y = Surface, color = Humidité)) +
    geom_point(position = position_dodge(width = 0.3)) + 
    scale_color_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="6E-Two_way_ANOVA_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>There is a clear <em>interaction</em> between the two factors. Notably, humid conditions are associated with a larger bacterial surface area for low and moderate antibiotic concentrations, but dry conditions have a larger bacterial surface when concentration is high.</p>
</div>
<div id="interaction-estimation" class="section level2">
<h2>Interaction estimation</h2>
<p>To represent the interaction between the two factors, a term <span class="math inline">\(\gamma_ij\)</span> is added to the ANOVA model.</p>
<p><span class="math display">\[ y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk} \]</span></p>
<p>The interaction effect <span class="math inline">\(\gamma_ij\)</span> is the difference between the mean effect of the treatments <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> applied together, and the sum of their mean effects considered separately. This effect is estimated from the means as follows.</p>
<p><span class="math display">\[ \hat{\gamma_{ij}} = (\bar{y_{ij}} - \bar{y}) - (\bar{y_i} - \bar{y}) - (\bar{y_j} - \bar{y}) \]</span> By simplifying: <span class="math display">\[ \hat{\gamma_{ij}} = \bar{y_{ij}} - \bar{y_i} - \bar{y_j} + \bar{y} \]</span></p>
<p>In this equation, <span class="math inline">\(\bar{y_{ij}}\)</span> is the mean of the observations that are part of both group <span class="math inline">\(i\)</span> of factor A and group <span class="math inline">\(j\)</span> of factor B. The residual for an observation <span class="math inline">\(y_{ijk}\)</span> equals <span class="math inline">\(y_{ijk} - \bar{y_{ij}}\)</span>.</p>
<p>By including the interaction, the two-factor ANOVA table takes the following form:</p>
<table style="width:100%;">
<colgroup>
<col width="5%" />
<col width="31%" />
<col width="31%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Sum of squares (SS)</th>
<th>Degrees of freedom (df)</th>
<th>Mean square (MS)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Factor A</td>
<td><span class="math inline">\(SSA = \sum_{i = 1}^l mn (\bar{y_i} - \bar{y})^2\)</span></td>
<td><span class="math inline">\(l - 1\)</span></td>
<td><span class="math inline">\(MSA = \frac{SSA}{l - 1}\)</span></td>
</tr>
<tr class="even">
<td>Factor B</td>
<td><span class="math inline">\(SSB = \sum_{j = 1}^m ln (\bar{y_j} - \bar{y})^2\)</span></td>
<td><span class="math inline">\(m - 1\)</span></td>
<td><span class="math inline">\(MSB = \frac{SSB}{m - 1}\)</span></td>
</tr>
<tr class="odd">
<td>Interaction AB</td>
<td><span class="math inline">\(SSI = \sum_{i = 1}^l \sum_{j = 1}^m n (\bar{y_{ij}} - \bar{y_i} - \bar{y_j} + \bar{y})^2\)</span></td>
<td><span class="math inline">\((l - 1)(m - 1)\)</span></td>
<td><span class="math inline">\(MSI = \frac{SSI}{(l-1)(m-1)}\)</span></td>
</tr>
<tr class="even">
<td>Residual</td>
<td><span class="math inline">\(SSE = \sum_{i = 1}^l \sum_{j = 1}^m \sum_{k = i}^n (y_{ijk} - \bar{y_{ij}})^2\)</span></td>
<td><span class="math inline">\(lm(n - 1)\)</span></td>
<td><span class="math inline">\(MSE = \frac{SSE}{lm(n-1)}\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(SST = \sum_{i = 1}^l \sum_{j = 1}^m \sum_{k = i}^n (y_{ijk} - \bar{y})^2\)</span></td>
<td><span class="math inline">\(lmn - 1\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<p>The number of degrees of freedom for the interaction is the product of the number of degrees of freedom of each factor. The interaction has its own <span class="math inline">\(F\)</span> statistic equal to the <span class="math inline">\(MSI / MSE\)</span> ratio. The null hypothesis is the absence of interaction, that is, the effects of the two factors are additive and all <span class="math inline">\(\gamma_ {ij}\)</span> are 0.</p>
</div>
<div id="application" class="section level2">
<h2>Application</h2>
<p>In R, to specify an interaction in the model formula, we place a multiplication symbol <code>*</code> between the two variables instead of <code>+</code>.</p>
<pre class="r"><code>aov_antibio &lt;- aov(Surface ~ Concentration * Humidité, antibiot)</code></pre>
<p>Here are the diagnostic plots and the summary table for this model:</p>
<p><img src="6E-Two_way_ANOVA_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>summary(aov_antibio)</code></pre>
<pre><code>##                        Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Concentration           2  15.93   7.965    71.5 7.76e-11 ***
## Humidité                1  20.23  20.228   181.6 1.09e-12 ***
## Concentration:Humidité  2  36.40  18.199   163.4 1.05e-14 ***
## Residuals              24   2.67   0.111                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The interaction between 3 levels of concentration and 2 levels of humidity defines 6 groups, so there are 15 possible pairwise comparison for this interaction, as can be seen from the result of <code>TukeyHSD</code>.</p>
<pre class="r"><code>TukeyHSD(aov_antibio)</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = Surface ~ Concentration * Humidité, data = antibiot)
## 
## $Concentration
##                      diff        lwr         upr     p adj
## modérée-faible -0.3939894 -0.7667378 -0.02124113 0.0368807
## élevée-faible  -1.7046765 -2.0774249 -1.33192823 0.0000000
## élevée-modérée -1.3106871 -1.6834354 -0.93793878 0.0000000
## 
## $Humidité
##                 diff       lwr       upr p adj
## sec-humide -1.642264 -1.893794 -1.390734     0
## 
## $`Concentration:Humidité`
##                                     diff          lwr        upr     p adj
## modérée:humide-faible:humide -0.82921989 -1.481887432 -0.1765523 0.0073592
## élevée:humide-faible:humide  -4.22827694 -4.880944489 -3.5756094 0.0000000
## faible:sec-faible:humide     -3.61481768 -4.267485222 -2.9621501 0.0000000
## modérée:sec-faible:humide    -3.57357668 -4.226244229 -2.9209091 0.0000000
## élevée:sec-faible:humide     -2.79589383 -3.448561371 -2.1432263 0.0000000
## élevée:humide-modérée:humide -3.39905706 -4.051724600 -2.7463895 0.0000000
## faible:sec-modérée:humide    -2.78559779 -3.438265333 -2.1329302 0.0000000
## modérée:sec-modérée:humide   -2.74435680 -3.397024340 -2.0916893 0.0000000
## élevée:sec-modérée:humide    -1.96667394 -2.619341482 -1.3140064 0.0000000
## faible:sec-élevée:humide      0.61345927 -0.039208277  1.2661268 0.0740073
## modérée:sec-élevée:humide     0.65470026  0.002032716  1.3073678 0.0489732
## élevée:sec-élevée:humide      1.43238312  0.779715574  2.0850507 0.0000070
## modérée:sec-faible:sec        0.04124099 -0.611426550  0.6939085 0.9999549
## élevée:sec-faible:sec         0.81892385  0.166256308  1.4715914 0.0082690
## élevée:sec-modérée:sec        0.77768286  0.125015314  1.4303504 0.0131278</code></pre>
<p>For the <code>growth</code> dataset on animal growth, the interaction is not significant:</p>
<pre class="r"><code>aov_growth_inter &lt;- aov(gain ~ diet * supplement, growth)
summary(aov_growth_inter)</code></pre>
<pre><code>##                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## diet             2 287.17  143.59   83.52 3.00e-14 ***
## supplement       3  91.88   30.63   17.82 2.95e-07 ***
## diet:supplement  6   3.41    0.57    0.33    0.917    
## Residuals       36  61.89    1.72                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Should we always estimate the interaction in a two-way ANOVA model? The main disadvantage of the model with interaction is that more parameters need to be estimated. In particular, since we estimate the mean of each combination of the two factors, we need a sufficient number of observations with the same value of both factors (replicates). The datasets shown as examples have 4 or 5 replicates per combination of the two factors, which is a bit low as a sample size in this case. Note that it is impossible to estimate interactions if there is only one observation for each combination of factors.</p>
</div>
</div>
<div id="block-design" class="section level1">
<h1>Block design</h1>
<p>In the class on experimental design, we discussed the use of randomized complete block experiments. In this type of design, the observation units are divided into blocks where the conditions in each block are more homogeneous, then the treatments are assigned randomly in each block. This is the equivalent of a paired sample experiment for more than two treatments.</p>
<p>The <a href="../donnees/pigs.csv">pigs.csv</a> file contains the results of an experiment measuring the weight of guinea pigs according to four diets. The twenty individuals are divided into five blocks, which could represent guinea pigs from the same litter, for example.</p>
<pre class="r"><code>pigs &lt;- read.csv(&quot;../donnees/pigs.csv&quot;)
str(pigs)</code></pre>
<pre><code>## &#39;data.frame&#39;:    20 obs. of  3 variables:
##  $ Block : int  1 1 1 1 2 2 2 2 3 3 ...
##  $ Diet  : int  1 2 3 4 1 2 3 4 1 2 ...
##  $ Weight: num  1.5 2.7 2.1 1.3 1.4 2.9 2.2 1 1.4 2.1 ...</code></pre>
<p>In this dataset, <em>Block</em> and <em>Diet</em> are categorical variables even though they are represented by numbers. These variables must therefore be converted into factors in R with the <code>as.factor</code> function.</p>
<pre class="r"><code>pigs &lt;- mutate(pigs, Block = as.factor(Block), Diet = as.factor(Diet))</code></pre>
<p>The mathematical model that we use here is the same as for the two-factor ANOVA, without interaction. We could not estimate the interaction here, since there is only one observation by combination of block and diet. We must therefore assume that the effects are additive: each block has a different mean weight, but the effect of each diet is the same for all blocks.</p>
<pre class="r"><code>aov_pigs &lt;- aov(Weight ~ Diet + Block, data = pigs)</code></pre>
<p><img src="6E-Two_way_ANOVA_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>summary(aov_pigs)</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Diet         3  8.154  2.7178  41.866 1.24e-06 ***
## Block        4  0.393  0.0982   1.513     0.26    
## Residuals   12  0.779  0.0649                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Although the model is identical to that of the two-way ANOVA, the interpretation is different. In this case, we are not interested in estimating the effect of the blocks. The block structure aims to reduce the variability inside the blocks, thus decreasing the residual portion of the variance and facilitating the detection of the effect of the treatments (diets).</p>
<div id="fixed-or-random-effects" class="section level2">
<h2>Fixed or random effects</h2>
<p>In the model above, the effects of the blocks are <strong>fixed effects</strong>, that is, they are estimated separately for each block. (The treatment effects are also fixed.) In some grouped data designs, it is better to consider the mean of each block as being a random variable itself, and to estimate the parameters of the distribution of block means rather than the block means taken individually. We will include this type of <strong>random effects</strong> in the mixed models part of the course later in the semester.</p>
</div>
</div>
<div id="linear-regression-and-contrasts" class="section level1">
<h1>Linear regression and contrasts</h1>
<p>The ANOVA model is an example of linear regression, so the same model can be analyzed with the <code>lm</code> function in R. For example, consider the effect of the supplement (a single factor) on weight gain in the <code>growth</code> data set.</p>
<pre class="r"><code>lm_growth_supp &lt;- lm(gain ~ supplement, data = growth)</code></pre>
<p>The summary results for <code>lm</code> put more emphasis on the estimation of effects (see the <em>Coefficients</em> section).</p>
<pre class="r"><code>summary(lm_growth_supp)</code></pre>
<pre><code>## 
## Call:
## lm(formula = gain ~ supplement, data = growth)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1309 -2.2142 -0.2459  1.7644  5.9339 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          23.0953     0.8170  28.267  &lt; 2e-16 ***
## supplementcontrol    -2.6967     1.1555  -2.334  0.02423 *  
## supplementsupergain  -3.3815     1.1555  -2.926  0.00541 ** 
## supplementsupersupp  -0.7274     1.1555  -0.629  0.53228    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.83 on 44 degrees of freedom
## Multiple R-squared:  0.2068, Adjusted R-squared:  0.1527 
## F-statistic: 3.823 on 3 and 44 DF,  p-value: 0.01614</code></pre>
<p>Before discussing the coefficients, let’s look at the values at the bottom of the table. The residual standard error is the square root of the <em>MSE</em> (the mean of the squared residuals) of the ANOVA table. The value <code>Mutipled R-squared</code> corresponds to the determination coefficient <span class="math inline">\(R^2\)</span> defined earlier. The <code>Adjusted R-squared</code> value has a slightly different definition; it is based on the ratio between the <em>MSE</em> and the total variance, rather than the ratio of sums of squares <em>SSE</em> and <em>SST</em>. Finally, the <span class="math inline">\(F\)</span> test in the last line (with its <span class="math inline">\(p\)</span>-value) corresponds to the test in one-way ANOVA. We can retrieve the ANOVA table by applying the <code>anova</code> function to the <code>lm</code> result.</p>
<pre class="r"><code>anova(lm_growth_supp)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: gain
##            Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## supplement  3  91.88 30.6270  3.8233 0.01614 *
## Residuals  44 352.47  8.0106                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Fitting the linear model with <code>lm</code> produces four coefficients: <code>(Intercept)</code>, <code>supplementcontrol</code>, <code>supplementsupergain</code> and <code>supplementsupersupp</code>. As we briefly mentioned in the last class, the <code>(Intercept)</code> value is the mean of the response for the first level of the <code>supplement</code> factor (<code>agrimore</code>), and the other coefficients are the mean differences between each of the other three levels and <code>agrimore</code>. In the next sections, we will see why the factors are coded this way and how to modify this encoding.</p>
<div id="encoding-a-categorical-variable" class="section level2">
<h2>Encoding a categorical variable</h2>
<p>The following equation describes a linear model for the relationship between a numerical predictor <span class="math inline">\(x\)</span> and a numerical response <span class="math inline">\(y\)</span>. The value of <span class="math inline">\(y\)</span> depends on a constant term (<span class="math inline">\(\beta_0\)</span>), a term proportional to <span class="math inline">\(x\)</span> (<span class="math inline">\(\beta_1 x\)</span>) and a random residual for each observation (<span class="math inline">\(\epsilon\)</span>) .</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x + \epsilon \]</span></p>
<p>Here, <span class="math inline">\(\beta_0\)</span> is the intercept of the graph of <span class="math inline">\(y\)</span> vs. <span class="math inline">\(x\)</span>, i.e. the mean of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span>.</p>
<p>Consider an experiment with a control group and two treatments (<span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span>). To represent these data in a regression model, we create two variables:</p>
<ul>
<li><span class="math inline">\(T_1\)</span> = 1 for the observations that received treatment 1, 0 for the others.</li>
<li><span class="math inline">\(T_2\)</span> = 1 for the observations that received treatment 2, 0 for the others.</li>
</ul>
<p>We obtain the model: <span class="math inline">\(y = \beta_0 + \beta_1 T_1 + \beta_2 T_2 + \epsilon\)</span></p>
<p>By substituting the values of <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span>, we can determine the mean of <span class="math inline">\(y\)</span> for each group as a function of the <span class="math inline">\(\beta\)</span> coefficients:</p>
<ul>
<li>Control group (<span class="math inline">\(T_1 = 0, T_2 = 0\)</span>): <span class="math inline">\(\mu_{tém} = \beta_0\)</span></li>
<li>Treatment 1 (<span class="math inline">\(T_1 = 1, T_2 = 0\)</span>): <span class="math inline">\(\mu_{tr1} = \beta_0 + \beta_1\)</span></li>
<li>Treatment 2 (<span class="math inline">\(T_1 = 0, T_2 = 1\)</span>): <span class="math inline">\(\mu_{tr2} = \beta_0 + \beta_2\)</span></li>
</ul>
<p>The intercept is the mean of the control group while the other two coefficients are the differences between the mean of each treatment and that of the control group. This type of coding of a categorical variable makes it easy to compare each treatment with a reference. This is the type of encoding used by default in R, as we have seen in the results above.</p>
</div>
<div id="contrasts" class="section level2">
<h2>Contrasts</h2>
<p>In statistics, a <em>contrast</em> is a numeric variable defined from a categorical variable (or factor) that represents a comparison between categories.</p>
<p>For a factor with <span class="math inline">\(k\)</span> categories, we can define <span class="math inline">\(k - 1\)</span> independent contrasts. In the previous example, the contrasts <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span> were used to compare treatment 1 to the control group and treatment 2 to the control group. Knowing these two differences, we also know the difference between treatments 1 and 2, so it would be redundant to add a third contrast.</p>
<p>In R, the <code>contrasts</code> function shows the matrix of contrasts associated with a factor.</p>
<pre class="r"><code>contrasts(growth$supplement)</code></pre>
<pre><code>##           control supergain supersupp
## agrimore        0         0         0
## control         1         0         0
## supergain       0         1         0
## supersupp       0         0         1</code></pre>
<p>The columns of this matrix correspond to contrasts (<code>control</code>, <code>supergain</code> and <code>supersupp</code>) which take a value of 1 for one of the treatments and 0 for the others. The first treatment, <code>agrimore</code>, is associated with a value of 0 for each of the contrasts. In this case, it would be better to use the control group as a reference. We can change the reference level with the <code>relevel</code> function.</p>
<pre class="r"><code>growth$supplement &lt;- relevel(growth$supplement, ref = &quot;control&quot;)
contrasts(growth$supplement)</code></pre>
<pre><code>##           agrimore supergain supersupp
## control          0         0         0
## agrimore         1         0         0
## supergain        0         1         0
## supersupp        0         0         1</code></pre>
<p>By re-fitting the linear model with these new contrasts, we obtain coefficients showing the difference between each supplement and the control.</p>
<pre class="r"><code>lm_growth_supp &lt;- lm(gain ~ supplement, data = growth)
summary(lm_growth_supp)</code></pre>
<pre><code>## 
## Call:
## lm(formula = gain ~ supplement, data = growth)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1309 -2.2142 -0.2459  1.7644  5.9339 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          20.3986     0.8170  24.967   &lt;2e-16 ***
## supplementagrimore    2.6967     1.1555   2.334   0.0242 *  
## supplementsupergain  -0.6848     1.1555  -0.593   0.5565    
## supplementsupersupp   1.9693     1.1555   1.704   0.0954 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.83 on 44 degrees of freedom
## Multiple R-squared:  0.2068, Adjusted R-squared:  0.1527 
## F-statistic: 3.823 on 3 and 44 DF,  p-value: 0.01614</code></pre>
<p>Note that the change in contrast affects only the coefficient estimates. The <span class="math inline">\(R^2\)</span> and the <span class="math inline">\(F\)</span>-test are the same.</p>
<p>For each coefficient, the results summary includes a <span class="math inline">\(t\)</span>-test that indicates whether each coefficient is significantly different from 0. Here, only one of the three supplements (agrimore) has a significant effect if we chose a threshold of 0.05. However, these results are based on independent <span class="math inline">\(t\)</span>-tests that do not account for multiple comparisons. They are therefore not as reliable as Tukey’s range test seen in the previous class.</p>
</div>
<div id="interpretation-of-coefficients-for-two-factors" class="section level2">
<h2>Interpretation of coefficients for two factors</h2>
<p>Let’s now add the <code>diet</code> variable to our regression.</p>
<pre class="r"><code>lm_growth &lt;- lm(gain ~ diet + supplement, data = growth)
summary(lm_growth)</code></pre>
<pre><code>## 
## Call:
## lm(formula = gain ~ diet + supplement, data = growth)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.30792 -0.85929 -0.07713  0.92052  2.90615 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          23.4263     0.4408  53.141  &lt; 2e-16 ***
## dietoats             -3.0928     0.4408  -7.016 1.38e-08 ***
## dietwheat            -5.9903     0.4408 -13.589  &lt; 2e-16 ***
## supplementagrimore    2.6967     0.5090   5.298 4.03e-06 ***
## supplementsupergain  -0.6848     0.5090  -1.345 0.185772    
## supplementsupersupp   1.9693     0.5090   3.869 0.000375 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.247 on 42 degrees of freedom
## Multiple R-squared:  0.8531, Adjusted R-squared:  0.8356 
## F-statistic: 48.76 on 5 and 42 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Before discussing the contrasts for the <code>diet</code> factor, notice the differences between this table and that of the previous regression. The coefficients associated with the different supplements are the same, but the <span class="math inline">\(p\)</span>-values are much lower. Why is the effect more significant in this case?</p>
<p>By adding the <code>diet</code> variable, we are able to explain a larger part of the response (the <span class="math inline">\(R^2\)</span> has gone from 0.2 to around 0.8) and the residual variation is smaller. This is why the standard error of the coefficients is lower and their value is therefore more significantly different from zero.</p>
<p>Here are the contrasts for the <code>diet</code> factor:</p>
<pre class="r"><code>contrasts(growth$diet)</code></pre>
<pre><code>##        oats wheat
## barley    0     0
## oats      1     0
## wheat     0     1</code></pre>
<p>While the coefficients of the different supplements are the same, the value of the intercept has changed between the two tables (23.4 instead of 20.4). The intercept is the mean when all the contrasts are 0. In the one-factor model, this coefficient represented the mean of the control group, regardless of the diet. Now, it represents the mean of the control group observations (reference value of <code>supplement</code>) that have the barley diet (reference value for <code>diet</code>).</p>
</div>
<div id="changing-the-type-of-contrasts" class="section level2">
<h2>Changing the type of contrasts</h2>
<p>The type of contrast used by default in R compares each category to a reference category. It is often called a treatment coding (<code>contr.treatment</code> in R) because it is useful for comparing treatments to a control group. Since we do not have a reference group for the <code>diet</code> factor, we could use another type of contrast. Here are effect coding (<code>contr.sum</code>) contrasts for the same variable.</p>
<pre class="r"><code>contrasts(growth$diet) &lt;- &quot;contr.sum&quot;
contrasts(growth$diet)</code></pre>
<pre><code>##        [,1] [,2]
## barley    1    0
## oats      0    1
## wheat    -1   -1</code></pre>
<p>To facilitate the interpretation of the results, we assign a name to the contrast variables.</p>
<pre class="r"><code>colnames(contrasts(growth$diet)) &lt;- c(&quot;barley&quot;, &quot;oats&quot;)</code></pre>
<p>In this type of encoding, each contrast takes the value of 1 for one of the categories, except for the last category which takes a value of -1 for all the contrasts. An important property of these contrasts is that the sum of each column is zero, which means that the mean of each contrast across all categories is zero.</p>
<blockquote>
<p>In the strict statistical sense, a contrast variable must have a sum of zero over all categories. The treatment coding used by default in R therefore does not form true contrasts.</p>
</blockquote>
<p>Let’s consider the regression model: <span class="math inline">\(y = \beta_0 + \beta_1 T_1 + \beta_2 T_2\)</span> with the effect coding defined above.</p>
<ul>
<li>Category 1 (<span class="math inline">\(T_1 = 1, T_2 = 0\)</span>): <span class="math inline">\(\mu_1 = \beta_0 + \beta_1\)</span></li>
<li>Category 2 (<span class="math inline">\(T_1 = 0, T_2 = 1\)</span>): <span class="math inline">\(\mu_2 = \beta_0 + \beta_2\)</span></li>
<li>Category 3 (<span class="math inline">\(T_1 = -1, T_2 = -1\)</span>): <span class="math inline">\(\mu_3 = \beta_0 - \beta_1 - \beta_2\)</span></li>
<li>Grand mean: <span class="math inline">\(\mu = (\mu_1 + \mu_2 + \mu_3)/3 = \beta_0\)</span></li>
</ul>
<p>The intercept thus corresponds to the grand mean while the coefficients are the difference between the mean of each category and the grand mean. The effect of the last category can be determined by taking the opposite of the sum of the other effects, so <span class="math inline">\(-(\beta_1 + \beta_2)\)</span> here.</p>
<p>Here is the result of the linear regression with these new contrasts.</p>
<pre class="r"><code>lm_growth &lt;- lm(gain ~ diet + supplement, data = growth)
summary(lm_growth)</code></pre>
<pre><code>## 
## Call:
## lm(formula = gain ~ diet + supplement, data = growth)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.30792 -0.85929 -0.07713  0.92052  2.90615 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         20.39861    0.35994  56.673  &lt; 2e-16 ***
## dietbarley           3.02770    0.25451  11.896 4.93e-15 ***
## dietoats            -0.06511    0.25451  -0.256 0.799333    
## supplementagrimore   2.69670    0.50903   5.298 4.03e-06 ***
## supplementsupergain -0.68476    0.50903  -1.345 0.185772    
## supplementsupersupp  1.96935    0.50903   3.869 0.000375 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.247 on 42 degrees of freedom
## Multiple R-squared:  0.8531, Adjusted R-squared:  0.8356 
## F-statistic: 48.76 on 5 and 42 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Question</strong>: What is the meaning of the coefficients in this table? What is the effect of the third diet (<code>wheat</code>)?</p>
<p><strong>Answer</strong>:</p>
<ul>
<li>The <code>dietbarley</code> and<code>dietoats</code> coefficients give the effect of that diet in relation to the mean across all diets.</li>
<li>The three supplement coefficients give the effect of that supplement compared to the control group.</li>
<li>The intercept is the mean weight gain of the control group (mean over all diets).</li>
<li>The effect of the <code>wheat</code> diet compared with the mean is about -2.96 (-(3.028 - 0.065)).</li>
</ul>
<p>Note that it is possible to use different encodings for different factors in the same regression. Treatment coding (the default) is useful for comparing categories to a reference category, while effect coding (<code>contr.sum</code>) is useful for comparing the categories to the mean response.</p>
<p>If both factors had effect coding, the intercept would be equal to the grand mean (across all diets and supplements).</p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>Two-way ANOVA makes it possible to evaluate the effect of two categorical variables (e.g. two types of treatment) and to determine if these effects are additive or if there is an interaction.</p></li>
<li><p>A randomized complete block experiment is analyzed as a two-way ANOVA without interaction. The grouping of observations in blocks makes it possible to control part of the variation to better estimate the effect of the treatments.</p></li>
<li><p>The ANOVA model is an example of linear regression. Categorical variables are represented in a regression model using contrasts.</p></li>
<li><p>We have seen two possible types of contrasts in R: the treatment coding (default option) compares the effect of each category to a reference category, while the effect coding compares the effect of each category to the mean of all categories.</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
