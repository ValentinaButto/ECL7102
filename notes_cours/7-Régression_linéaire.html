<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Régression linéaire</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Régression linéaire</h1>
<h4 class="date"><em><br/>15 octobre 2018</em></h4>

</div>


<div id="objectifs" class="section level1">
<h1>Objectifs</h1>
<ul>
<li><p>Estimer et interpréter les paramètres d’une régression linéaire simple et ceux d’une régression linéaire multiple, avec ou sans interactions.</p></li>
<li><p>Vérifier les suppositions d’un modèle de régression à partir des graphiques de diagnostic.</p></li>
<li><p>Différencier l’intervalle de confiance d’une droite de régression et l’intervalle de prédiction de nouvelles observations.</p></li>
<li><p>Identifier les problèmes dûs à la collinéarité de plusieurs prédicteurs.</p></li>
</ul>
</div>
<div id="regression-vue-densemble" class="section level1">
<h1>Régression: Vue d’ensemble</h1>
<p>Les six prochains cours porteront sur les modèles de régression. Ces modèles représentent la relation mathématique entre une variable <em>réponse</em> et une ou plusieurs variables nommées <em>prédicteurs</em>.</p>
<p>L’analyse de régression est notamment utile dans les cas suivants:</p>
<ul>
<li><p>Analyser les résultats d’une expérience lorsqu’une ou plusieurs variables de traitement sont numériques (ex.: température, dose).</p></li>
<li><p>Séparer l’effet de traitements discrets (variables catégorielles) de celui d’autres conditions expérimentales représentées par des variables numériques. Dans ce contexte, on parle souvent d’<strong>analyse de la covariance</strong>.</p></li>
<li><p>Déterminer l’importance des associations entre des variables mesurées dans la nature (sans supposer de lien de causalité).</p></li>
<li><p>Utiliser les associations entre prédicteurs et réponse afin de prédire la valeur de cette dernière pour de nouvelles observations.</p></li>
</ul>
<p>Le modèle mathématique demeure le même pour toutes ces situations, elles diffèrent donc dans l’interprétation et l’utilisation des résultats.</p>
</div>
<div id="regression-lineaire-simple" class="section level1">
<h1>Régression linéaire simple</h1>
<p>La régression linéaire simple désigne le cas où il y a un seul prédicteur numérique (<span class="math inline">\(x\)</span>).</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x + \epsilon \]</span></p>
<p>Rappelons-nous que <span class="math inline">\(\beta_0\)</span> et <span class="math inline">\(\beta_1\)</span> sont les <em>coefficients</em> de la régression qui seront estimés à partir des données, tandis que <span class="math inline">\(\epsilon\)</span> est le <em>résidu</em> aléatoire qui suit une distribution normale centrée sur zéro: <span class="math inline">\(N(0, \sigma)\)</span>.</p>
<p>Le modèle signifie que pour une valeur de <span class="math inline">\(x\)</span> donnée, la réponse <span class="math inline">\(y\)</span> suit une distribution normale de moyenne <span class="math inline">\(\mu = \beta_0 + \beta_1 x\)</span> et d’écart-type <span class="math inline">\(\sigma\)</span>. L’<em>ordonnée à l’origine</em> <span class="math inline">\(\beta_0\)</span> est la valeur moyenne de <span class="math inline">\(y\)</span> lorsque <span class="math inline">\(x = 0\)</span>, tandis que la <em>pente</em> <span class="math inline">\(\beta_1\)</span> est la différence moyenne de <span class="math inline">\(y\)</span> entre deux observations qui diffèrent par 1 unité de <span class="math inline">\(x\)</span>.</p>
<div id="methode-des-moindres-carres" class="section level2">
<h2>Méthode des moindres carrés</h2>
<div id="exemple" class="section level3">
<h3>Exemple</h3>
<p>Le tableau de données <a href="../donnees/plant_growth_rate.csv">plant_growth_rate.csv</a> (tiré du livre de Beckerman, Childs et Petchey, <em>Getting Started with R, An Introduction for Biologists</em>) contient des mesures de croissance d’une plante en fonction de l’humidité du sol.</p>
<pre class="r"><code>pgr &lt;- read.csv(&quot;../donnees/plant_growth_rate.csv&quot;)
str(pgr)</code></pre>
<pre><code>## &#39;data.frame&#39;:    50 obs. of  2 variables:
##  $ soil.moisture.content: num  0.47 0.541 1.698 0.826 0.857 ...
##  $ plant.growth.rate    : num  21.3 27 39 30.2 37.1 ...</code></pre>
<p>Graphiquement, l’estimation des coefficients de la régression linéaire consiste à trouver la droite qui passe le plus “près” des points du graphique de <span class="math inline">\(y\)</span> vs. <span class="math inline">\(x\)</span>.</p>
<pre class="r"><code>ggplot(pgr, aes(x = soil.moisture.content, y = plant.growth.rate)) +
    geom_point() +
    geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Plus précisément, il est possible de démontrer que les meilleurs estimateurs sans biais des paramètres de la régression linéaire sont ceux qui minimisent la somme du carré des résidus. Il s’agit de la <strong>méthode des moindres carrés</strong>.</p>
<p>Pour une série de <span class="math inline">\(n\)</span> observations de <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span>, cette somme du carré des résidus correspond à:</p>
<p><span class="math display">\[ \sum_{k = 1}^n \epsilon_k^2 = \sum_{k = 1}^n (y_k - (\beta_0 + \beta_1 x_k))^2 \]</span></p>
<p>Les estimés <span class="math inline">\(\hat{\beta_0}\)</span> et <span class="math inline">\(\hat{\beta_1}\)</span> qui minimisent cette somme sont obtenus à partir du calcul différentiel. (Il s’agit des valeurs pour lesquelles les dérivées partielles de la somme en fonction de chaque coefficient sont égales à zéro.)</p>
<p>L’estimateur de la pente <span class="math inline">\(\beta_1\)</span> est égal à la covariance de <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span> divisée par la variance de <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\hat{\beta_1} = \frac{\sum_{k = 1}^n (x_k - \bar{x})(y_k - \bar{y})}{\sum_{k = 1}^n (x_k - \bar{x})^2}\]</span></p>
<p>Ici, <span class="math inline">\(\bar{x}\)</span> et <span class="math inline">\(\bar{y}\)</span> représentent les moyennes de <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span>, respectivement, pour l’ensemble des observations.</p>
<p>L’estimateur de l’ordonnée à l’origine <span class="math inline">\(\beta_0\)</span> est égal à:</p>
<p><span class="math display">\[\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}\]</span></p>
<p>Cette équation montre aussi que la droite de régression estimée passe par le point <span class="math inline">\((\bar{x}, \bar{y})\)</span>, le “centre de gravité” du nuage de points de <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span>.</p>
</div>
</div>
<div id="interpretations-des-resultats" class="section level2">
<h2>Interprétations des résultats</h2>
<p>En effectuant la régression sur l’exemple ci-dessus, nous obtenons:</p>
<pre class="r"><code>mod &lt;- lm(plant.growth.rate ~ soil.moisture.content, data = pgr)
summary(mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = plant.growth.rate ~ soil.moisture.content, data = pgr)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.9089 -3.0747  0.2261  2.6567  8.9406 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             19.348      1.283   15.08   &lt;2e-16 ***
## soil.moisture.content   12.750      1.021   12.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.019 on 48 degrees of freedom
## Multiple R-squared:  0.7648, Adjusted R-squared:  0.7599 
## F-statistic: 156.1 on 1 and 48 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Le tableau des coefficients inclut l’ordonnée à l’origine <code>(Intercept)</code> et l’effet de chaque prédicteur. Si les suppositions du modèle sont respectées (voir section ci-dessous), chaque estimé <span class="math inline">\(\hat{\beta}\)</span> suit une distribution normale centrée sur la valeur du paramètre <span class="math inline">\(\beta\)</span>, avec une l’erreur-type indiquée dans le tableau. Ceci permet d’effectuer un test <span class="math inline">\(t\)</span> pour l’hypothèse nulle <span class="math inline">\(\beta = 0\)</span>, avec une valeur <span class="math inline">\(p\)</span> indiquée dans la dernière colonne.</p>
<p>Sous le tableau, <code>Residual standard error</code> correspond à l’écart-type des résidus du modèle, calculé avec 48 degrés de liberté (50 observations - 2 paramètres estimés).</p>
<pre class="r"><code>sqrt(sum(mod$residuals^2) / 48)</code></pre>
<pre><code>## [1] 4.019094</code></pre>
<p>Comme pour l’ANOVA, le coefficient de détermination (<span class="math inline">\(R^2\)</span>) représente la fraction de la variance expliquée par le modèle.</p>
<p><span class="math display">\[ R^2 = 1 - \frac{\sum_{k=1}^n (y_k - \hat{y_k})^2}{\sum_{k=1}^n (y_k - \bar{y})^2} \]</span></p>
<p>Le numérateur est la somme des résidus au carré, car <span class="math inline">\(\hat{y_k} = \hat{\beta_0} + \hat{\beta_1} x_k\)</span> est la valeur attendue (moyenne) pour <span class="math inline">\(y_k\)</span> selon le modèle.</p>
<pre class="r"><code>r2 &lt;- 1 - sum(mod$residuals^2) / sum((pgr$plant.growth.rate - mean(pgr$plant.growth.rate))^2)
r2</code></pre>
<pre><code>## [1] 0.764796</code></pre>
<p>Les valeurs attendues <span class="math inline">\(\hat{y_k}\)</span> à chaque point sont enregistrées dans l’élément <code>fitted.values</code> du résultat de <code>lm</code> (ex.: <code>mod$fitted.values</code>).</p>
<p>Pour une régression linéaire simple, la racine carrée de <span class="math inline">\(R^2\)</span> est égale à la corrélation entre <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span>.</p>
<pre class="r"><code>cor_pgr &lt;- cor(pgr$soil.moisture.content, pgr$plant.growth.rate) 
all.equal(cor_pgr, sqrt(r2))</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p><strong>Note</strong>: Lorsqu’on compare des nombres réels dans R, il est préférable d’utiliser <code>all.equal</code> que <code>==</code>. En raison de la précision limitée des calculs, les deux valeurs sont en fait <em>presque</em> égales, donc l’égalité exacte <code>==</code> retournerait <code>FALSE</code>.</p>
<pre class="r"><code>cor_pgr - sqrt(r2)</code></pre>
<pre><code>## [1] 1.110223e-16</code></pre>
<p>La dernière ligne du sommaire est un test <span class="math inline">\(F\)</span> semblable à l’ANOVA. Lorsque nous n’avons qu’un seul prédicteur, ce test donne la même information que le test <span class="math inline">\(t\)</span> pour ce prédicteur: la probabilité d’obtenir un effet estimé aussi loin de 0 si l’effet réel du prédicteur est 0.</p>
</div>
<div id="intervalle-de-confiance-et-intervalle-de-prediction" class="section level2">
<h2>Intervalle de confiance et intervalle de prédiction</h2>
<p>Pour afficher la droite de régression sur un nuage de points entre <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span> avec son intervalle de confiance, nous pouvons utiliser la fonction <code>geom_smooth</code> du package <em>ggplot2</em>, avec la méthode <code>lm</code> (modèle linéaire).</p>
<pre class="r"><code>ggplot(pgr, aes(x = soil.moisture.content, y = plant.growth.rate)) +
    geom_point() + 
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>À chaque valeur de <span class="math inline">\(x\)</span>, la surface grise donne un intervalle de confiance pour la valeur moyenne de <span class="math inline">\(y\)</span> selon le modèle linéaire. Par défaut, il s’agit d’un intervalle à 95%, ce qui peut être modifié avec l’argument <code>level</code> de <code>geom_smooth</code>.</p>
<p>Notez que l’intervalle de confiance devient plus large aux extrémités du graphique. Rappelez-vous que la droite de régression doit passer par le point <span class="math inline">\((\bar{x}, \bar{y})\)</span>, donc l’incertitude sur la pente fait “pivoter” la droite légèrement autour de ce point, ce qui génère une incertitude plus grande aux extrémités.</p>
<p>Supposons qu’en plus d’estimer la tendance moyenne entre <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span>, nous voulons prédire la valeur de <span class="math inline">\(y\)</span> pour de nouvelles observations, en ne connaissant que la valeur de <span class="math inline">\(x\)</span>. Dans le code ci-dessous, nous créons un nouveau tableau de données <code>pgr_nouv</code> avec 101 valeurs de l’humidité du sol, puis nous appelons la fonction <code>predict</code> pour obtenir les prédiction de croissance selon le modèle <code>mod</code>, avec un intervalle de prédiction. Nous rattachons ensuite ces colonnes à <code>pgr_nouv</code> avec <code>cbind</code>.</p>
<pre class="r"><code>pgr_nouv &lt;- data.frame(soil.moisture.content = seq(0, 2, 0.02))
pgr_pred &lt;- predict(mod, pgr_nouv, interval = &quot;prediction&quot;)
pgr_nouv &lt;- cbind(pgr_nouv, pgr_pred)
str(pgr_nouv)</code></pre>
<pre><code>## &#39;data.frame&#39;:    101 obs. of  4 variables:
##  $ soil.moisture.content: num  0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 ...
##  $ fit                  : num  19.3 19.6 19.9 20.1 20.4 ...
##  $ lwr                  : num  10.9 11.1 11.4 11.7 11.9 ...
##  $ upr                  : num  27.8 28.1 28.3 28.6 28.8 ...</code></pre>
<p>La colonne <code>fit</code> contient les valeurs prédites (qui correspondent aux points sur la droite de régression) tandis que <code>lwr</code> et <code>upr</code> sont les limites inférieure et supérieure de l’intervalle de prédiction à 95%.</p>
<p>Superposons maintenant la droite de régression, l’intervalle de prédiction (avec <code>geom_ribbon</code>) et le nuage de points:</p>
<pre class="r"><code>ggplot(pgr_nouv, aes(x = soil.moisture.content)) +
    labs(x = &quot;Humidité du sol&quot;, y = &quot;Croissance&quot;) +
    geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3) +
    geom_line(aes(y = fit), color = &quot;blue&quot;) +
    geom_point(data = pgr, aes(y = plant.growth.rate))</code></pre>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Contrairement à l’intervalle de confiance qui représente l’incertitude sur la valeur moyenne de la réponse pour une certaine valeur du prédicteur, l’intervalle de prédiction représente l’incertitude sur la valeur de la réponse pour une observation individuelle. Ainsi, on s’attend à ce qu’environ 95% des points se retrouvent dans l’intervalle de prédiction, ce qui est le cas ici (48/50).</p>
<p><strong>Note</strong>: En général, il n’est pas prudent d’utiliser le résultat d’une régression pour prédire la réponse pour des valeurs des prédicteurs hors de la plage des valeurs avec lesquelles le modèle a été estimé (dans l’exemple, pour des valeurs d’humidité &gt; 2). Ces extrapolations sont moins fiables que les prédictions à l’intérieur de la plage des valeurs observées (interpolation). En particulier, une relation approximativement linéaire sur une échelle restreinte de valeurs de <span class="math inline">\(x\)</span> peut devenir fortement non-linéaire lorsqu’on change d’échelle.</p>
</div>
</div>
<div id="suppositions-du-modele-de-regression-lineaire" class="section level1">
<h1>Suppositions du modèle de régression linéaire</h1>
<p>Comme pour l’ANOVA, les résidus sont:</p>
<ul>
<li>indépendants et</li>
<li>normalement distribués</li>
<li>avec la même variance.</li>
</ul>
<p>En plus:</p>
<ul>
<li>la relation entre la réponse moyenne et les prédicteurs est linéaire, et</li>
<li>les prédicteurs sont mesurés sans erreur (ou cette erreur est négligeable par rapport aux autres erreurs du modèles).</li>
</ul>
<div id="linearite" class="section level2">
<h2>Linéarité</h2>
<p>Le critère de linéarité est moins contraignant qu’il n’y parait à prime abord. Les transformations de variables permettent de convertir une relation non-linéaire en relation linéaire. Par exemple, si <span class="math inline">\(y\)</span> est la fonction d’une puissance à déterminer de <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[ y = a x^b \]</span></p>
<p>alors en prenant le logarithme de chaque côté de l’équation, on obtient une relation linéaire:</p>
<p><span class="math display">\[ \log(y) = \log(a) + b \log(x) \]</span></p>
<p>En général, l’équation reliant <span class="math inline">\(x\)</span> et <span class="math inline">\(y\)</span> peut contenir des fonctions non-linéaires de <span class="math inline">\(x\)</span>, en autant qu’elle soit une fonction linéaire des <em>coefficients</em>. Par exemple, l’équation quadratrique:</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x + \beta_2 x^2 \]</span></p>
<p>constitue un exemple de modèle linéaire; il s’agit d’une régression linéaire multiple, puisqu’on a deux prédicteurs, <span class="math inline">\(x\)</span> et <span class="math inline">\(x^2\)</span>.</p>
</div>
<div id="independance-des-residus" class="section level2">
<h2>Indépendance des résidus</h2>
<p>L’indépendance des résidus signifie que la portion des observations <span class="math inline">\(y\)</span> non-expliquée par les prédicteurs <span class="math inline">\(x\)</span> est indépendante d’une observation à l’autre.</p>
<p>En écologie, la non-indépendance des résidus est souvent due à une proximité de certaines observations dans l’espace et dans le temps. Par exemple, si les observations s’étalent sur plusieurs journées, les observations plus rapprochées dans le temps pourraient être plus semblables. On peut inclure dans le modèle les facteurs pouvant causer cette dépendance temporelle (ex.: météo) pour obtenir les résidus les plus indépendants possibles.</p>
<p>La non-indépendance des résidus ne biaise pas les estimés des coefficients du modèle, donc ceux-ci demeurent valides, mais leur incertitude sera sous-estimée. (On pourrait dire qu’un échantillon de mesures non-indépendantes est équivalent à un échantillon indépendant de plus petite taille.) Ainsi, les intervalles de confiance et les tests d’hypothèse sur la significativité des coefficients ne seront pas valides.</p>
</div>
<div id="graphiques-de-diagnostic" class="section level2">
<h2>Graphiques de diagnostic</h2>
<p>Voici les quatre graphiques de diagnostic obtenus avec la fonction <code>plot</code> appliquée au résultat de <code>lm</code>.</p>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Les deux premiers graphiques correspondent à ceux déjà vus avec l’ANOVA. Pour le graphique des résidus vs. valeurs attendues (<em>residuals vs. fitted</em>), il faut faire particulièrement attention aux points suivants:</p>
<ul>
<li><p>Les résidus doivent être dispersés aléatoirement autour de zéro. La présence d’une tendance (linéaire ou non) indique des effets systématiques ignorés par le modèle. Dans ce cas-ci, il est possible que nous ayons une légère tendance non-linéaire (quadratique) dans les résidus.</p></li>
<li><p>La variance des résidus doit être approximativement constante (homoscédasticité). Un type courant d’hétéroscédasticité survient lorsque la variance augmente avec la moyenne. Dans ce cas, le graphique des résidus vs. valeurs attendues a une forme d’entonnoir (la dispersion des points augmente le long de l’axe des <span class="math inline">\(x\)</span>).</p></li>
</ul>
<p>Le diagramme quantile-quantile permet de détecter des déviations systématiques de la normalité des résidus.</p>
<p>Le troisième graphique montre l’échelle des résidus (en valeur absolue) en fonction des valeurs attendues de <span class="math inline">\(y\)</span>. Ce graphique devrait aussi montrer une tendance si la variance n’est pas constante.</p>
</div>
<div id="effet-de-levier" class="section level2">
<h2>Effet de levier</h2>
<p>Le dernier graphique montre l’<strong>effet de levier</strong> (<em>leverage</em>) des observations relativement à la valeur des résidus. Une observation avec un fort effet de levier a une plus grande influence sur les coefficients de la régression; cela se produit le plus souvent dans le cas d’observations isolées et loin de la moyenne des prédicteurs. Une observation éloignée de la moyenne (résidu positif ou négatif important) qui a aussi un fort effet de levier risque d’éloigner la droite de régression de la tendance générale indiquée par les autres données.</p>
<p>La distance de Cook (<em>Cook’s distance</em>) <span class="math inline">\(D\)</span> est une mesure combinant l’effet de levier et la magnitude du résidu. Les droites pointillées sur le quatrième graphique permettent d’identifier les points problématiques qui dépassent une certaine valeur de <span class="math inline">\(D\)</span>, généralement <span class="math inline">\(D &gt; 1\)</span>. Dans notre exemple précédent, aucun point n’a une grande influence, donc ces lignes pointillées se retrouvent en dehors de la partie visible du graphique.</p>
</div>
</div>
<div id="regression-lineaire-multiple" class="section level1">
<h1>Régression linéaire multiple</h1>
<p>Le modèle de régression linéaire multiple représente la relation entre une variable réponse et <span class="math inline">\(m\)</span> prédicteurs <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, …, <span class="math inline">\(x_m\)</span>.</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_m x_m + \epsilon = \beta_0 + \sum_{i = 1}^m \beta_i x_i + \epsilon \]</span></p>
<p>Comme dans le cas de la régression linéaire simple, les coefficients <span class="math inline">\(\beta\)</span> peuvent être calculés à partir de la méthode des moindres carrés. Dans ce modèle, chaque coefficient <span class="math inline">\(\beta_i\)</span> (sauf <span class="math inline">\(\beta_0\)</span>) est la dérivée partielle de <span class="math inline">\(y\)</span> par rapport à un prédicteur <span class="math inline">\(x_i\)</span>. En d’autres mots, ce coefficient représente la variation moyenne de <span class="math inline">\(y\)</span> si <span class="math inline">\(x_i\)</span> augmente d’une unité <em>et que tous les autres prédicteurs restent constants</em>.</p>
</div>
<div id="analyse-de-la-covariance" class="section level1">
<h1>Analyse de la covariance</h1>
<p>Nous considérerons d’abord un modèle incluant un prédicteur catégoriel et un prédicteur numérique. Dans un contexte expérimental, ce type de modèle porte le nom d’analyse de la covariance (ANCOVA).</p>
<p>Le tableau de données <a href="../donnees/compensation.csv">compensation.csv</a> est tiré du livre de Crawley, <em>Statistics: An introduction using R</em>. Il contient des données sur la masse des graines produites par une espèce de plante (<em>Fruit</em>) en fonction de la taille des racines (<em>Root</em>) et selon que la plante subisse ou non un pâturage (<em>Grazing</em>).</p>
<pre class="r"><code>comp &lt;- read.csv(&quot;../donnees/compensation.csv&quot;)
str(comp)</code></pre>
<pre><code>## &#39;data.frame&#39;:    40 obs. of  3 variables:
##  $ Root   : num  6.22 6.49 4.92 5.13 5.42 ...
##  $ Fruit  : num  59.8 61 14.7 19.3 34.2 ...
##  $ Grazing: Factor w/ 2 levels &quot;Grazed&quot;,&quot;Ungrazed&quot;: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<p>Inspectons d’abord les données.</p>
<pre class="r"><code>ggplot(comp, aes(x = Root, y = Fruit, color = Grazing)) +
    geom_point() +
    scale_color_brewer(palette = &quot;Dark2&quot;)</code></pre>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Le graphique montre bien l’existence d’une relation linéaire entre la taille des racines et la production de graines, ainsi que l’effet du traitement: pour la même taille des racines, le pâturage réduit la production de graines. Notez que si on n’avait pas mesuré les racines, on pourrait croire que l’effet du pâturage est positif.</p>
<pre class="r"><code>ggplot(comp, aes(x = Grazing, y = Fruit)) +
    geom_boxplot()</code></pre>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Cela est dû au fait que les plantes subissant le pâturage avaient (en moyenne) de plus grandes racines au départ. La taille des racines est donc une variable <em>confondante</em> dont il faut tenir compte pour bien évaluer l’effet du pâturage.</p>
<p>Voici un modèle linéaire où l’effet des deux prédicteurs est additif:</p>
<pre class="r"><code>mod_comp &lt;- lm(Fruit ~ Grazing + Root, data = comp)
summary(mod_comp)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Fruit ~ Grazing + Root, data = comp)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.1920  -2.8224   0.3223   3.9144  17.3290 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -127.829      9.664  -13.23 1.35e-15 ***
## GrazingUngrazed   36.103      3.357   10.75 6.11e-13 ***
## Root              23.560      1.149   20.51  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.747 on 37 degrees of freedom
## Multiple R-squared:  0.9291, Adjusted R-squared:  0.9252 
## F-statistic: 242.3 on 2 and 37 DF,  p-value: &lt; 2.2e-16</code></pre>
<div id="interpretation-des-resultats" class="section level2">
<h2>Interprétation des résultats</h2>
<p>Si <span class="math inline">\(x_1\)</span> est la variable de pâturage (0 = Grazed, 1 = Ungrazed selon le codage par défaut dans R) et que <span class="math inline">\(x_2\)</span> est la taille des racines, l’expression mathématique de ce modèle est:</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon \]</span></p>
<p>Pour simplifier l’interprétation des coefficients, on peut séparer le cas avec pâturage (<span class="math inline">\(x_1 = 0\)</span>):</p>
<p><span class="math display">\[ y = \beta_0 + \beta_2 x_2 + \epsilon \]</span></p>
<p>du cas sans pâturage (<span class="math inline">\(x_1 = 1\)</span>):</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 + \beta_2 x_2 + \epsilon \]</span></p>
<p>On peut maintenant intepréter les coefficients comme suit:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> (<code>Intercept</code> dans le tableau sommaire) est l’ordonnée à l’origine de la droite <em>Fruit vs. Root</em> avec pâturage.</li>
<li><span class="math inline">\(\beta_1\)</span> (<code>GrazingUngrazed</code>) est l’effet de l’absence de pâturage sur l’ordonnée à l’origine de <em>Fruit vs. Root</em>.</li>
<li><span class="math inline">\(\beta_2\)</span> (<code>Root</code>) est la pente de la droite <em>Fruit vs. Root</em> avec ou sans pâturage.</li>
</ul>
<p>Puisque la pente est la même avec ou sans pâturage, le coefficient <span class="math inline">\(\beta_1\)</span> correspond à une translation sur l’axe des <span class="math inline">\(y\)</span> de la droite de régression. Ce modèle des effets <em>additifs</em> d’un traitement et d’une variable numérique est donc représenté par deux droites parallèles, ce qui correspond assez bien à notre visualisation des données. En outre, la valeur du <span class="math inline">\(R^2\)</span> (0.93) indique que le modèle explique une grande partie de la variation observée dans les données.</p>
<p>Même une grande valeur de <span class="math inline">\(R^2\)</span> ne signifie pas nécessairement que le modèle est approprié. Il faut toujours observer les graphiques de diagnostic.</p>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Notez que le numéro de la rangée du tableau de données est indiqué à côté de certains points extrêmes, pour faciliter l’identification de points problématiques.</p>
<p>Le test <span class="math inline">\(F\)</span> rapporté au bas du sommaire des résultats de <code>lm</code> correspond à l’hypothèse nulle d’absence d’effet pour tous les prédicteurs.</p>
<p>On peut aussi obtenir un tableau d’ANOVA conventionnel en appliquant la fonction <code>anova</code> au résultat de <code>lm</code>.</p>
<pre class="r"><code>anova(mod_comp)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Fruit
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Grazing    1  2910.4  2910.4  63.929 1.397e-09 ***
## Root       1 19148.9 19148.9 420.616 &lt; 2.2e-16 ***
## Residuals 37  1684.5    45.5                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="modele-avec-interaction" class="section level2">
<h2>Modèle avec interaction</h2>
<p>Comme dans l’ANOVA, on peut inclure l’interaction entre deux variables du modèle avec le symbole <code>*</code>:</p>
<pre class="r"><code>mod_comp_inter &lt;- lm(Fruit ~ Grazing * Root, data = comp)
summary(mod_comp_inter)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Fruit ~ Grazing * Root, data = comp)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.3177  -2.8320   0.1247   3.8511  17.1313 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          -125.173     12.811  -9.771 1.15e-11 ***
## GrazingUngrazed        30.806     16.842   1.829   0.0757 .  
## Root                   23.240      1.531  15.182  &lt; 2e-16 ***
## GrazingUngrazed:Root    0.756      2.354   0.321   0.7500    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 6.831 on 36 degrees of freedom
## Multiple R-squared:  0.9293, Adjusted R-squared:  0.9234 
## F-statistic: 157.6 on 3 and 36 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Si <span class="math inline">\(x_1\)</span> est la variable de pâturage (0 = Grazed, 1 = Ungrazed selon le codage par défaut dans R) et que <span class="math inline">\(x_2\)</span> est la taille des racines, l’expression mathématique de ce modèle est:</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \epsilon \]</span></p>
<p>L’interaction est donc équivalente à l’ajout d’un nouveau prédicteur au modèle, égal au produit des deux variables qui interagissent. Séparons de nouveau en deux équations selon le traitement:</p>
<p>Avec pâturage (<span class="math inline">\(x_1 = 0\)</span>):</p>
<p><span class="math display">\[ y = \beta_0 + \beta_2 x_2 \]</span></p>
<p>Sans pâturage (<span class="math inline">\(x_1 = 1\)</span>):</p>
<p><span class="math display">\[ y = (\beta_0 + \beta_1) + (\beta_2 + \beta_{12}) x_2 \]</span></p>
<p>Pour ce modèle avec interaction, l’interprétation des coefficients change un peu:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> (<code>Intercept</code> dans le tableau sommaire) est l’ordonnée à l’origine de la droite <em>Fruit vs. Root</em> avec pâturage.</li>
<li><span class="math inline">\(\beta_1\)</span> (<code>GrazingUngrazed</code>) est l’effet de l’absence de pâturage sur l’ordonnée à l’origine de <em>Fruit vs. Root</em>.</li>
<li><span class="math inline">\(\beta_2\)</span> (<code>Root</code>) est la pente de la droite <em>Fruit vs. Root</em> avec pâturage.</li>
<li><span class="math inline">\(\beta_{12}\)</span> (<code>GrazingUngrazed:Root</code>) est l’effet de l’absence de pâturage sur la pente de la droite <em>Fruit vs. Root</em>.</li>
</ul>
<p>Le modèle avec interaction est donc équivalent à estimer séparément la droite de régression (ordonnée à l’origine et pente) pour chacun des deux traitements.</p>
<p>Comparé au modèle additif, notez que l’effet de l’absence de pâturage (<code>GrazingUngrazed</code>) a maintenant une erreur-type beaucoup plus élevée et une valeur <span class="math inline">\(p\)</span> plus grande.</p>
<pre class="r"><code>summary(mod_comp)$coefficients</code></pre>
<pre><code>##                   Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)     -127.82936   9.664095 -13.22725 1.349804e-15
## GrazingUngrazed   36.10325   3.357396  10.75335 6.107286e-13
## Root              23.56005   1.148771  20.50892 8.408231e-22</code></pre>
<pre class="r"><code>summary(mod_comp_inter)$coefficients</code></pre>
<pre><code>##                          Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)          -125.1730569  12.811165 -9.7706222 1.150540e-11
## GrazingUngrazed        30.8057049  16.841823  1.8291194 7.567489e-02
## Root                   23.2403732   1.530771 15.1821314 3.173208e-17
## GrazingUngrazed:Root    0.7560338   2.354111  0.3211547 7.499503e-01</code></pre>
<p>Ceci est dû au fait que l’ordonnée à l’origine, correspondant à <em>Root</em> = 0, se situe loin de l’étendue des données (les valeurs de <em>Root</em> sont toutes entre 4 et 11). Donc, un petit changement de pente au milieu du graphique peut mener à un changement important d’ordonnée à l’origine et l’incertitude du coefficient d’interaction (la différence de pente) se répercute aussi sur l’estimation de la différence d’ordonnée à l’origine.</p>
<p>En consultant le tableau d’ANOVA, on peut confirmer que l’interaction n’est pas significative, le modèle additif est donc préférable.</p>
<pre class="r"><code>anova(mod_comp_inter)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Fruit
##              Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    
## Grazing       1  2910.4  2910.4  62.3795 2.262e-09 ***
## Root          1 19148.9 19148.9 410.4201 &lt; 2.2e-16 ***
## Grazing:Root  1     4.8     4.8   0.1031      0.75    
## Residuals    36  1679.6    46.7                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Pourquoi l’effet du pâturage (<em>Grazing</em>) est-il significatif ici? Dans le tableau d’ANOVA, on teste s’il y a une différence significative de la moyenne de <em>Fruit</em> entre les plantes subissant ou non un pâturage, plutôt que de savoir s’il y a une différence significative d’ordonnée à l’origine (dans le cas du coefficient <code>GrazingUngrazed</code> du modèle linéaire).</p>
</div>
</div>
<div id="regression-avec-plusieurs-predicteurs-numeriques" class="section level1">
<h1>Régression avec plusieurs prédicteurs numériques</h1>
<p>Le tableau de données <code>hills</code> du package <em>MASS</em> (inclus par défaut avec R) contient les records de temps (<em>time</em>, en minutes) pour des courses de vélo en Écosse en fonction de la distance horizontale (<em>dist</em>, en milles) et le dénivelé total du parcours (<em>climb</em>, en pieds).</p>
<pre class="r"><code>library(MASS)
str(hills)</code></pre>
<pre><code>## &#39;data.frame&#39;:    35 obs. of  3 variables:
##  $ dist : num  2.5 6 6 7.5 8 8 16 6 5 6 ...
##  $ climb: int  650 2500 900 800 3070 2866 7500 800 800 650 ...
##  $ time : num  16.1 48.4 33.6 45.6 62.3 ...</code></pre>
<p>Pour un tableau de données avec plusieurs variables numériques, la fonction <code>plot</code> affiche une matrice de nuages de points pour chaque paire de variables.</p>
<pre class="r"><code>plot(hills)</code></pre>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Les temps records semblent dépendre linéairement de la distance et du dénivelé. (La distance et le dénivelé semblent aussi corrélés, nous y reviendrons plus tard.) Nous appliquons donc un modèle linéaire à ces données.</p>
<pre class="r"><code>mod_hills &lt;- lm(time ~ dist + climb, hills)</code></pre>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Puisque les rangées de ce tableau de données sont identifiées par des noms (<code>rownames</code> dans R), ces noms apparaissent vis-à-vis les valeurs extrêmes dans les graphiques de diagnostic.</p>
<p>D’après ces graphiques, deux parcours (Knock Hill et Bens of Jura) ont un temps record beaucoup plus long qu’attendu (résidu positif important). Ces mêmes points ont aussi une grande influence sur les coefficients de la régression (d’après le quatrième graphique). Dans ce cas, il faudrait vérifier si ces parcours ont des particularités qui expliquent cette forte différence par rapport au modèle.</p>
<div id="normalisation-des-variables" class="section level2">
<h2>Normalisation des variables</h2>
<p>Regardons le sommaire des résultats du modèle:</p>
<pre class="r"><code>summary(mod_hills)</code></pre>
<pre><code>## 
## Call:
## lm(formula = time ~ dist + climb, data = hills)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.215  -7.129  -1.186   2.371  65.121 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -8.992039   4.302734  -2.090   0.0447 *  
## dist         6.217956   0.601148  10.343 9.86e-12 ***
## climb        0.011048   0.002051   5.387 6.45e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.68 on 32 degrees of freedom
## Multiple R-squared:  0.9191, Adjusted R-squared:  0.914 
## F-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>La valeur des coefficients signifie qu’en moyenne, chaque mille de distance ajoute 6.22 minutes au temps record tandis que chaque pied de dénivelé ajoute 0.01 minute. Puisque les prédicteurs n’ont pas les mêmes unités, la valeur des coefficients n’est pas indicatrice de l’importance de chaque variable. Dans ce cas-ci, <em>dist</em> varie entre 2 et 28 milles tandis que <em>climb</em> varie entre 300 et 7500 pieds.</p>
<p>Aussi, l’ordonnée à l’origine n’a pas vraiment de sens concret, puisqu’un parcours de longueur 0 n’est pas possible.</p>
<p>Afin de comparer l’influence de différents prédicteurs, il peut être utile de les normaliser ceux-ci, c’est-à-dire de transformer chaque valeur en soustrayant la moyenne et en divisant par l’écart-type. Dans R, la fonction <code>scale</code> effectue automatiquement cette transformation.</p>
<pre class="r"><code>hills_scl &lt;- hills
hills_scl[, -3] &lt;- scale(hills_scl[, -3]) # on ne normalise pas la réponse
mod_hills_scl &lt;- lm(time ~ dist + climb, data = hills_scl)
summary(mod_hills_scl)</code></pre>
<pre><code>## 
## Call:
## lm(formula = time ~ dist + climb, data = hills_scl)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.215  -7.129  -1.186   2.371  65.121 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   57.876      2.481  23.331  &lt; 2e-16 ***
## dist          34.348      3.321  10.343 9.86e-12 ***
## climb         17.888      3.321   5.387 6.45e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 14.68 on 32 degrees of freedom
## Multiple R-squared:  0.9191, Adjusted R-squared:  0.914 
## F-statistic: 181.7 on 2 and 32 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Pour chaque point, la variable normalisée indique l’écart à la moyenne de la variable originale, exprimé en multiple de l’écart-type de la variable originale. Par exemple, dans cette version du modèle, le coefficient <em>dist</em> indique la différence de temps associée à une augmentation d’un écart-type de la distance horizontale. Les coefficients normalisés représentent ainsi l’effet d’une variable relativement aux écarts typiques observés pour cette variable.</p>
<p>Autre avantage de cette représentation, puisque les prédicteurs normalisés prennent une valeur de 0 à leur moyenne, la valeur de l’ordonnée à l’origine de la régression est la moyenne générale de la réponse (ici, le temps record moyen est d’environ 58 minutes).</p>
<p>La normalisation des prédicteurs ne fait que changer l’échelle des effets estimés. La significativité de l’effet de chaque prédicteur et les prédictions du modèle restent les mêmes.</p>
</div>
<div id="interaction-entre-variables-continues" class="section level2">
<h2>Interaction entre variables continues</h2>
<p>Comment interpréter l’interaction entre deux variables continues? Par exemple:</p>
<pre class="r"><code>mod_hills_inter &lt;- lm(time ~ dist * climb, hills_scl)
summary(mod_hills_inter)</code></pre>
<pre><code>## 
## Call:
## lm(formula = time ~ dist * climb, data = hills_scl)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -25.994  -4.968  -2.220   2.381  56.115 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   52.304      2.793  18.728  &lt; 2e-16 ***
## dist          32.776      2.965  11.053 2.78e-12 ***
## climb         10.411      3.742   2.782  0.00911 ** 
## dist:climb     8.793      2.745   3.203  0.00314 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.92 on 31 degrees of freedom
## Multiple R-squared:  0.9392, Adjusted R-squared:  0.9333 
## F-statistic: 159.6 on 3 and 31 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Comme nous avons vu plus tôt, l’équation d’un modèle à deux variables avec interaction est:</p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1 x_2 + \epsilon \]</span></p>
<p>On peut ré-écrire cette équation de deux façons:</p>
<p><span class="math display">\[ y = \beta_0 + (\beta_1 + \beta_{12} x_2) x_1 + \beta_2 x_2 \]</span></p>
<p><span class="math display">\[ y = \beta_0 + \beta_1 x_1 + (\beta_2 + \beta_{12} x_1) x_2 \]</span></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> est la valeur de <span class="math inline">\(y\)</span> lorsque <span class="math inline">\(x_1 = 0\)</span> et <span class="math inline">\(x_2 = 0\)</span>;</li>
<li><span class="math inline">\(\beta_1\)</span> est l’effet sur <span class="math inline">\(y\)</span> d’une augmentation d’une unité de <span class="math inline">\(x_1\)</span> si <span class="math inline">\(x_2 = 0\)</span>;</li>
<li><span class="math inline">\(\beta_2\)</span> est l’effet sur <span class="math inline">\(y\)</span> d’une augmentation d’une unité de <span class="math inline">\(x_2\)</span> si <span class="math inline">\(x_1 = 0\)</span>;</li>
<li><span class="math inline">\(\beta_{12}\)</span> représente à la fois l’augmentation de la pente de la relation <span class="math inline">\(y\)</span> vs. <span class="math inline">\(x_1\)</span> lorsque <span class="math inline">\(x_2\)</span> augmente d’une unité, et l’augmentation de la pente de la relation <span class="math inline">\(y\)</span> vs. <span class="math inline">\(x_2\)</span> lorsque <span class="math inline">\(x_1\)</span> augmente d’une unité.</li>
</ul>
</div>
<div id="collinearite" class="section level2">
<h2>Collinéarité</h2>
<p>Pour le jeu de données <code>hills</code>, les deux prédicteurs (<em>dist</em> et <em>climb</em>) sont eux-mêmes corrélés.</p>
<pre class="r"><code>cor(hills$dist, hills$climb)</code></pre>
<pre><code>## [1] 0.6523461</code></pre>
<p>La corrélation entre deux prédicteurs complique l’estimation des effets de chaque prédicteur. Puisque les coefficients du modèle représente l’effet d’un prédicteur lorsque les autres demeurent constants, lorsque plusieurs d’entre eux varient ensemble, il devient difficile d’isoler l’effet de chacun. Ce problème se généralise aux modèles avec plus de deux prédicteurs, si l’un des prédicteurs est corrélé avec une combinaison linéaire des autres prédicteurs: on parle alors de <strong>collinéarité</strong>.</p>
<p>La fonction <code>vif</code> du package <em>car</em> calcule le facteur d’inflation de la variance (VIF, pour <em>variance inflation factor</em>) de chaque prédicteur.</p>
<pre class="r"><code>library(car)
vif(mod_hills)</code></pre>
<pre><code>##     dist    climb 
## 1.740812 1.740812</code></pre>
<p>Le VIF est égal à <span class="math inline">\(1 - 1/R^2\)</span>, où <span class="math inline">\(R^2\)</span> est le coefficient de détermination d’un modèle linéaire du prédicteur considéré en fonction de tous les autres. Par exemple, si un des prédicteurs peut être déterminé à partir de la valeur des autres avec un <span class="math inline">\(R^2\)</span> de 0.9, VIF = 10. Lorsque le VIF de certains prédicteurs dépasse 10, il est recommandé d’éliminer un des prédicteurs redondants.</p>
</div>
<div id="exemple-1" class="section level2">
<h2>Exemple</h2>
<p>Le tableau de données <code>msleep</code> inclus avec le package <em>ggplot</em>, contient des données sur le sommeil de différentes espèces de mammifères. Nous choisissons trois colonnes correspondant au temps de sommeil total (<em>sleep_total</em>), au poids de l’animal (<em>bodywt</em>) et au poids de son cerveau (<em>brainwt</em>).</p>
<pre class="r"><code>msleep2 &lt;- dplyr::select(msleep, sleep_total, bodywt, brainwt)
summary(msleep2)</code></pre>
<pre><code>##   sleep_total        bodywt            brainwt       
##  Min.   : 1.90   Min.   :   0.005   Min.   :0.00014  
##  1st Qu.: 7.85   1st Qu.:   0.174   1st Qu.:0.00290  
##  Median :10.10   Median :   1.670   Median :0.01240  
##  Mean   :10.43   Mean   : 166.136   Mean   :0.28158  
##  3rd Qu.:13.75   3rd Qu.:  41.750   3rd Qu.:0.12550  
##  Max.   :19.90   Max.   :6654.000   Max.   :5.71200  
##                                     NA&#39;s   :27</code></pre>
<p>Les variables <em>bodywt</em> et <em>brainwt</em> sont très asymétriques et varient sur plusieurs ordres de grandeur, donc nous effectuons une transformation logarithmique des trois variables.</p>
<pre class="r"><code>msleep2 &lt;- log(msleep2)</code></pre>
<p>Sur une échelle logarithmique, les variables <em>bodywt</em> et <em>brainwt</em> sont fortement corrélées.</p>
<pre class="r"><code>plot(msleep2)</code></pre>
<p><img src="7-Régression_linéaire_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Voici ce qu’il arrive lorsqu’on estime un modèle linéaire avec un ou deux prédicteurs.</p>
<pre class="r"><code>summary(lm(sleep_total ~ bodywt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ bodywt, data = msleep2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9757 -0.2712 -0.0096  0.2727  1.0004 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.31575    0.04647  49.830  &lt; 2e-16 ***
## bodywt      -0.10265    0.01389  -7.388 1.19e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4097 on 81 degrees of freedom
## Multiple R-squared:  0.4026, Adjusted R-squared:  0.3952 
## F-statistic: 54.58 on 1 and 81 DF,  p-value: 1.193e-10</code></pre>
<pre class="r"><code>summary(lm(sleep_total ~ brainwt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ brainwt, data = msleep2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.89025 -0.27043 -0.01841  0.30664  0.88271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.69551    0.10037  16.892  &lt; 2e-16 ***
## brainwt     -0.12640    0.02103  -6.011 1.64e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3943 on 54 degrees of freedom
##   (27 observations deleted due to missingness)
## Multiple R-squared:  0.4009, Adjusted R-squared:  0.3898 
## F-statistic: 36.13 on 1 and 54 DF,  p-value: 1.637e-07</code></pre>
<pre class="r"><code>summary(lm(sleep_total ~ bodywt + brainwt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.92817 -0.25572 -0.01949  0.28150  1.01779 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.317881   0.382108   6.066 1.42e-07 ***
## bodywt      -0.105861   0.062787  -1.686   0.0977 .  
## brainwt      0.002517   0.079212   0.032   0.9748    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3877 on 53 degrees of freedom
##   (27 observations deleted due to missingness)
## Multiple R-squared:  0.4314, Adjusted R-squared:  0.4099 
## F-statistic:  20.1 on 2 and 53 DF,  p-value: 3.181e-07</code></pre>
<p>Comparez les erreurs-types des coefficients du troisième modèle par rapport à ceux où chacun des prédicteurs est considéré séparément. Nous pouvons confirmer que ces variables sont fortement collinéaires avec <code>vif</code>.</p>
<pre class="r"><code>vif(lm(sleep_total ~ bodywt + brainwt, data = msleep2))</code></pre>
<pre><code>##   bodywt  brainwt 
## 14.67386 14.67386</code></pre>
<p>S’il faut ne conserver qu’une seule des deux variables du modèle, laquelle choisir? Nous discuterons des problèmes de comparaison et de sélection de modèles lors du prochain cours.</p>
</div>
</div>
<div id="resume" class="section level1">
<h1>Résumé</h1>
<ul>
<li><p>La fonction <code>lm</code> effectue l’ajustement d’un modèle de régression linéaire dans R.</p></li>
<li><p>Dans une régression linéaire multiple (sans interaction), le coefficient associé à un prédicteur mesure l’effet d’une variation de 1 du prédicteur sur la réponse, si les autres prédicteurs demeurent constants.</p></li>
<li><p>Pour un modèle sans interaction, l’effet d’une variable catégorielle peut être représenté par une translation de la droite de régression entre les traitements.</p></li>
<li><p>Le facteur d’inflation de la variance (VIF) indique si la valeur d’un prédicteur est fortement corrélée à celles des autres prédicteurs. Un VIF élevé rend difficile l’estimation des coefficients pour les prédicteurs corrélés.</p></li>
<li><p>L’intervalle de confiance d’une droite de régression représente l’incertitude sur la valeur moyenne de <span class="math inline">\(y\)</span> pour des valeurs données des prédicteurs. L’intervalle de prédiction représente l’incertitude sur la valeur d’une observation future de <span class="math inline">\(y\)</span>, connaissant la valeur des prédicteurs.</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
