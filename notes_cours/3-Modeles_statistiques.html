<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Modèles statistiques et intervalles de confiance</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Modèles statistiques et intervalles de confiance</h1>
<h4 class="date"><br/>13 septembre 2021</h4>

</div>


<div id="objectifs" class="section level1">
<h1>Objectifs</h1>
<ul>
<li><p>Décrire les caractéristiques et l’utilité des distributions normale et log-normale.</p></li>
<li><p>Connaître la relation entre densité de probabilité et probabilité cumulative pour une variable continue, et calculer ces quantités dans R.</p></li>
<li><p>Comparer des données à une distribution de référence avec un diagramme quantile-quantile.</p></li>
<li><p>Interpréter un intervalle de confiance et calculer l’intervalle de confiance pour la moyenne d’une distribution normale.</p></li>
</ul>
</div>
<div id="distributions-statistiques" class="section level1">
<h1>Distributions statistiques</h1>
<p>Une distribution statistique (aussi appelée loi de probabilité) est une fonction qui associe une probabilité à chaque valeur possible d’une variable aléatoire.</p>
<div id="distribution-discrète" class="section level2">
<h2>Distribution discrète</h2>
<p>Lorsque la variable est discrète, chaque valeur a une masse de probabilité, dont la somme doit être égale à 1. Par exemple, si <span class="math inline">\(x\)</span> correspond au nombre obtenu en lançant un dé équilibré à 6 faces, la probabilité de <span class="math inline">\(x\)</span> est de 1/6 pour chacun des nombres de 1 à 6. Puisque la probabilité est identique pour chaque valeur, on a ici une <strong>distribution uniforme</strong>.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="distribution-continue" class="section level2">
<h2>Distribution continue</h2>
<p>Lorsque la variable est continue, le nombre de valeurs possible est infini, donc la probabilité d’obtenir précisément une valeur donnée est de zéro. La fonction de distribution associe donc une <em>densité</em> de probabilité à une valeur donnée.</p>
<p>Par exemple, voici une distribution de probabilité uniforme entre 0 et 6. La densité de probabilité est constante (1/6) dans l’intervalle et zéro à l’extérieur.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Pour une distribution continue, la probabilité que la variable se trouve dans un intervalle donné correspond à l’intégrale (l’aire sous la courbe) de la densité de probabilité dans cet intervalle.</p>
<p>Ici, la densité de probabilité est rectangulaire, donc il est facile de calculer la probabilité d’un intervalle. Par exemple, la probabilité d’obtenir une valeur entre 2.5 et 3 est égale à 1/2 (largeur de l’intervalle) x 1/6 (densité de probabilité) = 1/12 (~0.083). Cette valeur correspond à l’aire du rectangle rempli dans le graphique ci-dessous.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>L’intégrale de la densité de probabilité sur l’ensemble des valeurs possibles de <span class="math inline">\(x\)</span> (probabilité totale) doit être égale à 1.</p>
</div>
<div id="loi-des-grands-nombres" class="section level2">
<h2>Loi des grands nombres</h2>
<p>En R, la commande suivante génère dix valeurs aléatoires (<code>n = 10</code>) tirées de la distribution uniforme (continue) entre 0 et 6, que nous avons vue dans la section précédente.</p>
<pre class="r"><code>x &lt;- runif(n = 10, min = 0, max = 6)
round(x, 2) # round affiche les valeurs avec 2 décimales</code></pre>
<pre><code>##  [1] 5.02 5.02 3.36 5.26 4.66 4.25 3.09 0.92 5.38 0.42</code></pre>
<p>Voici les histogrammes des valeurs obtenues pour différents <span class="math inline">\(n\)</span>. Que remarquez-vous?</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>La fonction <code>runif</code> simule l’échantillonnage d’une variable qui suit une distribution uniforme. Plus la taille de l’échantillon aléatoire est grand, plus la distribution des valeurs de cet échantillon s’approche de la distribution de la variable dans la population. C’est ce qu’on appelle la <strong>loi des grands nombres</strong>.</p>
</div>
</div>
<div id="la-distribution-normale" class="section level1">
<h1>La distribution normale</h1>
<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>Dans un système complexe, les variables que nous observons résultent de l’effet combiné de nombreux processus que nous ne pouvons pas directement percevoir. Par exemple, la taille d’une personne est influencée par un grand nombre de facteurs génétiques et environnementaux, le rendement d’un champ dépend de la météo pour chaque jour de la saison de croissance ainsi que du micro-habitat perçu par chaque plant, etc. Modéliser chacun de ces processus n’est généralement pas possible. Heureusement, lorsque de nombreux facteurs agissent indépendamment sur une même variable, leur effet total tend à converger vers certaines distributions statistiques bien connues. Nous verrons ici un exemple simple de ce phénomène.</p>
<p>Supposons que nous nous intéressons à une variable aléatoire qui est elle-même la <em>somme</em> de <span class="math inline">\(n\)</span> variables indépendantes, et que chacune de ces variables suit la distribution uniforme entre 0 et 6 présentée ci-dessus. Même si nous ne connaissons pas la distribution de cette somme, la loi des grands nombres nous permet de l’approximer à partir de simulations. Nous créons donc une fonction qui génère <span class="math inline">\(n\)</span> valeurs de la distribution uniforme et calcule leur somme, puis nous générons ensuite 10 000 valeurs de cette somme (avec <code>replicate</code>) pour une valeur de <span class="math inline">\(n\)</span> donnée.</p>
<pre class="r"><code># Somme de n variables aléatoires uniformes entre min et max
somme_unif &lt;- function(n, min, max) {
    sum(runif(n, min, max))
}

n &lt;- 10
x &lt;- replicate(10000, somme_unif(n, 0, 6))</code></pre>
<p>Voici l’histogramme des valeurs obtenues pour plusieurs valeurs de <span class="math inline">\(n\)</span>. Que remarquez-vous?</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>La somme de 2 valeurs a une distribution plutôt triangulaire, mais à partir de <span class="math inline">\(n\)</span> = 5, on voit apparaître la forme de cloche d’une distribution normale. Ce résultat est un exemple d’une loi statistique plus générale, le théorème de la limite centrale.</p>
</div>
<div id="théorème-de-la-limite-centrale" class="section level2">
<h2>Théorème de la limite centrale</h2>
<p>Le théorème de la limite centrale indique que lorsqu’on additionne un grand nombre de variables aléatoires indépendantes, peu importe la distribution des variables prises individuellement, la distribution de leur somme s’approche d’une distribution normale.</p>
<p><em>Pour être strict, il faudrait inclure quelques conditions techniques au sujet des variables qu’on additionne, mais la définition simplifiée ci-dessus suffit pour ce cours.</em></p>
<p>Cette propriété de la distribution normale explique en partie pourquoi elle constitue un modèle si important en statistiques. Comme nous avons mentionné plus tôt, les variables mesurées dans un système complexe représentent l’effet de nombreux processus à petite échelle. Si on suppose que ces effets sont indépendants et additifs, alors il est naturel que le résultat s’approche d’une distribution normale. Toutefois, il est important de vérifier cette supposition pour une variable donnée.</p>
</div>
<div id="distribution-normale" class="section level2">
<h2>Distribution normale</h2>
<p>Si une variable <span class="math inline">\(x\)</span> suit une distribution normale (aussi appelée gaussienne), sa densité de probabilité est donnée par:</p>
<p><span class="math display">\[f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2}\]</span></p>
<p>Cette distribution a deux <em>paramètres</em>, <span class="math inline">\(\mu\)</span> (qui correspond à la moyenne de <span class="math inline">\(x\)</span>) et <span class="math inline">\(\sigma\)</span> (qui correspond à son écart-type).</p>
<p>Sur un graphique de <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(\mu\)</span> correspond à la position du centre de la distribution, tandis que <span class="math inline">\(\sigma\)</span> correspond à sa dispersion; plus <span class="math inline">\(\sigma\)</span> est élevé, plus la distribution s’étend et moins elle est concentrée près de la moyenne.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-8-1.png" width="100%" /></p>
</div>
<div id="distribution-normale-centrée-réduite" class="section level2">
<h2>Distribution normale centrée réduite</h2>
<p>Si une variable <span class="math inline">\(x\)</span> suit une distribution normale de moyenne <span class="math inline">\(\mu\)</span> et d’écart-type <span class="math inline">\(\sigma\)</span>, on peut obtenir une version centrée réduite de <span class="math inline">\(x\)</span> (notée <span class="math inline">\(z\)</span>) en soustrayant <span class="math inline">\(\mu\)</span>, puis en divisant par <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[z = \frac{x - \mu}{\sigma}\]</span></p>
<blockquote>
<p>Dans R, la fonction <code>scale(x)</code> appliquée à un vecteur <code>x</code> produit une version centrée réduite (obtenue en soustrayant la moyenne de <code>x</code> et en divisant par l’écart-type).</p>
</blockquote>
<p>La variable <span class="math inline">\(z\)</span> suit alors une distribution normale centrée réduite, c’est-à-dire <span class="math inline">\(\mu\)</span> = 0 et <span class="math inline">\(\sigma\)</span> = 1:</p>
<p><span class="math display">\[f(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} z^2}\]</span></p>
<p>Autrement dit, n’importe quelle distribution normale peut être obtenue à partir de <span class="math inline">\(f(z)\)</span> en déplaçant le centre d’une distance <span class="math inline">\(\mu\)</span> et en “étirant” la distribution d’un facteur <span class="math inline">\(\sigma\)</span>.</p>
<p>Les valeurs de <span class="math inline">\(z\)</span> correspondent à la distance de la moyenne, exprimée en unités d’écart-type, ex.: <span class="math inline">\(z\)</span> = -1.5 signifie un écart-type et demi en-dessous de la moyenne.</p>
</div>
</div>
<div id="distribution-cumulative" class="section level1">
<h1>Distribution cumulative</h1>
<p>Nous avons vu précédemment que la probabilité qu’une variable aléatoire continue se retrouve dans un certain intervalle correspond à l’aire sous la courbe (l’intégrale) de la densité de probabilité dans cet intervalle.</p>
<p>La distribution cumulative d’une variable aléatoire (aussi appelée fonction de répartition) correspond pour chaque valeur <span class="math inline">\(x\)</span> à la probabilité que la valeur de la variable soit inférieure ou égale à <span class="math inline">\(x\)</span>. Elle est donc égale à l’aire sous la courbe de la densité de probabilité à gauche de <span class="math inline">\(x\)</span>.</p>
<p>Voici une illustration de la distribution cumulative <span class="math inline">\(F(z)\)</span> d’une variable normale centrée réduite <span class="math inline">\(z\)</span>.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>À partir de la distribution cumulative <span class="math inline">\(F(x)\)</span>, on peut facilement calculer la probabilité que <span class="math inline">\(x\)</span> se trouve dans un intervalle <span class="math inline">\((x_1, x_2)\)</span> en faisant la soustraction <span class="math inline">\(F(x_2) - F(x_1)\)</span>.</p>
</div>
<div id="les-fonctions-de-distribution-dans-r" class="section level1">
<h1>Les fonctions de distribution dans R</h1>
<p>Quatre fonctions dans R permettent de travailler avec la distribution normale. Dans chaque cas, il faut spécifier la moyenne (<code>mean</code>) et l’écart-type (<code>sd</code>) de la distribution.</p>
<ul>
<li><p><code>rnorm(n, mean, sd)</code> génère <code>n</code> valeurs aléatoires à partir d’une distribution normale de moyenne <code>mean</code> et d’écart-type <code>sd</code>.</p></li>
<li><p><code>dnorm(x, mean, sd)</code> donne la densité de probabilité associée à la valeur <code>x</code>.</p></li>
<li><p><code>pnorm(q, mean, sd)</code> donne la probabilité cumulative associée à une valeur <code>q</code>.</p></li>
<li><p><code>qnorm(p, mean, sd)</code> donne la valeur (<code>q</code> pour quantile) associée à une probabilité cumulative <code>p</code> donnée.</p></li>
</ul>
<p>Des fonctions semblables sont définies pour d’autres distributions courantes, comme nous le verrons plus tard.</p>
<p>Par exemple, pour la distribution normale centrée réduite:</p>
<ul>
<li>la probabilité cumulative à 2 écarts-type est de 98%;</li>
</ul>
<pre class="r"><code>pnorm(2, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] 0.9772499</code></pre>
<ul>
<li>la probabilité d’être à l’intérieur d’un écart-type de part et d’autre de la moyenne est de 68%;</li>
</ul>
<pre class="r"><code>pnorm(1, mean = 0, sd = 1) - pnorm(-1, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] 0.6826895</code></pre>
<ul>
<li>le troisième quartile (75% de probabilité cumulative) est à 0.67 écart-type au-dessus de la moyenne.</li>
</ul>
<pre class="r"><code>qnorm(0.75, mean = 0, sd = 1)</code></pre>
<pre><code>## [1] 0.6744898</code></pre>
</div>
<div id="diagramme-quantile-quantile" class="section level1">
<h1>Diagramme quantile-quantile</h1>
<p>Le diagramme quantile-quantile (<em>Q-Q plot</em>) sert à visualiser la correspondance entre deux distributions statistiques; le plus souvent, nous voulons comparer un échantillon à une distribution théorique donnée.</p>
<p>Par exemple, supposons que nous avons 99 observations d’une variable et nous voulons vérifier que sa distribution soit approximativement normale. Nous trions les observations en ordre croissant et associons la première observation au 1er centile de la distribution normale centrée réduite, la deuxième observation au 2e centile, et ainsi de suite jusqu’au 99e centile. Si l’échantillon provient d’une distribution normale, le nuage de points produit par cette association formera une ligne droite.</p>
<p><em>En effet, si <span class="math inline">\(x\)</span> a une distribution normale, alors <span class="math inline">\(x = \mu + \sigma z\)</span> où <span class="math inline">\(z\)</span> est une variable normale centrée réduite.</em></p>
<p>Dans R, nous pouvons comparer un vecteur à la distribution normale avec la fonction <code>qqnorm</code> et ajouter une ligne droite au graphique avec la fonction <code>qqline</code>.</p>
<pre class="r"><code>test &lt;- rnorm(99, mean = 6, sd = 4)

qqnorm(test)
qqline(test)</code></pre>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Comme vous voyez, pour un échantillon aléatoire tiré d’une distribution normale, la correspondance est très bonne; il y a néanmoins un peu de variation due à l’échantillonnage d’un nombre limité de points.</p>
<p>Maintenant, regardons le diagramme quantile-quantile du diamètre des arbres (DHP) dans le jeu de données de Kejimkujik, tel que vu au dernier cours.</p>
<pre class="r"><code>kejim &lt;- read.csv(&quot;../donnees/cours1_kejimkujik.csv&quot;)

dhp &lt;- kejim$dhp

qqnorm(dhp)
qqline(dhp)</code></pre>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>La distribution n’est clairement pas normale. Plus précisément, on constate que:</p>
<ul>
<li><p>Pour les valeurs sous la moyenne (à gauche), les points sont au-dessus de la droite, donc les quantiles de l’échantillon sont plus élevés que ceux d’une distribution normale. En étant plus élevés, ils sont plus rapprochés de la moyenne.</p></li>
<li><p>Pour les valeurs au-dessus de la moyenne (à droite), les quantiles de l’échantillon sont aussi plus élevés que ceux de la distribution normale. Mais dans ce cas-ci, ils sont donc plus éloignés de la moyenne.</p></li>
</ul>
<p>Ainsi, le diagramme quantile-quantile nous indique que la distribution est asymétrique avec des valeurs plus rapprochées à gauche et plus éloignées à droite. Puisqu’il s’agit d’une différence assez flagrante, on pouvait la détecter plus facilement avec un histogramme (ci-dessous). Toutefois, le diagramme quantile-quantile peut détecter des différences plus subtiles, il est donc utile d’apprendre à lire et interpréter ce graphique.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<div id="exercice" class="section level2">
<h2>Exercice</h2>
<p>Voici un diagramme quantile-quantile comparant un échantillon à une distribution normale. Pouvez-vous décrire comment cet échantillon diffère de la distribution théorique?</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
</div>
<div id="la-distribution-log-normale" class="section level1">
<h1>La distribution log-normale</h1>
<div id="définition" class="section level2">
<h2>Définition</h2>
<p>Une variable <span class="math inline">\(x\)</span> suit une distribution log-normale si <span class="math inline">\(y = log(x)\)</span> suit une distribution normale.</p>
<p>De façon équivalente, si <span class="math inline">\(y\)</span> suit une distribution normale, <span class="math inline">\(x = e^y\)</span> suit une distribution log-normale.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="propriétés-des-logarithmes" class="section level2">
<h2>Propriétés des logarithmes</h2>
<ul>
<li><p><span class="math inline">\(log(x)\)</span> est seulement défini pour <span class="math inline">\(x &gt; 0\)</span>;</p></li>
<li><p><span class="math inline">\(log(x) = 0\)</span> si <span class="math inline">\(x = 1\)</span>. Un logarithme négatif ou positif représente une valeur de <span class="math inline">\(x\)</span> inférieure ou supérieure à 1, respectivement.</p></li>
<li><p>Le logarithme transforme les multiplications en additions et les divisions en soustractions.</p></li>
</ul>
<p><span class="math display">\[log(xw) = log(x) + log(w)\]</span> <span class="math display">\[log(x/v) = log(x) - log(v)\]</span></p>
<ul>
<li><p>Donc, dans une échelle logarithmique, la distance entre deux nombres est proportionnelle à leur ratio dans l’échelle originale.</p></li>
<li><p>Si nous ne spécifions pas, les logarithmes sont des logarithmes naturels (base <span class="math inline">\(e\)</span>). Toutefois, un changement de base correspond seulement à un changement d’échelle et n’affecte pas la forme de la distribution. Par exemple, pour convertir en base 10:</p></li>
</ul>
<p><span class="math display">\[log_{10}(x) = \frac{log(x)}{log(10)}\]</span></p>
</div>
<div id="utilité-de-la-distribution-log-normale" class="section level2">
<h2>Utilité de la distribution log-normale</h2>
<p>Si la distribution normale tend à être associée à des processus additifs (somme de nombreux effets indépendants), la distribution log-normale est associée à des processus multiplicatifs. Par exemple, si une population croît de 5%, 10% et 3% lors de trois années consécutives, l’accroissement cumulatif correspond à la multiplication: 1.05 x 1.10 x 1.03 = 1.19, soit une augmentation de 19%. Dans un processus multiplicatif, plus une variable est grande, plus elle peut croître, ce qui explique que la distribution résultante soit asymétrique et s’étire vers la droite.</p>
<p>Souvenons-nous que la distribution du DHP de tous les arbres du jeu de données Kejimkujik avait ce type d’asymétrie. Pour vérifier si la distribution du DHP est approximativement log-normale, observons le diagramme quantile-quantile pour le logarithme du DHP.</p>
<pre class="r"><code>qqnorm(log(dhp))
qqline(log(dhp))</code></pre>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>On observe une meilleure correspondance que pour les données non-transformées, sauf au niveau des valeurs les plus petites du DHP dans l’échantillon, qui demeurent plus élevées que prévu par la distribution de référence. Avez-vous une hypothèse pour cette anomalie près du minimum? (Indice: Quels arbres ne sont pas échantillonnés?)</p>
</div>
<div id="transformation-logarithmique" class="section level2">
<h2>Transformation logarithmique</h2>
<p>Dans les prochaines semaines, nous verrons plusieurs méthodes statistiques qui supposent toutes que la variable observée est expliquée par des effets additifs, avec une composante aléatoire suivant une distribution normale.</p>
<p>Ainsi, si le processus qui nous intéresse est plutôt multiplicatif et que la variable mesurée s’approche d’une distribution log-normale, nous pouvons modéliser cette variable après lui avoir appliqué une transformation logarithmique. Il faut toutefois être prudent lors de l’interprétation des résultats. En particulier, la moyenne de <span class="math inline">\(log(x)\)</span> n’est <em>pas</em> égale au logarithme de la moyenne de <span class="math inline">\(x\)</span>, comme le montre ce graphique.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Dans le graphique ci-dessus, les deux distributions de <span class="math inline">\(y = log(x)\)</span> ont le même mode (maximum de probabilité), la même médiane et la même moyenne à 0. Toutefois, sur l’échelle originale, la moyenne de <span class="math inline">\(x\)</span> est plus élevée pour la distribution en bleu, tandis que le mode est plus petit; les deux distributions ont la même valeur médiane (égale à 1).</p>
</div>
<div id="exercice-1" class="section level2">
<h2>Exercice</h2>
<p>Parmi les variables suivantes, lesquelles s’approchent à votre avis le plus d’une distribution normale, et pourquoi?</p>
<ol style="list-style-type: lower-alpha">
<li><p>La température moyenne de septembre (variation d’une année à l’autre) à Rouyn-Noranda.</p></li>
<li><p>La population des municipalités du Québec.</p></li>
<li><p>Le nombre d’abonnés par compte dans un réseau social (ex.: Twitter).</p></li>
<li><p>Les ventes hebdomadaires de pain dans un supermarché.</p></li>
</ol>
</div>
</div>
<div id="résumé" class="section level1">
<h1>Résumé</h1>
<ul>
<li><p>Une distribution discrète est représenté par une fonction de masse de probabilité; une distribution continue est représentée par une fonction de densité de probabilité.</p></li>
<li><p>La distribution cumulative d’une variable au point <span class="math inline">\(x\)</span> donne la probabilité que cette variable soit inférieure ou égale à <span class="math inline">\(x\)</span>.</p></li>
<li><p>Quelques distributions continues: uniforme, normale, log-normale. (Nous verrons plus d’exemples de distributions discrètes et continues au cours de la session.)</p></li>
<li><p>La distribution normale est caractérisée par sa moyenne <span class="math inline">\(\mu\)</span> et son écart-type <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>Toute distribution normale peut être ramenée à la distribution normale centrée réduite (<span class="math inline">\(\mu\)</span> = 0, <span class="math inline">\(\sigma\)</span> = 1) avec la transformation linéaire: <span class="math inline">\(z = (x - \mu)/\sigma\)</span>.</p></li>
<li><p>La transformation logarithmique convertit les effets multiplicatifs en effets additifs, et les distributions log-normales en distributions normales.</p></li>
<li><p>Le diagramme quantile-quantile permet de comparer visuellement des données à une distribution de référence.</p></li>
</ul>
</div>
<div id="intervalle-de-confiance" class="section level1">
<h1>Intervalle de confiance</h1>
<div id="estimateur-suivant-une-distribution-normale" class="section level2">
<h2>Estimateur suivant une distribution normale</h2>
<p>Si un échantillon est tiré d’une distribution de moyenne <span class="math inline">\(\mu\)</span> et d’écart-type <span class="math inline">\(\sigma\)</span>, nous avons vu que la moyenne de l’échantillon <span class="math inline">\(\bar{x}\)</span> a une moyenne de <span class="math inline">\(\mu\)</span> et un écart-type égal à <span class="math inline">\(\sigma / \sqrt{n}\)</span>.</p>
<p>Supposons en plus que <span class="math inline">\(\bar{x}\)</span> suit une distribution normale. C’est toujours le cas lorsque <span class="math inline">\(x\)</span> suit elle-même une distribution normale. Mais grâce au théorème de la limite centrale, c’est aussi une bonne approximation pour d’autres distributions de <span class="math inline">\(x\)</span>, en autant que l’échantillon soit assez grand.</p>
<p>Dans ce cas, la variable <span class="math inline">\(z\)</span> que nous définirons comme:</p>
<p><span class="math display">\[ z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \]</span></p>
<p>suit une distribution normale centrée réduite. On peut donc utiliser cette distribution théorique pour déterminer la probabilité que <span class="math inline">\(\bar{x}\)</span> se retrouve dans un intervalle donné.</p>
</div>
<div id="intervalle-de-probabilité-déterminée" class="section level2">
<h2>Intervalle de probabilité déterminée</h2>
<p>À l’opposé, on peut déterminer l’intervalle de <span class="math inline">\(\bar{x}\)</span> correspondant à une probabilité donnée autour de la moyenne.</p>
<p>Par exemple, l’intervalle entre le premier quartile (probabilité cumulative de 25%) et le troisième quartile (probabilité cumulative de 75%) correspond à une probabilité de 50%. On peut déterminer ces quantiles dans R avec <code>qnorm</code>.</p>
<pre class="r"><code>c(qnorm(0.25), qnorm(0.75))</code></pre>
<pre><code>## [1] -0.6744898  0.6744898</code></pre>
<p><em>Note</em>: Par défaut, <code>qnorm</code> utilise les paramètres <code>mean = 0</code> et <code>sd = 1</code>.</p>
<p>L’intervalle est symétrique autour de la moyenne (0) parce que la distribution normale est symétrique et qu’on a choisi des probabilités à distance égale de 50%.</p>
<p>Convertissons maintenant cet intervalle de <span class="math inline">\(z\)</span> en intervalle de <span class="math inline">\(\bar{x}\)</span>:</p>
<p><span class="math display">\[ \left( -0.674 \le \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \le 0.674 \right)\]</span></p>
<p><span class="math display">\[ \left( - 0.674 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 0.674 \frac{\sigma}{\sqrt{n}} \right)\]</span> Il y a une probabilité de 50% que la moyenne de l’échantillon <span class="math inline">\(\bar{x}\)</span> se trouve dans un intervalle de 0.674 fois l’erreur-type de part et d’autre du paramètre <span class="math inline">\(\mu\)</span>.</p>
<p>Admettons qu’on représente la valeur de <span class="math inline">\(z\)</span> correspondant à une probabilité cumulative <span class="math inline">\(p\)</span> par <span class="math inline">\(z_p\)</span>. Par exemple, <span class="math inline">\(z_{0.25}\)</span> est le premier quartile. Alors, nous ré-écrivons l’intervalle ci-dessus comme:</p>
<p><span class="math display">\[ \left( z_{0.25} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{0.75} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>Pour un intervalle de probabilité de 90%, nous remplacerions <span class="math inline">\(z_{0.25}\)</span> et <span class="math inline">\(z_{0.75}\)</span> par <span class="math inline">\(z_{0.05}\)</span> et <span class="math inline">\(z_{0.95}\)</span>. En effet, un intervalle de 90% exclut 10% de la distribution et puisqu’on souhaite un intervalle centré, on exclut 5% des deux extrémités de la distribution, tel qu’indiqué par la section coloriée en rouge ci-dessous.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Donc, de façon générale, si on représente par <span class="math inline">\(\alpha\)</span> la probabilité exclue, l’intervalle contenant (100% - <span class="math inline">\(\alpha\)</span>) de la distribution de <span class="math inline">\(\bar{x}\)</span> correspond à:</p>
<p><span class="math display">\[ \left( z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>Pour des raisons historiques, l’intervalle de 95% correspondant à <span class="math inline">\(\alpha\)</span> = 0.05 est le plus souvent utilisé:</p>
<p><span class="math display">\[ \left( z_{0.025} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le  z_{0.975} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>En remplaçant les quantiles par leur valeur, on obtient: <span class="math display">\[ \left(- 1.96 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 1.96 \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
</div>
<div id="intervalle-de-confiance-1" class="section level2">
<h2>Intervalle de confiance</h2>
<p>Pour résumer, si nous échantillonnons une variable <span class="math inline">\(x\)</span> et calculons sa moyenne <span class="math inline">\(\bar{x}\)</span>, nous pouvons dire, par exemple, que nous avons 95% de probabilité d’obtenir un estimé <span class="math inline">\(\bar{x}\)</span> qui se trouve à <span class="math inline">\(\pm\)</span> 1.96 erreurs-type du paramètre <span class="math inline">\(\mu\)</span>, qui est inconnu.</p>
<p><em>Cela suppose toujours que notre modèle est bon, c’est-à-dire que la statistique <span class="math inline">\(\bar{x}\)</span> est bien représentée par une distribution normale.</em></p>
<p>Donc, après avoir calculé <span class="math inline">\(\bar{x}\)</span> et calculé son erreur-type, nous établissons un intervalle de 1.96 erreurs-type autour de <span class="math inline">\(\bar{x}\)</span>:</p>
<p><span class="math display">\[ \left(\bar{x} - 1.96 \frac{\sigma}{\sqrt{n}}, \bar{x} + 1.96 \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>D’après notre modèle, nous pouvons affirmer que pour 95% des échantillons possibles de <span class="math inline">\(x\)</span>, l’intervalle ainsi calculé contiendra la valeur de <span class="math inline">\(\mu\)</span>. Il s’agit donc d’un <strong>intervalle de confiance</strong> à 95% pour <span class="math inline">\(\bar{x}\)</span>.</p>
</div>
<div id="interprétation-de-lintervalle-de-confiance" class="section level2">
<h2>Interprétation de l’intervalle de confiance</h2>
<ul>
<li><p>La probabilité associée à un intervalle de confiance est basée sur la variabilité de <span class="math inline">\(\bar{x}\)</span> d’un échantillon à l’autre. Elle constitue une probabilité <em>a priori</em> (avant d’avoir échantillonné).</p></li>
<li><p>Le paramètre <span class="math inline">\(\mu\)</span> est fixe. Une fois que l’estimé <span class="math inline">\(\bar{x}\)</span> est obtenu pour un échantillon donné, l’intervalle de confiance contient <span class="math inline">\(\mu\)</span> ou ne le contient pas.</p></li>
<li><p>Parce que le paramètre <span class="math inline">\(\mu\)</span> n’est pas une variable aléatoire, il n’a pas de distribution statistique. Il est donc erroné d’affirmer, après avoir calculé un intervalle de confiance pour un échantillon donné, que “le paramètre <span class="math inline">\(\mu\)</span> a 95% de probabilité d’être à l’intérieur de cet intervalle”.</p></li>
</ul>
</div>
<div id="intervalle-de-confiance-dune-moyenne" class="section level2">
<h2>Intervalle de confiance d’une moyenne</h2>
<p>Nous avons vu que l’intervalle de confiance à (100% - <span class="math inline">\(\alpha\)</span>) de la moyenne <span class="math inline">\(\bar{x}\)</span> est donné par:</p>
<p><span class="math display">\[ \left( \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + z_{1 - \alpha/2} \frac{\sigma}{\sqrt{n}} \right)\]</span></p>
<p>Le seul problème avec cette équation est que nous ne connaissons pas le paramètre <span class="math inline">\(\sigma\)</span>. De plus, si nous remplaçons <span class="math inline">\(\sigma\)</span> par son estimé <span class="math inline">\(s\)</span>, la probabilité associée à l’intervalle devient inférieure à (100% - <span class="math inline">\(\alpha\)</span>). En pratique, il faut donc élargir l’intervalle pour prendre en compte notre connaissance imparfaite de l’écart-type des données.</p>
<p>La solution de ce problème a été découverte par William Gosset, qui l’a publié sous le pseudonyme de Student. Lorsqu’on utilise un estimé de l’écart-type, l’intervalle de confiance n’est plus basé sur la distribution normale centrée réduite <span class="math inline">\(z\)</span>, mais plutôt la distribution <span class="math inline">\(t\)</span> de Student.</p>
<p>La distribution comporte un paramètre, le nombre de degrés de liberté, qui correspond dans ce cas-ci à <span class="math inline">\(n - 1\)</span>. Ainsi, la version corrigée de l’intervalle de confiance à (100% - <span class="math inline">\(\alpha\)</span>) pour <span class="math inline">\(\bar{x}\)</span> est:</p>
<p><span class="math display">\[ \left( \bar{x} + t_{(n-1)\alpha/2} \frac{s}{\sqrt{n}}, \bar{x} + t_{(n-1)1 - \alpha/2} \frac{s}{\sqrt{n}} \right)\]</span></p>
<p>où le <span class="math inline">\(n-1\)</span> entre parenthèses indique le nombre de degrés de liberté de la distribution <span class="math inline">\(t\)</span>.</p>
</div>
<div id="distribution-t" class="section level2">
<h2>Distribution t</h2>
<p>Le graphique ci-dessous compare la distribution normale centrée réduite (<span class="math inline">\(z\)</span>) avec des distributions <span class="math inline">\(t\)</span> à 4 et 9 degrés de liberté. Plus le nombre de degrés de liberté est petit, plus la distribution <span class="math inline">\(t\)</span> s’éloigne de la normale. En particulier, l’écart-type augmente et les valeurs loins de la moyenne ont une probabilité plus grande, ce qui explique que l’intervalle de confiance construit à partir de la distribution <span class="math inline">\(t\)</span> est plus large.</p>
<p><img src="3-Modeles_statistiques_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Bien que ce soit difficile à voir sur ce graphique, la distribution <span class="math inline">\(t\)</span> a aussi une forme différente. Même comparée à une distribution normale de même écart-type, la distribution <span class="math inline">\(t\)</span> a une plus grande probabilité d’obtenir des valeurs extrêmes (très loin de la moyenne).</p>
<p>Lorsque le nombre de degrés de liberté est élevé, comme c’est le cas si on calcule la moyenne de nombreuses observations, la distribution <span class="math inline">\(t\)</span> s’approche de la distribution normale centrée réduite.</p>
</div>
</div>
<div id="résumé-1" class="section level1">
<h1>Résumé</h1>
<ul>
<li><p>Un intervalle de confiance est défini autour d’un estimé de manière à ce que sur l’ensemble des échantillons possibles, il y ait une probabilité spécifique que l’intervalle de confiance obtenu contienne la valeur du paramètre à estimer.</p></li>
<li><p>La distribution <span class="math inline">\(t\)</span> de Student remplace la distribution normale centrée réduite pour estimer l’intervalle de confiance de la moyenne d’un échantillon, lorsque l’écart-type de la population est inconnu. Cette distribution a des valeurs extrêmes plus fréquentes que la distribution normale, surtout lorsque le nombre de degrés de liberté est faible.</p></li>
</ul>
</div>
<div id="référence" class="section level1">
<h1>Référence</h1>
<p>Le site <a href="https://students.brown.edu/seeing-theory/">Seeing Theory</a> présente de façon visuelle et interactive plusieurs concepts statistiques. Par exemple, les chapitres 3 (<em>Probability Distributions</em>) et 4 (<em>Frequentist Inference</em>) se rapportent aux concepts vus dans ce cours.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
