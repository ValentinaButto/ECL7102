<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="date" content="2019-09-23" />

<title>Chi-squared test and ANOVA</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Chi-squared test and ANOVA</h1>
<h4 class="date"><br/>September 23, 2019</h4>

</div>


<div id="general-plan" class="section level1">
<h1>General plan</h1>
<p>At the last class, we used the <span class="math inline">\(t\)</span>-test to determine if the mean value of a variable differed between two groups. The analysis of variance (ANOVA), which we will start discussing today, makes it possible to extend this comparison to several groups.</p>
<p>More generally, we could say that the <span class="math inline">\(t\)</span>-test and ANOVA deal with the effect of categorical predictors (e.g. different treatments) on a numerical response. Another test that we will use in this class is the chi-squared test, which aims to detect an association between two categorical variables.</p>
<p>Later this semester, we will focus on regression models. These are broader in scope because they link a response variable to categorical and numerical predictors. In particular, we will see that the <span class="math inline">\(t\)</span>-test and ANOVA are examples of linear regression models.</p>
<table>
<colgroup>
<col width="33%" />
<col width="37%" />
<col width="28%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Categorical response</th>
<th>Numerical response</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Categorical predictor</td>
<td>Chi-squared test</td>
<td><span class="math inline">\(t\)</span>-test (2 categories) or ANOVA (more than 2 categories)</td>
</tr>
<tr class="even">
<td>Categorical or numerical predictor</td>
<td>Logistic regression</td>
<td>Linear regression</td>
</tr>
</tbody>
</table>
</div>
<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li><p>Use the chi-squared test to compare the frequencies of a categorical variable to a reference distribution, or to test the association between two categorical variables in a contingency table.</p></li>
<li><p>Understand the principle of the analysis of variance and perform a one-way ANOVA.</p></li>
<li><p>Determine the significant differences between treatments using Tukey’s range test.</p></li>
</ul>
</div>
<div id="compare-the-frequencies-of-a-variable-to-a-reference-distribution" class="section level1">
<h1>Compare the frequencies of a variable to a reference distribution</h1>
<p><strong>Example</strong>: To check if a die is balanced, we compile the result of 100 throws. The following table shows the number of times each number was obtained (its <strong>frequency</strong> <span class="math inline">\(f\)</span>).</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(f_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>12</td>
</tr>
<tr class="even">
<td>2</td>
<td>17</td>
</tr>
<tr class="odd">
<td>3</td>
<td>16</td>
</tr>
<tr class="even">
<td>4</td>
<td>18</td>
</tr>
<tr class="odd">
<td>5</td>
<td>11</td>
</tr>
<tr class="even">
<td>6</td>
<td>26</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>100</strong></td>
</tr>
</tbody>
</table>
<p>Our null hypothesis is that the die is balanced, so there is an equal probability of getting each value (<span class="math inline">\(p_i\)</span> = 1/6 for <span class="math inline">\(i\)</span> from 1 to 6). If we multiply these probabilities by the total number of throws, we obtain the expected frequency (<span class="math inline">\(\hat{f_i}\)</span>) for each number.</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(f_i\)</span></th>
<th><span class="math inline">\(p_i\)</span></th>
<th><span class="math inline">\(\hat{f_i}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>12</td>
<td>1/6</td>
<td>16.7</td>
</tr>
<tr class="even">
<td>2</td>
<td>17</td>
<td>1/6</td>
<td>16.7</td>
</tr>
<tr class="odd">
<td>3</td>
<td>16</td>
<td>1/6</td>
<td>16.7</td>
</tr>
<tr class="even">
<td>4</td>
<td>18</td>
<td>1/6</td>
<td>16.7</td>
</tr>
<tr class="odd">
<td>5</td>
<td>11</td>
<td>1/6</td>
<td>16.7</td>
</tr>
<tr class="even">
<td>6</td>
<td>26</td>
<td>1/6</td>
<td>16.7</td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><strong>100</strong></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div id="pearsons-chi-squared-test" class="section level2">
<h2>Pearson’s chi-squared test</h2>
<p>For a variable with <span class="math inline">\(k\)</span> categories, the value of the chi-squared (<span class="math inline">\(\chi^2\)</span>) statistic is calculated as:</p>
<p><span class="math display">\[ \chi^2 = \sum_{i = 1}^k \frac{(f_i - \hat{f_i})^2}{\hat{f_i}} \]</span></p>
<p>The chi-squared statistic thus measures the sum of the deviations between the observed and expected frequencies (normalized by the expected value). When the <span class="math inline">\(\hat{f_i}\)</span> for each category are large enough (typically, 5 or more), this statistic roughly follows a <span class="math inline">\(\chi^2_{k-1}\)</span> distribution, where $k - 1 $is the number degrees of freedom.</p>
<p><strong>Reminder</strong>: The degrees of freedom correspond to the number of independent data used in the calculation of a statistic. Here, the <span class="math inline">\(\chi^2\)</span> is calculated from the deviations between observed and expected frequencies for <span class="math inline">\(k\)</span> categories. However, since the sum of the deviations must equal 0 (because the total of <span class="math inline">\(f_i\)</span> and <span class="math inline">\(\hat{f_i}\)</span> is the same) there are <span class="math inline">\(k - 1\)</span> independent deviations.</p>
<p><span class="math display">\[ \sum_{i=1}^k f_i = \sum_{i=1}^k \hat{f_i} \]</span>, thus <span class="math display">\[\sum_{i=1}^k (f_i - \hat{f_i}) = 0\]</span></p>
<p>Here is the distribution of <span class="math inline">\(\chi^2_{k}\)</span> for different values of <span class="math inline">\(k\)</span>:</p>
<p><img src="../images/khi_carre.png" /></p>
<p>In R, the <code>pchisq(q, df)</code> function gives the probability of getting a value less than or equal to <span class="math inline">\(q\)</span> for a chi-squared distribution with <span class="math inline">\(df\)</span> degrees of freedom. Let’s calculate this probability for our example.</p>
<pre class="r"><code># Data
x &lt;- c(12, 17, 16, 18, 11, 26)
n &lt;- sum(x) # total

# Theoretical probabilities
p &lt;- rep(1/6, 6)

# Calculate chi-squared statistic
khi2 &lt;- sum((x - n*p)^2 / (n*p))
khi2</code></pre>
<pre><code>## [1] 8.6</code></pre>
<pre class="r"><code>pchisq(khi2, df = 5)</code></pre>
<pre><code>## [1] 0.8738776</code></pre>
<p><strong>Question</strong>: What is the <span class="math inline">\(p\)</span>-value for this test? Is it a one-sided or two-sided test?</p>
<p>This is a one-sided test, since if the theoretical model was a bad fit, the sum of the deviations would be larger than expected. The <span class="math inline">\(p\)</span>-value is <code>1 - pchisq(chi2, df = 5)</code>, which is about 0.126.</p>
<p>Rather than manually calculating the statistic, we can use the <code>chisq.test</code> function.</p>
<pre class="r"><code>chisq.test(x, p = p)</code></pre>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  x
## X-squared = 8.6, df = 5, p-value = 0.1261</code></pre>
</div>
</div>
<div id="test-of-association-between-two-categorical-variables" class="section level1">
<h1>Test of association between two categorical variables</h1>
<div id="contingency-table" class="section level2">
<h2>Contingency table</h2>
<p>Often, we do not have a reference distribution for a categorical variable, but we want to check if its distribution depends on the value of another categorical variable, in other words, if the two variables are <em>associated</em>.</p>
<p>For example, suppose the number of dead and live trees of three coniferous species (ABBA: balsam fir, PIGL: white spruce, PIMA: black spruce) were counted in a plot following a spruce budworm epidemic.</p>
<pre class="r"><code># rbind creates a matrix by binding vectors as rows
survie &lt;- rbind(c(29, 11, 12), c(31, 29, 38)) 
rownames(survie) &lt;- c(&quot;dead&quot;, &quot;alive&quot;)
colnames(survie) &lt;- c(&quot;ABBA&quot;, &quot;PIGL&quot;, &quot;PIMA&quot;)
survie</code></pre>
<pre><code>##       ABBA PIGL PIMA
## dead    29   11   12
## alive   31   29   38</code></pre>
<p>This type of matrix is called a <strong>contingency table</strong>.</p>
<p>In our example, the two variables (survival and species) are associated if the mortality rate depends on the species. The null hypothesis represents the absence of association, that is, survival is <em>independent</em> of the species.</p>
</div>
<div id="chi-squared-test-for-two-variables" class="section level2">
<h2>Chi-squared test for two variables</h2>
<p>As in the previous section, we will calculate the <span class="math inline">\(\chi^2\)</span> from the deviations between the observed (<span class="math inline">\(f_ {ij}\)</span>) and expected (<span class="math inline">\(\hat{f_{ij}}\)</span>) frequencies.</p>
<p><span class="math display">\[ \chi^2 = \sum_{i = 1}^r \sum_{j = 1}^c \frac{(f_{ij} - \hat{f_{ij}})^2}{\hat{f_{ij}}} \]</span></p>
<p>Here, <span class="math inline">\(r\)</span> and <span class="math inline">\(c\)</span> refer to the number of rows and columns in the table, respectively.</p>
<p>How to determine the expected frequencies <span class="math inline">\(\hat{f_{ij}}\)</span>? Let’s first calculate the totals in each row and column, as well as the grand total.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>ABBA</th>
<th>PIGL</th>
<th>PIMA</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dead</td>
<td>29</td>
<td>11</td>
<td>12</td>
<td>52</td>
</tr>
<tr class="even">
<td>alive</td>
<td>31</td>
<td>29</td>
<td>38</td>
<td>98</td>
</tr>
<tr class="odd">
<td>Total</td>
<td>60</td>
<td>40</td>
<td>50</td>
<td>150</td>
</tr>
</tbody>
</table>
<p>Let <span class="math inline">\(N_i\)</span> be the total of the row <span class="math inline">\(i\)</span>, <span class="math inline">\(N_j\)</span> the total of the column <span class="math inline">\(j\)</span> and <span class="math inline">\(N\)</span> be the grand total. We estimate the probability of each category by the proportion of the grand total included in this category: <span class="math inline">\(\hat{p_i} = N_i / N\)</span> and <span class="math inline">\(\hat{p_j} = N_j / N\)</span>.</p>
<p>The joint probability of two independent variables is the product of the probabilities of the variables taken separately, e.g.: (Prob. that the tree is a live fir) = (Prob. that the tree is a fir) x (Prob. that the tree is alive). Thus, the expected frequencies according to the null hypothesis are calculated as follows.</p>
<p><span class="math display">\[ \hat{f_{ij}} = N p_i p_j = \frac{N_i N_j}{N}\]</span></p>
<p>If the null hypothesis is correct, then the <span class="math inline">\(\chi^2\)</span> statistic follows a distribution with <span class="math inline">\((r - 1) \times (c - 1)\)</span> degrees of freedom. In our example, <span class="math inline">\(df = 2\)</span>. Indeed, since the expected frequencies are based on the totals of each row and each column, the sum of the deviations in each row and each column must be zero.</p>
<p>If we choose a threshold <span class="math inline">\(\alpha = 0.05\)</span> and then apply the <code>chisq.test</code> function to that matrix, we get a <span class="math inline">\(p\)</span>-value of 0.01, meaning there is a significant association between the two variables.</p>
<pre class="r"><code>chisq.test(survie)</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  survie
## X-squared = 8.3669, df = 2, p-value = 0.01525</code></pre>
<p>To specify the association, we can assign the result of the test to a variable <code>khi2</code> and inspect the expected frequencies (<code>khi2$expected</code>) and the residuals (<code>khi2$residuals</code>).</p>
<pre class="r"><code>khi2 &lt;- chisq.test(survie)
khi2$expected</code></pre>
<pre><code>##       ABBA     PIGL     PIMA
## dead  20.8 13.86667 17.33333
## alive 39.2 26.13333 32.66667</code></pre>
<pre class="r"><code>khi2$residuals</code></pre>
<pre><code>##            ABBA       PIGL       PIMA
## dead   1.797969 -0.7698235 -1.2810252
## alive -1.309697  0.5607636  0.9331389</code></pre>
<p>The residuals are the standardized deviations:</p>
<p><span class="math display">\[ \frac{f_{ij} - \hat{f_{ij}}}{\sqrt{\hat{f_{ij}}}} \]</span></p>
<p>The sum of the squared values of these deviations are equal to <span class="math inline">\(\chi^2\)</span>.</p>
<p>How can we interpret this matrix of residuals? Since there is an excess of dead fir (positive residual), the fir mortality rate is higher than predicted by the null hypothesis, whereas it is lower than expected for both spruces.</p>
<p>However, the rejection of the null hypothesis (independence between mortality and species) in the table does not tell us between which species the mortality rate varies significantly. Later this semester, we will see how a logistic regression estimates the probability of a binary result (e.g., survival) based on a categorical or continuous predictor.</p>
</div>
<div id="notes-on-using-the-chi-squared-test" class="section level2">
<h2>Notes on using the chi-squared test</h2>
<ul>
<li><p>The chi-squared test must always be done on the frequencies (number of observations), not on the proportions. Without knowing the size of the sample, the proportions themselves do not tell us if a deviation is significant. For example, if two categories should be in equal proportions (50% / 50%), frequencies of 60 and 40 may be a significant deviation, but not frequencies of 6 and 4.</p></li>
<li><p>Since the chi-squared test approximates discrete data by a continuous distribution, it becomes less accurate as the sample size decreases. The test is therefore not recommended if one of the expected frequencies (<span class="math inline">\(\hat{f_ {ij}}\)</span>) is less than 5. In this case, we can use Fisher’s exact test (<code>fisher.test</code> function in R), which calculates the exact probabilities of different contingency tables, assuming that the row and column totals are fixed.</p></li>
</ul>
<pre class="r"><code>tab &lt;- matrix(c(4, 6, 8, 2), nrow = 2)
tab</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    4    8
## [2,]    6    2</code></pre>
<pre class="r"><code>chisq.test(tab)</code></pre>
<pre><code>## Warning in chisq.test(tab): Chi-squared approximation may be incorrect</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  tab
## X-squared = 1.875, df = 1, p-value = 0.1709</code></pre>
<pre class="r"><code>fisher.test(tab)</code></pre>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  tab
## p-value = 0.1698
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.01252647 1.65925396
## sample estimates:
## odds ratio 
##  0.1841181</code></pre>
</div>
</div>
<div id="analysis-of-variance-anova" class="section level1">
<h1>Analysis of variance (ANOVA)</h1>
<p>Suppose we want to compare the mean of a variable between several (&gt;2) groups. We could compare the groups in pairs with <span class="math inline">\(t\)</span>-tests (for example, A-B, B-C, and A-C for three groups), but as we saw in the previous class, performing multiple tests increases the probability of making a type I error.</p>
<p>For a sample divided into several groups, the analysis of variance (ANOVA) compares the variation between observations within each group to the variation between the groups. It thus makes it possible to globally test the null hypothesis according to which observations in each group come from populations with the same mean.</p>
<p>As an example, let’s take the first 10 observations of each species in the <code>iris</code> data set included with R. Here is the distribution of the sepal widths for this sample.</p>
<pre class="r"><code>iris_ech &lt;- iris[c(1:10, 51:60, 101:110), ]
ggplot(iris_ech, aes(x = Species, y = Sepal.Width)) +
    geom_boxplot()</code></pre>
<p><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>This graph shows the variation of a numeric variable based on a categorical variable (or factor) that has three categories. A <em>one-way</em> ANOVA compares groups along one categorical variable.</p>
<div id="one-way-anova-model" class="section level2">
<h2>One-Way ANOVA Model</h2>
<p>Suppose we measure the variable <span class="math inline">\(y\)</span> for <span class="math inline">\(l\)</span> groups each including <span class="math inline">\(n\)</span> observations. The difference between an observation <span class="math inline">\(k\)</span> of the group <span class="math inline">\(i\)</span> (<span class="math inline">\(y_ {ik}\)</span>) and the (theoretical) mean of that group (<span class="math inline">\(\mu_i\)</span>) is the residual <span class="math inline">\(\epsilon_{ik}\)</span>.</p>
<p><span class="math display">\[ y_{ik} = \mu_i + \epsilon_{ik} \]</span></p>
<p>In the ANOVA model, we assume that residuals follow a normal distribution with mean 0 and a fixed standard deviation.</p>
<p><span class="math display">\[ \epsilon_{ik} \sim N(0, \sigma) \]</span></p>
<p>The null hypothesis is that <span class="math inline">\(\mu_i\)</span> is the same for all groups. We can also represent the same model based on the grand mean, <span class="math inline">\(\mu\)</span>, and the deviation <span class="math inline">\(\alpha_i\)</span> between the mean of group <span class="math inline">\(i\)</span> and <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display">\[ y_{ik} = \mu + \alpha_i + \epsilon_{ik} \]</span></p>
</div>
<div id="sum-of-squared-deviations" class="section level2">
<h2>Sum of squared deviations</h2>
<p>If <span class="math inline">\(\bar{y}\)</span> is the grand mean of the observed <span class="math inline">\(y\)</span> and <span class="math inline">\(\bar{y_i}\)</span> is the mean of observations in group <span class="math inline">\(i\)</span>, then we can prove the following relatinship between sums of squared deviations.</p>
<p><span class="math display">\[ \sum_{i = 1}^l \sum_{k = i}^n (y_{ik} - \bar{y})^2 = \sum_{i = 1}^l \sum_{k = i}^n (y_{ik} - \bar{y_i})^2 + \sum_{i = 1}^l \sum_{k = i}^n (\bar{y_i} - \bar{y})^2 \]</span></p>
<p>Since the last term doesn’t depend on <span class="math inline">\(k\)</span>, we can re-write the equation as:</p>
<p><span class="math display">\[ \sum_{i = 1}^l \sum_{k = i}^n (y_{ik} - \bar{y})^2 = \sum_{i = 1}^l \sum_{k = i}^n (y_{ik} - \bar{y_i})^2 + \sum_{i = 1}^l n (\bar{y_i} - \bar{y})^2 \]</span></p>
<p>The term on the left is the total sum of squares (<em>SST</em>), the first term on the right is error (residual) sum of squares (<em>SSE</em>) and the second term on the right is the sum of squares between groups of factor A (<em>SSA</em>, the only factor in this case).</p>
<p>We thus obtain the equation <em>SST = SSE + SSA</em>, which decomposes the total sum of squares into two components: one due to the differences observed within each group (SSE) and the other due to the differences observed between groups (SSA).</p>
</div>
<div id="analysis-of-variance-table" class="section level2">
<h2>Analysis of variance table</h2>
<p>From the sums of squared deviations seen above, the mean squared deviations can be calculated by dividing each sum by the appropriate number of degrees of freedom.</p>
<ul>
<li><p>For the deviations from the grand mean, there are <span class="math inline">\(nl - 1\)</span> degrees of freedom; since the sum of the deviations is zero, the last one is not independent from the others.</p></li>
<li><p>For the residuals, there are <span class="math inline">\((n-1)l\)</span> degrees of freedom, since there are <span class="math inline">\(n - 1\)</span> independent deviations per group.</p></li>
<li><p>For the differences between the group means and the grand mean, there is <span class="math inline">\(l - 1\)</span> degrees of freedom.</p></li>
</ul>
<p>Notice that the sum of the degrees of freedom is the same on both sides of the equation: <span class="math inline">\(nl - 1 = (n-1) l + (l - 1)\)</span>.</p>
<p>The sum of squared deviations, degrees of freedom and mean squared deviations can be presented in an ANOVA table.</p>
<table style="width:100%;">
<colgroup>
<col width="5%" />
<col width="31%" />
<col width="31%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Sum of squares (SS)</th>
<th>Degrees of freedom (df)</th>
<th>Mean square (MS)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Factor A</td>
<td><span class="math inline">\(SSA = \sum_{i = 1}^l n (\bar{y_i} - \bar{y})^2\)</span></td>
<td><span class="math inline">\(l - 1\)</span></td>
<td><span class="math inline">\(MSA = \frac{SSA}{l - 1}\)</span></td>
</tr>
<tr class="even">
<td>Residual</td>
<td><span class="math inline">\(SSE = \sum_{i = 1}^l \sum_{k = i}^n (y_{ik} - \bar{y_i})^2\)</span></td>
<td><span class="math inline">\((n-1)l\)</span></td>
<td><span class="math inline">\(MSE = \frac{SSE}{(n-1)l}\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(SST = \sum_{i = 1}^l \sum_{k = i}^n (y_{ik} - \bar{y})^2\)</span></td>
<td><span class="math inline">\(nl - 1\)</span></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="null-hypothesis-test" class="section level2">
<h2>Null hypothesis test</h2>
<p>Recall the model for a one-way ANOVA:</p>
<p><span class="math display">\[ y_{ik} = \mu + \alpha_i + \epsilon_{ik} \]</span></p>
<p><span class="math display">\[ \epsilon_{ik} \sim N(0, \sigma) \]</span></p>
<p>The mean of squared residuals (<em>MSE</em>) is an estimator of the variance in that model (<span class="math inline">\(\sigma^2\)</span>).</p>
<p>If the null hypothesis is correct and there are no systematic differences between groups (that is, all <span class="math inline">\(\alpha_i\)</span> are equal to 0 in the model), then the mean of squared group differences (<em>MSA</em>) is also an estimator of <span class="math inline">\(\sigma^2\)</span>. Indeed, according to the null hypothesis, the different groups are independent samples of the same population. In this case, the term <em>MSA</em> corresponds to the variance of <span class="math inline">\(\bar{y_i}\)</span> multiplied by <span class="math inline">\(n\)</span> (the number of observations per group). The variance of a mean of <span class="math inline">\(n\)</span> observations is precisely equal to <span class="math inline">\(\sigma^2 / n\)</span>, where <span class="math inline">\(\sigma^2\)</span> is the variance of the individual observations.</p>
<p>If <em>MSA</em> and <em>MSE</em> are two estimators of the same variance under the null hypothesis, then their ratio <span class="math inline">\(F = MSA / MSE\)</span> follows the <span class="math inline">\(F\)</span> distribution. This distribution has two parameters (<span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span>) corresponding to the degrees of freedom of <em>MSA</em> and <em>MSE</em>.</p>
<p><img src="../images/distr_f.png" /></p>
<p>Conversely, if the null hypothesis is false, the value of <em>MSA</em> is expected to be higher than <em>MSE</em>, since systematic differences between groups will be added to the random variations in sampling. It is therefore a one-sided test: the <span class="math inline">\(p\)</span>-value is the probability of a <span class="math inline">\(F\)</span> ratio equal to or greater than the one observed in the data.</p>
</div>
<div id="assumptions-of-the-anova-model" class="section level2">
<h2>Assumptions of the ANOVA model</h2>
<p>For the model on which the ANOVA is based to be valid, the residuals must be (1) independent between observations and follow (2) a normal distribution with (3) the same variance <span class="math inline">\(\sigma^2\)</span> in each group.</p>
<ul>
<li><p>The independence of the residuals requires, among other things, that the unmeasured factors that influence the response are distributed in a similar way for each group. For an experimental design, the random assignment of treatments helps to ensure this independence.</p></li>
<li><p>Like the <span class="math inline">\(t\)</span>-test, the ANOVA is robust to weak to moderate deviations from the normal distribution.</p></li>
<li><p>Unlike the <span class="math inline">\(t\)</span>-test, the equality of the group variances (<strong>homoscedasticity</strong>) is essential for the ANOVA. If this assumption is not met, we must transform the data or resort to a more complex model, as we will see later.</p></li>
</ul>
<p>The calculations presented above apply only to a <strong>balanced</strong> sample, that is, where the number of observations is the same in each group. For today’s class, we will limit ourselves to balanced samples. In order to perform an unbalanced ANOVA in R, we must use linear regression methods that we will see in future courses.</p>
</div>
</div>
<div id="examples-of-one-way-anova" class="section level1">
<h1>Examples of one-way ANOVA</h1>
<p>With the <code>aov</code> function in R, we perform an ANOVA of the sepal width by species of iris, from the sample of the<code>iris</code> data frame chosen at the beginning of this section.</p>
<pre class="r"><code>anova1 &lt;- aov(Sepal.Width ~ Species, data = iris_ech) 
summary(anova1)</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)  
## Species      2  1.118  0.5590   5.179 0.0125 *
## Residuals   27  2.914  0.1079                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <code>aov</code> function requires a description of the model in the<code>response ~ predictor</code> form, as well as the name of the data frame containing these variables (<code>data</code> argument). By applying the <code>summary</code> function to the result of an ANOVA, we obtain the ANOVA table as presented above, in addition to the value of the <span class="math inline">\(F\)</span> statistic and its <span class="math inline">\(p\)</span>-value (<code>Pr(&gt;F)</code>). With a threshold of 0.05, the null hypothesis that species have the same mean sepal width would be rejected.</p>
<p>To verify that our data conforms to the assumptions of the model, we must consult the diagnostic plots, obtained by applying the <code>plot</code> function to the result.</p>
<pre class="r"><code>plot(anova1)</code></pre>
<p><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-9-1.png" width="672" /><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-9-2.png" width="672" /><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-9-3.png" width="672" /><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-9-4.png" width="672" /></p>
<p>The first two graphs are the most important here. The <code>Residuals vs. Fitted</code> plot shows the value of the residuals according to the estimated mean for each group; if the model is valid, there should be no visible trend and the residuals of the different groups should have a similar variance around zero, which is the case here. The second graph is a quantile-quantile plot to check if the residuals approximately follow a normal distribution.</p>
<p>To display the group means, we must call the <code>coef</code> function (for coefficients).</p>
<pre class="r"><code>coef(anova1)</code></pre>
<pre><code>##       (Intercept) Speciesversicolor  Speciesvirginica 
##              3.31             -0.44             -0.37</code></pre>
<p>By default, the <code>(Intercept)</code> is the mean of the first group (here, the species <em>setosa</em>), while the other coefficients indicate the difference between the other groups and that first mean. The mean for species <em>versicolor</em> is 3.31 - 0.44 = 2.87 while the mean for species <em>virginica</em> is 3.31 - 0.37 = 2.94. The result does not tell us which of these differences are significant.</p>
<p>If there are only two groups, the ANOVA is equivalent to a <span class="math inline">\(t\)</span>-test between two samples with equal variance.</p>
<pre class="r"><code>library(dplyr)
iris_2sp &lt;- filter(iris_ech, Species != &quot;setosa&quot;)
anova2 &lt;- aov(Sepal.Width ~ Species, data = iris_2sp)
summary(anova2)</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)
## Species      1 0.0245  0.0245   0.214   0.65
## Residuals   18 2.0650  0.1147</code></pre>
<pre class="r"><code>coef(anova2)</code></pre>
<pre><code>##      (Intercept) Speciesvirginica 
##             2.87             0.07</code></pre>
<pre class="r"><code>t.test(Sepal.Width ~ Species, data = iris_2sp, var.equal = TRUE)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  Sepal.Width by Species
## t = -0.46212, df = 18, p-value = 0.6495
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.3882356  0.2482356
## sample estimates:
## mean in group versicolor  mean in group virginica 
##                     2.87                     2.94</code></pre>
<p>The <span class="math inline">\(p\)</span>-value and the estimated mean by species are the same for both tests.</p>
<p>Now, let’s consider the <code>InsectSprays</code> data frame that describes the number of insects (<code>count</code>) on plots treated with different insecticides (<code>spray</code>).</p>
<pre class="r"><code>ggplot(InsectSprays, aes(x = spray, y = count)) + 
    geom_boxplot()</code></pre>
<p><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Before looking at the ANOVA results, we inspect the first two diagnostic plots.</p>
<pre class="r"><code>anova3 &lt;- aov(count ~ spray, InsectSprays)
plot(anova3, which = 1:2)</code></pre>
<p><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-13-1.png" width="672" /><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-13-2.png" width="672" /></p>
<p>The graph of residuals vs. fitted values shows that the variance of the residuals is higher for the groups with higher means, which contradicts the assumption of equal variance. When this happens, the distribution of residuals also departs from normal, as can be seen in the quantile-quantile plot.</p>
<div id="square-root-transformation" class="section level2">
<h2>Square root transformation</h2>
<p>An increase of the variance for groups with a higher mean occurs frequently when dealing with count data. For this type of data, taking the square root (<code>sqrt</code>) of the original variable can make the variances more homogeneous. Note that a logarithmic transformation is not possible when the data includes 0s.</p>
<pre class="r"><code>anova3_racine &lt;- aov(sqrt(count) ~ spray, InsectSprays)
plot(anova3_racine, which = 1:2)</code></pre>
<p><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-14-1.png" width="672" /><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-14-2.png" width="672" /></p>
<pre class="r"><code>summary(anova3_racine)</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)    
## spray        5  88.44  17.688    44.8 &lt;2e-16 ***
## Residuals   66  26.06   0.395                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Instead of transforming the response variable (the number of insects), another option would be to choose a model that is not based on a normal distribution of the residuals. This is the approach taken by generalized linear models, which we will study later in the semester.</p>
</div>
</div>
<div id="multiple-comparisons-between-groups" class="section level1">
<h1>Multiple comparisons between groups</h1>
<p>As we saw above, the rejection of the null hypothesis in ANOVA indicates that it is unlikely that all groups would have the same mean, but does not tell us which groups have significantly different means.</p>
<p>Tukey’s range test is designed for this scenario. The test compares the groups in pairs with a statistic based on the <span class="math inline">\(t\)</span> distribution, but which takes into account the multiple comparisons to ensure that the overall probability of Type I error remains below the desired threshold (usually 5%).</p>
<p>This test is implemented in R by the <code>TukeyHSD</code> function (for Honest Significant Difference). For example, we apply the Tukey test to the result of our first ANOVA, on the variation of the sepal width by species of iris.</p>
<pre class="r"><code>anova1 &lt;- aov(Sepal.Width ~ Species, data = iris_ech) 
tukey1 &lt;- TukeyHSD(anova1)
tukey1</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = Sepal.Width ~ Species, data = iris_ech)
## 
## $Species
##                       diff        lwr          upr     p adj
## versicolor-setosa    -0.44 -0.8042735 -0.075726495 0.0155401
## virginica-setosa     -0.37 -0.7342735 -0.005726495 0.0459626
## virginica-versicolor  0.07 -0.2942735  0.434273505 0.8829240</code></pre>
<p>There seems to be a significant difference between <em>setosa</em> and the two other species.</p>
<p>The <code>plot</code> function produces a graph of the confidence intervals for Tukey’s range test.</p>
<pre class="r"><code>plot(tukey1)</code></pre>
<p><img src="5E-Chi2_ANOVA_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Since this method targets a 5% probability of Type I error across all comparisons, each of the individual comparisons must have a <span class="math inline">\(\alpha\)</span> &lt; 0.05. In this case, the probability of Type II error (not detecting a significant difference) increases. We can therefore control the type I error rate in a multiple comparison problem, but at the cost of decreasing the power of the test when the number of comparisons to be made increases. Since ANOVA has greater power than multiple comparisons, it is even possible that none of the differences between two groups are significant, even if the ANOVA null hypothesis was rejected.</p>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<div id="chi-squared-test" class="section level3">
<h3>Chi-squared test</h3>
<ul>
<li><p>A contingency table is a cross-tabulation of the number of observations (frequencies) according to two categorical variables.</p></li>
<li><p>The chi-squared test is used to compare the frequencies of a categorical variable to a reference distribution, or to check the independence of two categorical variables in a contingency table.</p></li>
<li><p>When the expected frequencies are very low (&lt;5), the approximation the of chi-squared test must be replaced by a test counting the exact probabilities of the contingency tables, like Fisher’s exact test.</p></li>
</ul>
</div>
<div id="anova" class="section level3">
<h3>ANOVA</h3>
<ul>
<li><p>The analysis of variance aims to determine if samples from different groups (e.g. treatments) are distributed with the same mean.</p></li>
<li><p>It assumes that the residuals in each group are independent and follow a normal distribution with the same variance.</p></li>
<li><p>If the ANOVA null hypothesis is rejected, Tukey’s range test can be used to identify significant differences between groups.</p></li>
</ul>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
