---
title: "Modèles statistiques, paramètres et estimateurs"
date: "<br/>4 septembre 2018"
output:
    html_document:
        theme: spacelab
        toc: true
        toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cowplot)
```

# Objectifs

Distributions statistiques

- Décrire les caractéristiques et l'utilité des distributions normale et log-normale
- Connaître la relation entre densité de probabilité et probabilité cumulative pour une variable continue, et calculer ces quantités dans R.
- Comparer des données à une distribution de référence avec un diagramme quantile-quantile.

Estimation de paramètres

- Estimer la moyenne et la variance d'une population à partir d'un échantillon.
- Définir le biais et l'erreur-type d'un estimateur.
- Calculer les propriétés d'un estimateur en simulant l'échantillonnage.
- Interpréter un intervalle de confiance et calculer l'intervalle de confiance pour la moyenne d'une distribution normale.

# Statistiques, paramètres et estimateurs

Au dernier cours, nous avons vu une série de statistiques descriptives: moyenne, variances, quantiles et autres. De façon générale, une *statistique* est une quantité calculée à partir d'observations de variables aléatoires. 

Dans ce cours-ci, nous considérerons les observations comme le résultat d'un processus aléatoire décrit par un modèle statistique comprenant certains *paramètres*. Notre but principal sera de déterminer dans quelle mesure une statistique calculée à partir des observations constitue un bon *estimateur* du paramètre recherché.

Par exemple, si on mesure le poids de écureuils roux et qu'on fait la moyenne de ces mesures (une statistique), quel est notre estimé du poids moyen de la population locale d'écureuils roux (un paramètre)? Quelle est sa marge d'erreur? 

> En général, un paramètre est une quantité théorique. Dans notre exemple, même si on pourrait imaginer recenser tous les écureuils, le poids des individus varie constamment et la composition de la population aussi (en raison des naissances, décès et migrations).

À la fin du cours, nous pourrons décrire ce qu'est le *biais*, la *variance* et l'*intervalle de confiance* d'un estimateur. Avant d'y arriver, nous allons d'abord revoir les concepts mathématiques de base permettant de décrire des modèles statistiques et un modèle particulièrement important, soit la distribution normale.


# Distributions statistiques

Une distribution statistique (aussi appelée loi de probabilité) est une fonction qui associe une probabilité à chaque valeur possible d'une variable aléatoire.

## Distribution discrète

Lorsque la variable est discrète, chaque valeur a une masse de probabilité, dont la somme doit être égale à 1. Par exemple, si $x$ correspond au nombre obtenu en lançant un dé équilibré à 6 faces, la probabilité de $x$ est de 1/6 pour chacun des nombres de 1 à 6. Puisque la probabilité est identique pour chaque valeur, on a ici une **distribution uniforme**.

```{r echo = FALSE}
ggplot(NULL, aes(x = 1:6, y = 1/6)) +
    labs(x = "x", y = "Probabilité de x") +
    geom_bar(stat = "identity") +
    scale_x_continuous(breaks = 1:6) +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 0.2))
```

## Distribution continue

Lorsque la variable est continue, le nombre de valeurs possible est infini, donc la probabilité d'obtenir précisément une valeur donnée est de zéro. La fonction de distribution associe donc une *densité* de probabilité à une valeur donnée.

Par exemple, voici une distribution de probabilité uniforme entre 0 et 6. La densité de probabilité est constante (1/6) dans l'intervalle et zéro à l'extérieur.

```{r echo = FALSE}
ggplot(NULL, aes(x = c(-1E-6, 1E-6, 6 - 1E-6, 6 + 1E-6), y = c(0, 1/6, 1/6, 0))) +
    labs(x = "x", y = "Densité de probabilité de x") +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 0.2)) +
    geom_line(color = "blue")
```

Pour une distribution continue, la probabilité que la variable se trouve dans un intervalle donné correspond à l'intégrale (l'aire sous la courbe) de la densité de probabilité dans cet intervalle. 

Ici, la densité de probabilité est rectangulaire, donc il est facile de calculer la probabilité d'un intervalle. Par exemple, la probabilité d'obtenir une valeur entre 2.5 et 3 est égale à 1/2 (largeur de l'intervalle) x 1/6 (densité de probabilité) = 1/12 (~0.083). Cette valeur correspond à l'aire du rectangle rempli dans le graphique ci-dessous.

```{r echo = FALSE}
ggplot(NULL, aes(x = c(-1E-6, 1E-6, 6 - 1E-6, 6 + 1E-6), y = c(0, 1/6, 1/6, 0))) +
    labs(x = "x", y = "Densité de probabilité de x") +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 0.2)) +
    geom_line(color = "blue") +
    geom_rect(xmin = 2.5, xmax = 3, ymin = 0, ymax = 1/6, fill = "blue")
```

L'intégrale de la densité de probabilité sur l'ensemble des valeurs de $x$ (probabilité totale) doit être égale à 1.


# La distribution normale

## Motivation

En R, la commande suivante génère une valeur (`n = 1`) tirée de la distribution uniforme entre 0 et 6.
```{r}
x <- runif(n = 1, min = 0, max = 6)
x
```

Une quantité calculée à partir de variables aléatoires est elle-même une variable aléatoire. Supposons donc qu'on s'intéresse à la somme de plusieurs tirages à patir de cette distribution uniforme. Nous créons donc une fonction qui génère $n$ valeurs de la distribution et calcule leur somme. Nous générons ensuite 10 000 valeurs de cette somme (avec `replicate`) pour une valeur de $n$ donnée.

```{r}
# Somme de n variables aléatoires uniformes entre min et max
somme_unif <- function(n, min, max) {
    sum(runif(n, min, max))
}

n <- 10
x <- replicate(10000, somme_unif(n, 0, 6))
```

Voici l'histogramme des valeurs obtenues pour plusieurs valeurs de $n$. Qu'est-ce que vous remarquez?

```{r echo = FALSE, message = FALSE}
df <- data.frame(n = c(2, 5, 10, 20)) %>%
    mutate(x = map(n, ~ replicate(10000, somme_unif(., 0, 6)))) %>%
    unnest()
ggplot(df, aes(x = x)) +
    geom_histogram(color = "white") + 
    facet_wrap(~n, scales = "free", labeller = label_both) +
    theme(strip.background = element_blank(), strip.text = element_text(face = "bold"))
```

La somme de 2 valeurs a une distribution plutôt triangulaire, mais à partir de $n$ = 5, on voit apparaître la forme de cloche d'une distribution normale.

## Loi des grands nombres

Dans l'exemple, nous avons utilisé une **simulation** pour illustrer la distribution de variables aléatoires obtenues en faisant la somme de 2, 5, 10 et 20 valeurs d'une variable aléatoire uniforme entre 0 et 6. Chacune de ces quatre variables (somme de 2, 5, 10 et 20) a une certaine distribution théorique, que nous ne connaissons pas. Toutefois, en simulant le processus qui génère cette variable un grand nombre de fois (ici, 10 000), on obtient un échantillon virtuel dont la distribution empirique (histogramme) s'approche bien de la distribution théorique.

Ainsi, plus la taille d'un l'échantillon aléatoire est grand, plus la distribution des valeurs de cet échantillon s'approche de la distribution de la variable dans la population. C'est ce qu'on appelle la **loi des grands nombres**.

La loi des grands nombres explique pourquoi notre échantillon simulé offre une bonne approximation de la distribution théorique de chaque somme. Le fait que cette distribution théorique s'approche d'une distribution normale lorsque le nombre de variables dans la somme augmente relève d'une autre loi importante en statistiques, le théorème de la limite centrale.

## Théorème de la limite centrale

Le théorème de la limite centrale indique que lorsqu'on additionne un grand nombre de variables aléatoires indépendantes, peu importe la distribution des variables prises individuellement, la distribution de leur somme s'approche d'une distribution normale. 

*Pour être strict, il faudrait inclure quelques conditions techniques au sujet des variables qu'on additionne, mais la définition simplifiée ci-dessus suffit pour ce cours.*

Cette propriété de la distribution normale explique en partie pourquoi elle constitue un modèle si important en statistiques. Comme nous l'avons discuté dans l'introduction du cours, une distribution statistique sert souvent à représenter la variation inexpliquée d'une variable due à un grand nombre de processus non observés dans un système complexe. Si on suppose que cette variation est due à plusieurs petits effets qui sont indépendants et additifs, alors il est naturel que le résultat s'approche d'une distribution normale. Toutefois, il est important de vérifier cette supposition pour une variable donnée.

## Distribution normale

Si une variable $x$ suit une distribution normale (aussi appelée gaussienne), sa densité de probabilité est donnée par:

$$f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2}$$

Cette distribution a deux *paramètres*, $\mu$ (qui correspond à la moyenne de $x$) et $\sigma$ (qui correspond à son écart-type). 

Sur un graphique de $f(x)$, $\mu$ correspond à la position du centre de la distribution, tandis que $\sigma$ correspond à sa dispersion; plus $\sigma$ est élevé, plus la distribution s'étend et moins elle est concentrée près de la moyenne.

```{r echo = FALSE, out.width = "100%"}
df <- data.frame(mu = c(-1, 0, 2), sigma = c(0.5, 1, 3)) %>%
    expand(mu, sigma) %>%
    mutate(x = map2(mu, sigma, ~ rnorm(10000, .x, .y))) %>%
    unnest()

p1 <- ggplot(filter(df, mu == 0), aes(x = x, color = as.factor(sigma))) +
    labs(y = "f(x)", title = expression(mu == 0), color = expression(sigma)) +
    geom_density(size = 1) +
    scale_y_continuous(expand = c(0, 0)) +
    scale_color_brewer(palette = "Dark2") +
    theme(legend.position = c(0.7, 0.5))

p2 <- ggplot(filter(df, sigma == 1), aes(x = x, color = as.factor(mu))) +
    labs(y = "f(x)", title = expression(sigma == 1), color = expression(mu)) +
    geom_density(size = 1) +
    scale_y_continuous(expand = c(0, 0)) +
    scale_color_brewer(palette = "Dark2")

plot_grid(p1, p2, nrow = 1)
```

## Distribution normale centrée réduite

Si une variable $x$ suit une distribution normale de moyenne $\mu$ et d'écart-type $\sigma$, on peut obtenir une version centrée réduite de $x$ (notée $z$) en soustrayant $\mu$, puis en divisant par $\sigma$:

$$z = \frac{x - \mu}{\sigma}$$

> Dans R, la fonction `scale(x)` appliquée à un vecteur `x` produit une version centrée réduite (obtenue en soustrayant la moyenne de `x` et en divisant par l'écart-type). 

La variable $z$ suit alors une distribution normale centrée réduite, c'est-à-dire $\mu$ = 0 et $\sigma$ = 1:

$$f(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} z^2}$$

Autrement dit, n'importe quelle distribution normale peut être obtenue à partir de $f(z)$ en déplaçant le centre d'une distance $\mu$ et en "étirant" la distribution d'un facteur $\sigma$.

Les valeurs de $z$ correspondent à la distance de la moyenne, exprimée en unités d'écart-type, ex.: $z$ = -1.5 signifie un écart-type et demi en-dessous de la moyenne.


## Distribution cumulative

Nous avons vu précédemment que la probabilité qu'une variable aléatoire continue se retrouve dans un certain intervalle correspond à l'aire sous la courbe (l'intégrale) de la densité de probabilité dans cet intervalle. 

La distribution cumulative d'une variable aléatoire (aussi appelée fonction de répartition) correspond pour chaque valeur $x$ à la probabilité que la valeur de la variable soit inférieure ou égale à $x$. Elle est donc égale à l'aire sous la courbe de la densité de probabilité à gauche de $x$. 

Voici une illustration de la distribution cumulative $F(z)$ d'une variable normale centrée réduite $z$.

```{r echo = FALSE}

plot_c <- function(q) {
    ggplot(NULL, aes(x = c(-3, 3))) +
    labs(x = "z", y = "F(z)") +
    stat_function(fun = pnorm) +
    geom_point(aes(x = q, y = pnorm(q)), color = "blue", size = 2) +
    scale_y_continuous(expand = c(0, 0))
} 

plot_d <- function(q) {
    ggplot(NULL, aes(x = c(-3, 3))) +
    labs(x = "z", y = "f(z)") +
    stat_function(fun = dnorm) +
    stat_function(fun = function(x) ifelse(x <= q, dnorm(x), NA), geom = "area", fill = "blue") +
    scale_y_continuous(expand = c(0, 0))
}

plot_grid(plot_c(-1.5), plot_c(0), plot_c(1.25),
          plot_d(-1.5), plot_d(0), plot_d(1.25), nrow = 2, align = "v")
```

À partir de la distribution cumulative $F(x)$, on peut facilement calculer la probabilité dans un intervalle ($x_1$, $x_2$) en faisant la soustraction $F(x_2)$ - $F(x_1)$. 

## Les fonctions de distribution dans R

Quatre fonctions dans R permettent de travailler avec la distribution normale. Dans chaque cas, il faut spécifier la moyenne (`mean`) et l'écart-type (`sd`) de la distribution.

- `rnorm(n, mean, sd)` génère `n` valeurs aléatoires à partir d'une distribution normale avec de moyenne `mean` et d'écart-type `sd`.

- `dnorm(x, mean, sd)` donne la densité de probabilité associée à la valeur `x`.

- `pnorm(q, mean, sd)` donne la probabilité cumulative associée à une valeur `q`. 

- `qnorm(p, mean, sd)` donne la valeur (`q` pour quantile) associé à une probabilité cumulative `p` donnée.

Des fonctions semblables sont définies pour d'autres distributions courantes, comme nous le verrons plus tard.

Par exemple, pour la distribution normale centrée réduite:

- la probabilité cumulative à 2 écarts-type est de 98%;

```{r}
pnorm(2, mean = 0, sd = 1)
```

- la probabilité d'être à l'intérieur d'un écart-type de part et d'autre de la moyenne est de 68%;

```{r}
pnorm(1, mean = 0, sd = 1) - pnorm(-1, mean = 0, sd = 1)
```

- le troisième quartile (75% de probabilité cumulative) est à 0.67 écart-type au-dessus de la moyenne.

```{r}
qnorm(0.75, mean = 0, sd = 1)
```

## Diagramme quantile-quantile

Le diagramme quantile-quantile (*Q-Q plot*) sert à visualiser la correspondance entre deux distributions statistiques; le plus souvent, nous voulons comparer un échantillon à une distribution théorique donnée.

Par exemple, supposons que nous avons 99 observations d'une variable et nous voulons vérifier que sa distribution soit approximativement normale. Nous trions les observations en ordre croissant et associons la première observation au 1er centile de la distribution normale centrée réduite, la deuxième observation au 2e centile, et ainsi de suite jusqu'au 99e centile. Si l'échantillon provient d'une distribution normale, le nuage de points produit par cette association formera une ligne droite.

*En effet, si $x$ a une distribution normale, alors $x = \mu + \sigma z$ où $z$ est une variable normale centrée réduite.*

Dans R, nous pouvons comparer un vecteur à la distribution normale avec la fonction `qqnorm` et ajouter une ligne droite au graphique avec la fonction `qqline`.

```{r}
test <- rnorm(99, mean = 6, sd = 4)

qqnorm(test)
qqline(test)
```

Comme vous voyez, pour un échantillon aléatoire tiré d'une distribution normale, la correspondance est très bonne; il y a néanmoins un peu de variation due à l'échantillonnage d'un nombre limité de points.

Maintenant, regardons le diagramme quantile-quantile du diamètre des arbres (DHP) dans le jeu de données de Kejimkujik, tel que vu au dernier cours.

```{r}
kejim <- read.csv("../donnees/cours1_kejimkujik.csv")

dhp <- kejim$DHP

qqnorm(dhp)
qqline(dhp)
```

La distribution n'est clairement pas normale. Plus précisément, on constate que:

- Pour les valeurs sous la moyenne (à gauche), les points sont au-dessus de la droite, donc les quantiles de l'échantillon sont plus élevés que ceux d'une distribution normale. En étant plus élevés, ils sont plus rapprochés de la moyenne.

- Pour les valeurs au-dessus de la moyenne (à droite), les quantiles de l'échantillon sont aussi plus élevés que ceux de la distribution normale. Mais dans ce cas-ci, ils sont donc plus éloignés de la moyenne.

Ainsi, le diagramme quantile-quantile nous indique que la distribution est asymétrique avec des valeurs plus rapprochées à gauche et plus éloignées à droite. Puisqu'il s'agit d'une différence assez flagrante, on pouvait la détecter plus facilement avec un histogramme (ci-dessous). Toutefois, le diagramme quantile-quantile peut détecter des différences plus subtiles, il est donc utile d'apprendre à lire et interpréter ce graphique. 

```{r echo = FALSE}
ggplot(kejim, aes(x = DHP)) +
    geom_histogram(color = "white", fill = "blue", binwidth = 5) +
    scale_y_continuous(expand = c(0, 0))
```



# La distribution log-normale

## Définition

Une variable $x$ suit une distribution log-normale si $y = log(x)$ suit une distribution normale.

De façon équivalente, si $y$ suit une distribution normale, $x = e^y$ suit une distribution log-normale.

```{r echo = FALSE}
p1 <- ggplot(NULL, aes(x = c(-3, 3))) +
    labs(title = "Distribution normale", x = "y", y = "f(y)") +
    stat_function(fun = dnorm) +
    scale_y_continuous(expand = c(0, 0))
p2 <- ggplot(NULL, aes(x = c(exp(-5), exp(2)))) +
    labs(title = "Distribution log-normale", x = "x", y = "f(x)") +
    stat_function(fun = dlnorm) +
    scale_y_continuous(expand = c(0, 0))
plot_grid(p1, p2, nrow = 1)
```

## Propriétés des logarithmes

- $log(x)$ est seulement défini pour $x > 0$; 

- $log(x) = 0$ si $x = 1$. Un logarithme négatif ou positif représente une valeur de $x$ inférieure ou supérieure à 1, respectivement.

- Le logarithme transforme les multiplications en additions et les divisions en soustractions.

$$log(xw) = log(x) + log(w)$$
$$log(x/v) = log(x) - log(v)$$ 

- Donc, dans une échelle logarithmique, la distance entre deux nombres est proportionnelle à leur ratio dans l'échelle originale. 

- Si nous ne spécifions pas, les logarithmes sont des logarithmes naturels (base $e$). Toutefois, un changement de base correspond seulement à un changement d'échelle et n'affecte pas la forme de la distribution. Par exemple, pour convertir en base 10:

$$log_{10}(x) = \frac{log(x)}{log(10)}$$

## Utilité de la distribution log-normale

Si la distribution normale tend à être associés à des processus additifs (somme de nombreux effets indépendants), la distribution log-normale est associée à des processus multiplicatifs. Par exemple, si une population croît de 5%, 10% et 3% lors de trois années consécutives, l'accroissement cumulatif correspond à la multiplication: 1.05 x 1.10 x 1.03 = 1.19, soit une augmentation de 19%. Dans un processus multiplicatif, plus une variable est grande, plus elle peut croître, ce qui explique que la distribution résultante soit asymétrique et s'étire vers la droite.

Souvenons-nous que la distribution du DHP de tous les arbres du jeu de données Kejimkujik avait ce type d'asymétrie. Il est aussi plausible que le diamètre d'un arbre soit distribué de façon log-normale, considérant que les plus grands arbres peuvent capter plus de ressources pour croître.  

Pour vérifier si la distribution du DHP est approximativement log-normale, observons le diagramme quantile-quantile pour le logarithme du DHP.

```{r}
qqnorm(log(dhp))
qqline(log(dhp))
```

On observe une meilleure correspondance que pour les données non-transformées, sauf au niveau des valeurs les plus petites du DHP dans l'échantillon, qui sont encore plus élevées que prévus par la distribution de référence. Avez-vous une hypothèse pour cette anomalie près du minimum? (Indice: Quels arbres ne sont pas échantillonnés?)

## Transformation logarithmique

Puisque la plupart des méthodes que nous verrons sont basées sur la distribution normale, la façon la plus simple de traiter une variable log-normale est de lui appliquer une transformation logarithmique et de travailler avec la variable normale ainsi obtenue. Il faut toutefois être prudent lors de l'interprétation des résultats. En particulier, la moyenne de $log(x)$ n'est *pas* égale au logarithme de la moyenne de $x$, comme le montre ce graphique.

```{r echo = FALSE}
p1 <- ggplot(NULL, aes(x = c(-3, 3))) +
    labs(title = "Échelle logarithmique", x = "y", y = "f(y)") +
    stat_function(fun = dnorm, color = "blue", size = 1) +
    stat_function(fun = function(x) dnorm(x, 0, 0.5), color = "orange", size = 1) +
    scale_y_continuous(expand = c(0, 0))
p2 <- ggplot(NULL, aes(x = c(exp(-5), exp(2)))) +
    labs(title = "Échelle originale", x = "x", y = "f(x)") +
    stat_function(fun = dlnorm, color = "blue", size = 1) +
    stat_function(fun = function(x) dlnorm(x, 0, 0.5), color = "orange", size = 1) +
    scale_y_continuous(expand = c(0, 0))
plot_grid(p1, p2, nrow = 1)
```

Dans le graphique ci-dessus, les deux distributions de $y = log(x)$ ont le même mode (maximum de probabilité), la même médiane et la même moyenne à 0. Toutefois, sur l'échelle originale, la moyenne de $x$ est plus élevée pour la distribution en bleu, tandis que le mode est plus petit; les deux distributions ont la même valeur médiane (égale à 1).

# Résumé

- Une distribution discrète est représenté par une fonction de masse de probabilité; une distribution continue est représentée par une fonction de densité de probabilité. 

- La distribution cumulative d'une variable au point $x$ donne la probabilité que cette variable soit inférieure ou égale à $x$.

- Quelques distributions continues: uniforme, normale, log-normale. (Nous verrons plus d'exemples de distributions discrètes plus tard dans la session.)

- La distribution normale est caractérisée par sa moyenne $\mu$ et son écart-type $\sigma$. 

- Toute distribution normale peut être ramenée à la distribution normale centrée réduite ($\mu$ = 0, $\sigma$ = 1) avec la transformation linéaire: $z = (x - \mu)/\sigma$.

- La transformation logarithmique convertit les effets multiplicatifs en effets additifs, et les distributions log-normales en distributions normales.

- Le diagramme quantile-quantile permet de comparer visuellement des données à une distribution de référence.



# Estimation de paramètres 

## Estimation de la moyenne

Supposons qu'on mesure une variable $x$ sur un échantillon de $n$ individus choisis aléatoirement dans une population. (Nous discuterons davantage des stratégies d'échantillonnage dans le prochain cours.) Nous nous servons de la moyenne de l'échantillon:

$$\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i$$

comme estimateur de $\mu$, la moyenne de la distribution de $x$ dans la population. Pour l'instant, nous ne supposons pas que $x$ suit une distribution normale.

Pour cet exemple, imaginons que les 1161 arbres du tableau de données Kejimkujik représentent la population entière, et que nous échantillonnons une partie de ces arbres. 

```{r}
# dhp est le vecteur du DHP des 1161 arbres
paste("La population a un DHP moyen de", round(mean(dhp), 2), "cm avec un écart-type de", round(sd(dhp), 2), "cm.")
```

Dans R, la fonction `sample` sert à tirer un échantillon aléatoire des éléments d'un vecteur.

```{r}
mean(sample(dhp, 20)) # moyenne d'un échantillon de n = 20 arbres
```

Les histogrammes ci-dessous montrent la distribution (sur 10 000 répétitions) du DHP moyen avec une taille d'échantillon $n$ = 10, 20 ou 40.

```{r echo = FALSE, message = FALSE}
df <- data.frame(n = c(10, 20, 40)) %>%
    mutate(x = map(n, ~ replicate(1E4, mean(sample(dhp, size = .))))) %>%
    unnest()

ggplot(df, aes(x = x)) +
     geom_histogram(color = "white") +
     labs(x = "DHP moyen de l'échantillon", y = "Nombre de répétitions") +
     scale_y_continuous(expand = c(0, 0)) +
     facet_wrap(~ n, labeller = label_both) +
     theme(strip.background = element_blank(), strip.text = element_text(face = "bold"))
```

À mesure que la taille de l'échantillon augmente, la distribution devient moins dispersée, mais aussi plus symétrique. Cette deuxième observation est une conséquence du théorème de la limite centrale. Puisque la moyenne est calculée à partir de la somme des valeurs échantillonnées, sa distribution s'approche de la distribution normale pour un $n$ suffisamment grand.

Pour une variable $x$ dont la distribution a une moyenne $\mu$ et une variance $\sigma^2$, on peut démontrer que $\bar{x}$ a une moyenne égale à $\mu$ et une variance égale à $\sigma^2 / n$. L'écart-type de $\bar{x}$, qu'on appelle dans ce contexte l'erreur-type (*standard error*), est donc inversement proportionnelle à la racine carrée de $n$.

Erreur-type de la moyenne: $$\sigma_{\bar{x}} = \frac{\sigma_{x}}{\sqrt{n}}$$

La moyenne et l'erreur-type de $\bar{x}$ calculées à partir des 10 000 échantillons simulés ci-dessus concordent avec les prédictions théoriques.

|  n|  Moyenne (cm)| Erreur-type (cm)| $\sigma / \sqrt{n}$|
|--:|--------:|-----------:|--------:|
| 10|    21.76|        3.85| 3.87|
| 20|    21.81|        2.75| 2.74|
| 40|    21.78|        1.91| 1.94|

Puisque la moyenne de l'estimateur correspond à la valeur du paramètre estimé ($\mu$), $\bar{x}$ est un estimateur *non-biaisé* de $\mu$.

## Écart-type ou erreur-type

Il est important de ne pas confondre l'écart-type de $x$ avec l'erreur-type d'un estimateur, comme $\bar{x}$. L'écart-type de $x$ mesure la dispersion des valeurs individuelles de la variable par rapport à leur moyenne. L'erreur-type de $\bar{x}$ mesure la dispersion de la moyenne d'un échantillon par rapport à la moyenne de la population. L'erreur-type diminue avec la taille de l'échantillon. 

Puisque l'erreur-type diminue selon $\sqrt{n}$ plutôt que $n$, si on veut diminuer cette erreur-type de moitié, il faut multiplier la taille de l'échantillon par 4. 

```{r echo = FALSE}
ggplot(NULL, aes(x = c(10, 100))) +
    stat_function(fun = function(x) x^(-0.5), geom = "line", size = 1) +
    labs(x = "n", y = expression(1/sqrt(n))) +
    geom_segment(aes(x = 0, xend = 20, y = 1/sqrt(20), yend = 1/sqrt(20)),
                 linetype = "dotted") +
    geom_segment(aes(x = 20, xend = 20, y = 0, yend = 1/sqrt(20)),
                 linetype = "dotted") +
    geom_segment(aes(x = 0, xend = 40, y = 1/sqrt(40), yend = 1/sqrt(40)),
                 linetype = "dotted") +
    geom_segment(aes(x = 40, xend = 40, y = 0, yend = 1/sqrt(40)),
                 linetype = "dotted") +
    geom_segment(aes(x = 0, xend = 60, y = 1/sqrt(60), yend = 1/sqrt(60)),
                 linetype = "dotted") +
    geom_segment(aes(x = 60, xend = 60, y = 0, yend = 1/sqrt(60)),
                 linetype = "dotted") +
    scale_x_continuous(breaks = seq(10, 80, 10), limits = c(0, 90), expand = c(0, 0)) +
    scale_y_continuous(breaks = seq(0, 0.3, 0.05), limits = c(0, 0.35), expand = c(0, 0))
    
```

Notez aussi que l'erreur-type dépend seulement de la taille de l'échantillon, pas de celle de la population. Cela est vrai tant que l'échantillon est petit par rapport à la population. Lorsqu'on échantillonne une fraction significative de la population (disons plus de 5%), l'erreur-type réelle est plus petite que $\sigma / \sqrt{n}$. 

## Estimation de la variance

Pour estimer la variance $\sigma^2$ d'une variable $x$, on pourrait calculer la variance de l'échantillon avec l'équation vue au dernier cours.

$$s^2 = \frac{1}{n} \sum_{i = 1}^n \left( x_i - \bar{x} \right)^2  $$

Ici, on utilise $s^2$ pour la variance d'un échantillon pour différencier de $\sigma^2$, le paramètre de la population. 

Comme auparavant, nous testons cet estimateur en simulant 10 000 échantillons du vecteur de DHP avec $n$ = 10, 20 et 40. Le tableau suivant montre la moyenne de $s^2$ et son ratio avec la valeur de $\sigma^2$ pour la population (150.1 cm$^2$).


|  n| Moyenne de $s^2$ (cm$^2$)| Moyenne de $s^2$ / $\sigma^2$ |
|--:|--------:|-----------:|
| 10|    136.3|        0.90|
| 20|    143.1|        0.95|
| 40|    146.6|        0.97|

Ce résultat montre que la variance de l'échantillon ainsi calculée sous-estime systématiquement la variance de la population. Il s'agit donc d'un estimateur *biaisé*. Pourquoi est-ce le cas?

Le problème est que l'estimateur $s^2$ n'est pas basé sur la moyenne de la population, mais sur son estimé $\bar{x}$ calculé à partir du même échantillon. Par définition, l'échantillon est toujours centré sur $\bar{x}$, mais $\bar{x}$ est à une certaine distance de $\mu$. Donc, les écarts à $\mu$ sont un peu plus grands que les écarts à $\bar{x}$.

En fait, l'estimateur définit ci-dessus sous-estime la variance de la population dans un ratio $(n-1)/n$, comme le montre la dernière colonne du tableau (0.9 = 9/10, 0.95 = 19/20). Dans ce cas, on peut corriger le biais en multipliant l'estimateur par $n/(n-1)$, ce qui donne l'estimateur non-biaisé:

$$s^2 = \frac{1}{n - 1} \sum_{i = 1}^n \left( x_i - \bar{x} \right)^2$$

Sa racine carrée constitue un estimateur pour l'écart-type de la population:

$$s = \sqrt{\frac{1}{n - 1} \sum_{i = 1}^n \left( x_i - \bar{x} \right)^2}$$

Contrairement à $s^2$, l'estimateur $s$ pour l'écart-type est biaisé, mais il demeure le plus utilisé, puisqu'il n'existe pas de formule aussi simple et non biaisée pour l'écart-type.

Finalement, on utilise aussi $s$ comme estimateur de $\sigma$ pour le calcul de l'erreur-type de $\bar{x}$ ($s / \sqrt{n}$). 

## Biais et erreur-type d'un estimateur

Les notions de biais et d'erreur-type ont été présentées brièvement dans la section précédente. 

Plus généralement, si on estimons un paramètre $\theta$ (ex.: $\mu$) avec un estimateur $\hat{\theta}$ (ex.: $\bar{x}$), on peut diviser l'erreur carrée moyenne (*mean square error*) entre $\hat{\theta}$ et $\theta$ en deux composantes. (Dans l'équation ci-dessous, la fonction $E[]$ est une autre façon de représenter l'opération de la moyenne.)

$$ E[(\hat{\theta} - \theta)^2] = E[(\hat{\theta} - E[\hat{\theta}])^2] + (E[\hat{\theta}] - \theta)^2 $$

Cette équation nous dit que l'écart carré moyen entre un estimateur et le paramètre est la somme de:

- l'écart carré moyen entre l'estimateur et la moyenne de l'estimateur (autrement dit, la variance de l'estimateur, ou le carré de son erreur-type);

- le carré de l'écart entre la moyenne de l'estimateur et le paramètre (cet écart est le biais);

Donc, on a la relation suivante: *Erreur carrée moyenne = (Erreur-type)$^2$ + (Biais)$^2$*.

Ces deux sources d'erreur ont des propriétés différentes. L'erreur-type est due à la taille limitée de l'échantillon et diminue lorsque $n$ augmente. Le biais est une erreur systématique qui ne dépend pas de la taille de l'échantillon, mais peut être dû à un estimateur biaisé où à un échantillonnage non représentatif de la population. 


# Intervalle de confiance

## Estimateur suivant une distribution normale

Si un échantillon est tiré d'une distribution de moyenne $\mu$ et d'écart-type $\sigma$, nous avons vu que la moyenne de l'échantillon $\bar{x}$ a une moyenne de $\mu$ et un écart-type égal à $\sigma / \sqrt{n}$. 

Supposons en plus que $\bar{x}$ suit une distribution normale. C'est toujours le cas lorsque $x$ suit elle-même une distribution normale. Mais grâce au théorème de la limite centrale, c'est aussi une bonne approximation pour d'autres distributions de $x$, en autant que l'échantillon soit assez grand.

Dans ce cas, la variable $z$ que nous définirons comme:

$$ z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} $$

suit une distribution normale centrée réduite. On peut donc utiliser cette distribution théorique pour déterminer la probabilité que $\bar{x}$ se retrouve dans un intervalle donné.

## Intervalle de probabilité déterminée

À l'opposé, on peut déterminer l'intervalle de $\bar{x}$ correspondant à une probabilité donnée autour de la moyenne.

Par exemple, l'intervalle entre le premier quartile (probabilité cumulative de 25%) et le troisième quartile (probabilité cumulative de 75%) correspond à une probabilité de 50%. On peut déterminer ces quantiles dans R avec `qnorm`.

```{r}
c(qnorm(0.25), qnorm(0.75))
```

*Note*: Par défaut, `qnorm` utilise les paramètres `mean = 0` et `sd = 1`.

L'intervalle est symétrique autour de la moyenne (0) parce que la distribution normale est symétrique et qu'on a choisi des probabilités à distance égale de 50%.
 
Convertissons maintenant cet intervalle de $z$ en intervalle de $\bar{x}$:

$$ \left( -0.674 \le \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \le 0.674 \right)$$

$$ \left( - 0.674 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 0.674 \frac{\sigma}{\sqrt{n}} \right)$$
Il y a une probabilité de 50% que la moyenne de l'échantillon $\bar{x}$ se trouve dans un intervalle de 0.674 fois l'erreur-type de part et d'autre du paramètre $\mu$.

Admettons qu'on représente la valeur de $z$ correspondant à une probabilité cumulative $p$ par $z_p$. Par exemple, $z_{0.25}$ est le premier quartile. Alors, nous ré-écrivons l'intervalle ci-dessus comme:

$$ \left( z_{0.25} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{0.75} \frac{\sigma}{\sqrt{n}} \right)$$

Pour un intervalle de probabilité de 90%, nous remplacerions $z_{0.25}$ et $z_{0.75}$ par $z_{0.05}$ et $z_{0.95}$. En effet, un intervalle de 90% exclut 10% de la distribution et puisqu'on souhaite un intervalle centré, on exclut 5% des deux extrémités de la distribution, tel qu'indiqué par la section coloriée en rouge ci-dessous.

```{r echo = FALSE}
ggplot(NULL, aes(x = c(-3, 3))) +
    labs(x = "z", y = "f(z)") +
    stat_function(fun = dnorm) +
    stat_function(fun = function(x) ifelse(abs(x) > qnorm(0.95), dnorm(x), NA), geom = "area", fill = "red") +
    scale_y_continuous(expand = c(0, 0))
```

Donc, de façon générale, si on représente par $\alpha$ la probabilité exclue, l'intervalle contenant (100% - $\alpha$) de la distribution de $\bar{x}$ correspond à:

$$ \left( z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right)$$

Pour des raisons historiques, l'intervalle de 95% correspondant à $\alpha$ = 0.05 est le plus souvent utilisé:

$$ \left( z_{0.025} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le  z_{0.975} \frac{\sigma}{\sqrt{n}} \right)$$

En remplaçant les quantiles par leur valeur, on obtient:
$$ \left(- 1.96 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 1.96 \frac{\sigma}{\sqrt{n}} \right)$$



## Intervalle de confiance

Pour résumer, si nous échantillonnons une variable $x$ et calculons sa moyenne $\bar{x}$, nous pouvons dire, par exemple, que nous avons 95% de probabilité d'obtenir un estimé $\bar{x}$ qui se trouve à $\pm$ 1.96 erreurs-type du paramètre $\mu$, qui est inconnu.

*Cela suppose toujours que notre modèle est bon, c'est-à-dire que la statistique $\bar{x}$ est bien représentée par une distribution normale.*

Donc, après avoir calculé $\bar{x}$ et calculé son erreur-type, nous établissons un intervalle de 1.96 erreurs-type autour de $\bar{x}$:

$$ \left(\bar{x} - 1.96 \frac{\sigma}{\sqrt{n}}, \bar{x} + 1.96 \frac{\sigma}{\sqrt{n}} \right)$$

D'après notre modèle, nous pouvons affirmer que pour 95% des échantillons possibles de $x$, l'intervalle ainsi calculé contiendra la valeur de $\mu$. Il s'agit donc d'un **intervalle de confiance** à 95% pour $\bar{x}$. 

## Interprétation de l'intervalle de confiance

- La probabilité associée à un intervalle de confiance est basée sur la variabilité de $\bar{x}$ d'un échantillon à l'autre. Elle constitue une probabilité *a priori* (avant d'avoir échantillonné).

- Le paramètre $\mu$ est fixe. Une fois que l'estimé $\bar{x}$ est obtenu pour un échantillon donné, l'intervalle de confiance contient $\mu$ ou ne le contient pas. 

- En particulier, lorsqu'on obtient un intervalle de confiance pour un échantillon donné, il est incorrect d'affirmer que "le paramètre $\mu$ a 95% de probabilité d'être à l'intérieur de cet intervalle".

## Intervalle de confiance d'une moyenne

Nous avons vu que l'intervalle de confiance à (100% - $\alpha$) de la moyenne $\bar{x}$ est donné par:

$$ \left( \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + z_{1 - \alpha/2} \frac{\sigma}{\sqrt{n}} \right)$$

Le seul problème avec cette équation est que nous ne connaissons pas le paramètre $\sigma$. De plus, si nous remplaçons $\sigma$ par son estimé $s$, la probabilité associée à l'intervalle devient inférieure à (100% - $\alpha$). En pratique, il faut donc élargir l'intervalle pour prendre en compte notre connaissance imparfaite de l'écart-type des données. 

La solution de ce problème a été découverte par William Gosset, qui l'a publié sous le pseudonyme de Student. Lorsqu'on utilise un estimé de l'écart-type, l'intervalle de confiance n'est plus basé sur la distribution normale centrée réduite $z$, mais plutôt la distribution $t$ de Student.

La distribution comporte un paramètre, le nombre de degrés de liberté, qui correspond dans ce cas-ci à $n$ - 1. Ainsi, la version corrigée de l'intervalle de confiance à (100% - $\alpha$) pour $\bar{x}$ est:

$$ \left( \bar{x} + t_{(n-1)\alpha/2} \frac{s}{\sqrt{n}}, \bar{x} + t_{(n-1)1 - \alpha/2} \frac{s}{\sqrt{n}} \right)$$

où le $n-1$ entre parenthèses indique le nombre de degrés de liberté de la distribution $t$. Plus $n$ est petit, plus la différence entre la distribution $t$ et la distribution normale centrée réduite $z$ est importante.

# Résumé

- Un estimateur est biaisé lorsque sa moyenne sur l'ensemble des échantillons possibles diffère de la valeur du paramètre à estimer. 

- L'erreur-type mesure la dispersion d'un estimateur d'un échantillon à l'autre, elle diminue avec la taille de l'échantillon.

- Un intervalle de confiance est défini autour d'un estimé de manière à ce que sur l'ensemble des échantillons possibles, il y ait une probabilité spécifique que l'intervalle de confiance obtenu contienne la valeur du paramètre à estimer.

# Référence

Le site [Seeing Theory](https://students.brown.edu/seeing-theory/) présente de façon visuelle et interactive plusieurs concepts statistiques. Par exemple, les chapitres 3 (*Probability Distributions*) et 4 (*Frequentist Inference*) se rapportent aux concepts vus dans ce cours.


