---
title: "Statistical models, parameters and estimators"
date: "<br/>Sept. 9, 2019"
output:
    html_document:
        self_contained: false
        lib_dir: libs
        theme: spacelab
        toc: true
        toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cowplot)
```

# Objectives

Statistical distributions

- Describe the characteristics and use of normal and log-normal distributions.
- Know the relationship between probability density and cumulative probability for a continuous variable, and calculate these quantities in R.
- Compare data to a reference distribution with a quantile-quantile plot.

Parameter estimation

- Estimate the mean and variance of a population from a sample.
- Define the bias and standard error of an estimator.
- Calculate the properties of an estimator by simulation.
- Interpret a confidence interval and calculate the confidence interval for the mean of a normal distribution.

# Statistics, parameters and estimators

At the last class, we saw a series of descriptive statistics: mean, variances, quantiles and others. In general, a *statistic* is a quantity calculated from observations of random variables.

In this class, we will consider the observations as the result of a random process described by a statistical model including some *parameters*. Our main goal will be to determine to what extent a statistic computed from the observations constitutes a good *estimator* of the desired parameter.

For example, if we measure the weight of red squirrels and average these measurements (a statistic), what is our estimate of the average weight of the red squirrel population (a parameter)? What is its margin of error?

> In general, a parameter is a theoretical quantity. In our example, even if we could census all the squirrels, the weight of the individuals varies constantly and the composition of the population too (because of the births, deaths and migrations).

At the end of the course, we will be able to describe the *bias*, *variance* and *confidence interval* of an estimator. Before we get there, we will first review the basic mathematical concepts for describing statistical models, as well as a particularly important model, the normal distribution.


# Probability distributions

A distribution is a function that associates a probability with each possible value of a random variable.

## Discrete distribution

When the variable is discrete, each value has a probability mass, the sum of which must be equal to 1. For example, if $x$ is the number obtained by rolling a balanced six-sided die, the probability of $x$ is 1/6 for each of the numbers from 1 to 6. Since the probability is the same for each value, this would be a **uniform distribution**.

```{r echo = FALSE}
ggplot(NULL, aes(x = 1:6, y = 1/6)) +
    labs(x = "x", y = "Probability of x") +
    geom_bar(stat = "identity") +
    scale_x_continuous(breaks = 1:6) +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 0.2))
```

## Continuous Distribution

When the variable is continuous, the number of possible values is infinite, so the probability of precisely obtaining a given value is zero. The distribution function therefore associates a *density* of probability with a given value.

For example, here is a uniform probability distribution between 0 and 6. The probability density is constant (1/6) in the interval and zero outside.

```{r echo = FALSE}
ggplot(NULL, aes(x = c(-1E-6, 1E-6, 6 - 1E-6, 6 + 1E-6), y = c(0, 1/6, 1/6, 0))) +
    labs(x = "x", y = "Probability density of x") +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 0.2)) +
    geom_line(color = "blue")
```

For a continuous distribution, the probability that the variable is within a given interval is the integral (the area under the curve) of the probability density in that interval.

Here, the probability density is rectangular, so it is easy to calculate the probability of an interval. For example, the probability of getting a value between 2.5 and 3 is 1/2 (width of the interval) x 1/6 (probability density) = 1/12 (~ 0.083). This value corresponds to the area of the filled rectangle in the graph below.

```{r echo = FALSE}
ggplot(NULL, aes(x = c(-1E-6, 1E-6, 6 - 1E-6, 6 + 1E-6), y = c(0, 1/6, 1/6, 0))) +
    labs(x = "x", y = "Probability density of x") +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 0.2)) +
    geom_line(color = "blue") +
    geom_rect(xmin = 2.5, xmax = 3, ymin = 0, ymax = 1/6, fill = "blue")
```

The integral of the probability density over all possible values of $x$ (total probability) must be equal to 1.

## Law of large numbers

In R, the following command generates ten random values (`n = 10`) drawn from the (continuous) uniform distribution between 0 and 6, which we saw in the preceding section.

```{r}
x <- runif(n = 10, min = 0, max = 6)
round(x, 2) # round shows values with 2 decimals
```

The histograms below show the distribution of values within samples with different $n$. What do you notice?

```{r echo = FALSE, message = FALSE}
df <- data.frame(n = c(100, 500, 2000, 10000)) %>%
    mutate(x = map(n, ~ runif(n = ., min = 0, max = 6))) %>%
    unnest()
ggplot(df, aes(x = x)) +
    labs(title = "Histogram of runif(n, 0, 6)") +
    geom_histogram(color = "white") +
    facet_wrap(~n, scales = "free", labeller = label_both) +
    theme(strip.background = element_blank(), strip.text = element_text(face = "bold"))
```

The `runif` function simulates sampling from a variable that follows a uniform distribution. The greater the size of that random sample, the more the distribution of values in the sample approaches the population distribution. This is what we call the **law of large numbers**.


# The normal distribution

## Motivation

In a complex system, the variables we observe result from the combined effect of many processes that we can not directly perceive. For example, the height of a person is influenced by a large number of genetic and environmental factors, the yield of a field depends on the weather for each day of the growing season as well as the micro-habitat perceived by each plant, etc. Modeling each of these processes is usually not possible. Fortunately, when many factors act independently on the same variable, their total effect tends to converge towards some well-known statistical distributions. We will see here a simple example of this phenomenon.

Suppose we are interested in a random variable that is itself the sum of $n$ independent variables, and that each of these variables follows the uniform distribution between 0 and 6 presented above. Even if we do not know the distribution of this sum, the law of large numbers allows us to approximate it from simulations. So we create a function that generates $n$ values of the uniform distribution and calculates their sum, then we generate 10,000 values of that sum (with `replicate`) for a given value of $n$.

```{r}
# Sum of n random variables with a uniform distribution between min and max
sum_unif <- function(n, min, max) {
    sum(runif(n, min, max))
}

n <- 10
x <- replicate(10000, sum_unif(n, 0, 6))
```

Here is a histogram of the values of the sum for different values of $n$. What do you notice?

```{r echo = FALSE, message = FALSE}
df <- data.frame(n = c(2, 5, 10, 20)) %>%
    mutate(x = map(n, ~ replicate(10000, sum_unif(., 0, 6)))) %>%
    unnest()
ggplot(df, aes(x = x)) +
    geom_histogram(color = "white") + 
    facet_wrap(~n, scales = "free", labeller = label_both) +
    theme(strip.background = element_blank(), strip.text = element_text(face = "bold"))
```

The sum of 2 values has a triangular distribution, but starting from $n$ = 5, we see the bell shape of a normal distribution. This is a specific case of a more general statistical law, the central limit theorem.

## Central limit theorem

The central limit theorem stipulates that when we sum a large number of independent random variables, regardless of the distribution of individual variables, the distribution of their sum approximates a normal distribution.

*To be strict, some technical conditions should be included about the variables that are summed, but the simplified definition above is sufficient for this course.*

This property of the normal distribution partly explains why it constitutes such an important model in statistics. As we mentioned earlier, a statistical distribution is often used to represent the unexplained variation of a variable due to a large number of unobserved processes in a complex system. If we suppose that this variation is due to several small effects that are independent and additive, then it is natural that the result approaches a normal distribution. However, it is important to check this assumption for a given variable.

## Normal distribution

If a variable $x$ follows a normal (also called Gaussian) distribution, its probability density is given by:

$$f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2}$$

This distribution has two *parameters*, $\mu$ (which is the average of $x$) and $\ sigma$ (which is its standard deviation).

On a graph of $f(x)$, $\mu$ is the position of the center of the distribution, while $\sigma$ is its dispersion; as $\sigma$ increases, the distribution widens and becomes less concentrated around its mean.

```{r echo = FALSE, out.width = "100%"}
p1 <- ggplot(data.frame(x = c(-10, 10)), aes(x)) +
    labs(y = "f(x)", title = expression(mu == 0), color = expression(sigma)) +
    stat_function(aes(color = "0.5"), fun = dnorm, args = list(mean = 0, sd = 0.5), size = 1) +
    stat_function(aes(color = "1"), fun = dnorm, args = list(mean = 0, sd = 1), size = 1) +
    stat_function(aes(color = "3"), fun = dnorm, args = list(mean = 0, sd = 3), size = 1) +
    scale_color_brewer(palette = "Dark2") +
    theme(legend.position = c(0.7, 0.5))

p2 <- ggplot(data.frame(x = c(-6, 8)), aes(x)) +
    labs(y = "f(x)", title = expression(sigma == 1), color = expression(mu)) +
    stat_function(aes(color = "-1"), fun = dnorm, args = list(mean = -1, sd = 1), size = 1) +
    stat_function(aes(color = "0"), fun = dnorm, args = list(mean = 0, sd = 1), size = 1) +
    stat_function(aes(color = "2"), fun = dnorm, args = list(mean = 2, sd = 1), size = 1) +
    scale_color_brewer(palette = "Dark2") +
    theme(legend.position = c(0.7, 0.5))

plot_grid(p1, p2, nrow = 1)
```

## Standard normal distribution

If a variable $x$ follows a normal distribution with mean $\mu$ and standard deviation $\sigma$, we can obtain a standardized version of $x$ (denoted $z$) by subtracting $\mu$, then dividing by $\sigma$:

$$z = \frac{x - \mu}{\sigma}$$

> In R, the `scale(x)` function applied to a vector `x` standardizes it (by subtracting the mean of `x` and dividing by the standard deviation).

The variable $z$ then follows a standard normal distribution, that is, with $\mu$ = 0 and $\sigma$ = 1:

$$f(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} z^2}$$

In other words, any normal distribution can be obtained from $f(z)$ by moving the center by a distance $\mu$ and widening the distribution by a factor $\sigma$.

The values of $z$ represent the distance from the mean, expressed in standard deviation units, eg: $z$ = -1.5 means one standard deviation and half below average.

## Cumulative distribution

We have previously seen that the probability that a continuous random variable is found in a certain interval corresponds to the area under the curve (the integral) of the probability density in this interval.

The cumulative distribution of a random variable is, for each value $x$, the probability that the value of the variable is less than or equal to $x$. It is therefore equal to the area under the curve of the probability density on the left of $x$.

Here is an illustration of the cumulative distribution $F(z)$ of a standard normal variable $z$.

```{r echo = FALSE}

plot_c <- function(q) {
    ggplot(NULL, aes(x = c(-3, 3))) +
    labs(x = "z", y = "F(z)") +
    stat_function(fun = pnorm) +
    geom_point(aes(x = q, y = pnorm(q)), color = "blue", size = 2) +
    scale_y_continuous(expand = c(0, 0))
} 

plot_d <- function(q) {
    ggplot(NULL, aes(x = c(-3, 3))) +
    labs(x = "z", y = "f(z)") +
    stat_function(fun = dnorm) +
    stat_function(fun = function(x) ifelse(x <= q, dnorm(x), NA), geom = "area", fill = "blue") +
    scale_y_continuous(expand = c(0, 0))
}

plot_grid(plot_c(-1.5), plot_c(0), plot_c(1.25),
          plot_d(-1.5), plot_d(0), plot_d(1.25), nrow = 2, align = "v")
```

From the cumulative distribution $F(x)$, we can easily calculate the probability in an interval ($x_1$, $x_2$) by subtraction, i.e. $F(x_2)$ - $F(x_1)$.

## Distribution functions in R

R provides four functions to work with the normal distribution. In each case, the mean (`mean`) and standard deviation (`sd`) of the distribution must be specified.

- `rnorm(n, mean, sd)` generates `n` random values from a normal distribution with average` mean` and standard deviation `sd`.

- `dnorm(x, mean, sd)` gives the probability density associated with a value `x`.

- `pnorm(q, mean, sd)` gives the cumulative probability associated with a value `q`.

- `qnorm(p, mean, sd)` gives the value (`q` for quantile) associated with a given cumulative probability `p`.

Similar functions are defined for other frequently used distributions, as we will see later.

For example, for the standard normal distribution:

- the cumulative probability at 2 standard deviations above the mean is 98%;

```{r}
pnorm(2, mean = 0, sd = 1)
```

- the probability of being within one standard deviation on either side of the mean is 68%;

```{r}
pnorm(1, mean = 0, sd = 1) - pnorm(-1, mean = 0, sd = 1)
```

- the third quartile (75% cumulative probability) is at 0.67 standard deviation above the mean.

```{r}
qnorm(0.75, mean = 0, sd = 1)
```

## Quantile-quantile plot

The quantile-quantile plot (or Q-Q plot) is used to visualize the correspondence between two statistical distributions; most often, we want to compare a sample to a given theoretical distribution.

For example, suppose we have 99 observations of a variable and we want to check that its distribution is approximately normal. We sort the observations in ascending order and associate the first observation with the 1st percentile of the standard normal distribution, the second observation with the 2nd percentile, and so on until the 99th percentile. If the sample comes from a normal distribution, the scatter plot produced by this association will form a straight line.

*Indeed, if $x$ follows a normal distribution, then $x = \mu + \sigma z$ where $z$ is a standard normal variable.*

In R, we can compare a vector to the normal distribution with the `qqnorm` function and add a straight line to the graph with the` qqline` function.

```{r}
test <- rnorm(99, mean = 6, sd = 4)

qqnorm(test)
qqline(test)
```

As you can see, for a random sample drawn from a normal distribution, the correspondence is very good; however, there is some variation due to the sampling of a limited number of points.

Now, let's look at the Q-Q plot of tree diameter in the Kejimkujik dataset, as seen in the last class.

```{r}
kejim <- read.csv("../donnees/cours1_kejimkujik.csv")

dhp <- kejim$dhp

qqnorm(dhp)
qqline(dhp)
```

The distribution is clearly not normal. More specifically, we find that:

- For the values below the average (on the left), the points are above the line, so the quantiles of the sample are higher than those of a normal distribution. Being higher, they are closer to the mean.

- For values above the mean (on the right), the quantiles of the sample are also higher than those of the normal distribution. But in this case, they would then be farther from the average.

Thus, the Q-Q plot indicates that the distribution is asymmetrical with quantiles closer together on the left and further away on the right. Since this is a fairly obvious difference, it could be detected more easily with a histogram (below). However, the Q-Q plot can detect more subtle differences, so it is useful to learn how to read and interpret this graph.

```{r echo = FALSE}
ggplot(kejim, aes(x = dhp)) +
    geom_histogram(color = "white", fill = "blue", binwidth = 5) +
    scale_y_continuous(expand = c(0, 0))
```

## Exercise

Here is a quantile-quantile plot comparing a sample to a normal distribution. Describe how this sample differs from the theoretical distribution.

```{r echo = FALSE}
tval <- rt(100, df = 2)
qqnorm(tval)
qqline(tval)
```


# The log-normal distribution

## Definition

A variable $x$ follows a log-normal distribution if $y = log(x)$ follows a normal distribution.

Equivalently, if $y$ follows a normal distribution, $x = e^y$ follows a log-normal distribution.

```{r echo = FALSE}
p1 <- ggplot(NULL, aes(x = c(-3, 3))) +
    labs(title = "Normal distribution", x = "y", y = "f(y)") +
    stat_function(fun = dnorm) +
    scale_y_continuous(expand = c(0, 0))
p2 <- ggplot(NULL, aes(x = c(exp(-5), exp(2)))) +
    labs(title = "Log-normal distribution", x = "x", y = "f(x)") +
    stat_function(fun = dlnorm) +
    scale_y_continuous(expand = c(0, 0))
plot_grid(p1, p2, nrow = 1)
```

## Properties of logarithms

- $log(x)$ is only defined for $x > 0$; 

- $log(x) = 0$ if $x = 1$. Negative and positive logarithms represent values under and over 1, respectively.

- The logarithm transforms products into sums, and ratios into differences.

$$log(xw) = log(x) + log(w)$$
$$log(x/v) = log(x) - log(v)$$ 

- Therefore, on a logarithmic scale, the distance between two numbers is proportional to their ratio in the original scale.

- Unless we specify otherwise, the logarithms are natural logarithms (base $e$). However, a base change only causes a change of scale and does not affect the shape of the distribution. For example, to convert to base 10:

$$log_{10}(x) = \frac{log(x)}{log(10)}$$

## Use of the log-normal distribution

If the normal distribution tends to be associated with additive processes (sum of many independent effects), the log-normal distribution is associated with multiplicative processes. For example, if a population increases by 5%, 10% and 3% in three consecutive years, the cumulative increase corresponds to the multiplication: 1.05 x 1.10 x 1.03 = 1.19, or a 19% increase. In a multiplicative process, the larger a variable, the more it can grow, which explains why the resulting distribution is asymmetric and stretched to the right.

Remember that the DBH distribution of all trees in the Kejimkujik dataset had this type of asymmetry. To check if the distribution of DBH is approximately log-normal, see the Q-Q plot for the logarithm of DBH.

```{r}
qqnorm(log(dhp))
qqline(log(dhp))
```

A better match is observed than for the untransformed data, except for the smallest DBH values in the sample, which are still higher than predicted by the reference distribution. Do you have a hypothesis for this anomaly near the minimum? (Hint: Which trees are not sampled?)

## Logarithmic transformation

In the next few weeks, we will see several statistical methods that all assume that the observed variable is explained by additive effects, with a random component following a normal distribution.

Thus, if the process we are interested in is multiplicative and the measured variable approaches a log-normal distribution, we can model this variable after applying a logarithmic transformation. However, you must be cautious when interpreting the results. In particular, the mean of $log(x)$ is *not* equal to the logarithm of the mean of $x$, as this graph shows.

```{r echo = FALSE}
p1 <- ggplot(NULL, aes(x = c(-3, 3))) +
    labs(title = "Logarithmic scale", x = "y", y = "f(y)") +
    stat_function(fun = dnorm, color = "blue", size = 1) +
    stat_function(fun = function(x) dnorm(x, 0, 0.5), color = "orange", size = 1) +
    scale_y_continuous(expand = c(0, 0))
p2 <- ggplot(NULL, aes(x = c(exp(-5), exp(2)))) +
    labs(title = "Original scale", x = "x", y = "f(x)") +
    stat_function(fun = dlnorm, color = "blue", size = 1) +
    stat_function(fun = function(x) dlnorm(x, 0, 0.5), color = "orange", size = 1) +
    scale_y_continuous(expand = c(0, 0))
plot_grid(p1, p2, nrow = 1)
```

In the graph above, the two distributions of $y = log(x)$ have the same mode (peak of the distribution), the same median and the same mean at 0. However, on the original scale, the mean of $x$ is higher for the blue distribution, while its mode is smaller; both distributions have the same median value (equal to 1).

## Exercise

Among the following variables, which ones do you think would most likely follow a normal distribution, and why?

(a) The mean temperature in September in Rouyn-Noranda.
(b) The population of cities and towns in Québec.
(c) The number of followers of a social media account (e.g. Twitter).
(d) The time runners take to complete a marathon.


# Summary

- A discrete distribution is represented by a probability mass function; a continuous distribution is represented by a probability density function.

- The cumulative distribution of a variable at a point $x$ gives the probability that this variable is less than or equal to $x$.

- Examples of continuous distributions: uniform, normal, log-normal. (We will see more examples of discrete distributions later in the session.)

- The normal distribution is characterized by its average $\mu$ and its standard deviation $\sigma$.

- Any normal distribution can be reduced to the standard normal distribution ($\mu$ = 0, $\sigma$ = 1) with the linear transformation: $z = (x - \mu) / \sigma$.

- A logarithmic transformation converts multiplicative effects into additive effects, and log-normal distributions into normal distributions.

- The quantile-quantile plot provides a visual way to compare data to a reference distribution.


# Parameter estimation

## Estimation of the mean

Suppose we measure a variable $x$ on a sample of $n$ randomly selected individuals in a population. We use the sample mean:

$$\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i$$
as the estimator of $\mu$, the mean of the population distribution of $x$. For now, we do not assume that $x$ follows a normal distribution.

For this example, imagine that the 1161 trees in the Kejimkujik dataset represent the entire population, and that we sample some of those trees.

```{r}
# dhp is the vector of DBH for 1161 trees
paste("The population has a mean DBH of", round(mean(dhp), 2), "cm with a standard deviation of", round(sd(dhp), 2), "cm.")
```

In R, the `sample` function draws a random sample from the elements of a vector.

```{r}
mean(sample(dhp, 20)) # mean DBH for a sample of n = 20 trees
```

The histograms below show the distributions (out of 10,000 replicates) of the mean DBH with a sample size $n$ = 10, 20 or 40.

```{r echo = FALSE, message = FALSE}
df <- data.frame(n = c(10, 20, 40)) %>%
    mutate(x = map(n, ~ replicate(1E4, mean(sample(dhp, size = .))))) %>%
    unnest()

ggplot(df, aes(x = x)) +
     geom_histogram(color = "white") +
     labs(x = "Sample mean of DBH", y = "Number of replicates") +
     scale_y_continuous(expand = c(0, 0)) +
     facet_wrap(~ n, labeller = label_both) +
     theme(strip.background = element_blank(), strip.text = element_text(face = "bold"))
```

As the sample size increases, the distribution becomes less dispersed, but also more symmetrical. This second observation is a consequence of the central limit theorem. Since the mean is calculated from the sum of the sampled values, its distribution approaches the normal distribution for a sufficiently large $n$.

For a variable $x$ that is distributed with a mean $\mu$ and a variance $\sigma^2$, we can prove that $\bar{x}$ has a mean equal to $\mu$ and a variance equal to $\sigma^2 / n$. The standard deviation of $\bar{x}$, which in this context is called the *standard error*, is therefore inversely proportional to the square root of $n$.

Standard error of the mean: $$\sigma_{\bar{x}} = \frac{\sigma_{x}}{\sqrt{n}}$$

The mean and standard error of $\bar{x}$ calculated from the 10,000 samples simulated above are consistent with the theoretical predictions.

|  n|  Mean (cm)| Standard error (cm)| $\sigma / \sqrt{n}$|
|--:|--------:|-----------:|--------:|
| 10|    21.76|        3.85| 3.87|
| 20|    21.81|        2.75| 2.74|
| 40|    21.78|        1.91| 1.94|

Since the mean of the estimator is equal to the value of the estimated parameter ($\mu$), $\bar{x}$ is an *unbiased* estimator of $\mu$.

## Standard deviation or standard error

It is important not to confuse the standard deviation of $x$ with the standard error of an estimator, such as $\bar{x}$. The standard deviation of $x$ measures the dispersion of the individual values of the variable relative to their mean. The standard error of $\bar{x}$ measures the dispersion of the sample mean relative to the population mean. The standard error decreases with the size of the sample.

Since the standard error decreases according to $\sqrt{n}$ rather than $n$, if we want to reduce this standard error by half, we must increase the sample size by a factor of 4.

```{r echo = FALSE}
ggplot(NULL, aes(x = c(10, 100))) +
    stat_function(fun = function(x) x^(-0.5), geom = "line", size = 1) +
    labs(x = "n", y = expression(1/sqrt(n))) +
    geom_segment(aes(x = 0, xend = 20, y = 1/sqrt(20), yend = 1/sqrt(20)),
                 linetype = "dotted") +
    geom_segment(aes(x = 20, xend = 20, y = 0, yend = 1/sqrt(20)),
                 linetype = "dotted") +
    geom_segment(aes(x = 0, xend = 40, y = 1/sqrt(40), yend = 1/sqrt(40)),
                 linetype = "dotted") +
    geom_segment(aes(x = 40, xend = 40, y = 0, yend = 1/sqrt(40)),
                 linetype = "dotted") +
    geom_segment(aes(x = 0, xend = 60, y = 1/sqrt(60), yend = 1/sqrt(60)),
                 linetype = "dotted") +
    geom_segment(aes(x = 60, xend = 60, y = 0, yend = 1/sqrt(60)),
                 linetype = "dotted") +
    scale_x_continuous(breaks = seq(10, 80, 10), limits = c(0, 90), expand = c(0, 0)) +
    scale_y_continuous(breaks = seq(0, 0.3, 0.05), limits = c(0, 0.35), expand = c(0, 0))
    
```

Note also that the standard error depends only on the size of the sample, not on the population size. This is true as long as the sample is small relative to the population. When sampling a significant fraction of the population (say more than 5%), the actual standard error is smaller than $\sigma / \sqrt{n}$.

## Estimation of the variance

To estimate the variance $\sigma^2$ of a variable $x$, one could calculate the variance of the sample with the equation seen at the last class.

$$s^2 = \frac{1}{n} \sum_{i = 1}^n \left( x_i - \bar{x} \right)^2  $$

Here, we $s^2$ for the variance of a sample to differentiate from the population parameter $\sigma^2$.

As before, we test this estimator by simulating 10,000 samples from the DBH vector with $n$ = 10, 20, and 40. The following table shows the average of $s^2$ and its ratio to the popultion parameter $\sigma^2$ (150.1 cm$^2$).

|  n| Mean of $s^2$ (cm$^2$)| Mean of $s^2$ / $\sigma^2$ |
|--:|--------:|-----------:|
| 10|    136.3|        0.90|
| 20|    143.1|        0.95|
| 40|    146.6|        0.97|

This result shows that the calculated variance of the sample systematically underestimates the variance of the population. It is therefore a *biased* estimator. Why is this the case?

The problem is that the estimator $s^2$ is not based on the population mean, but on its estimate $\bar{x}$ calculated from the same sample. By definition, the sample is always centered on $\bar{x}$, but $\bar{x}$ is at some distance from $\mu$. Therefore, the deviations from $\mu$ are slightly larger than the deviations from $\bar{x}$.

In fact, the estimator defined above underestimates the variance of the population by a ratio $(n-1)/n$, as shown in the last column of the table (0.9 = 9/10, 0.95 = 19/20). In that case, the bias can be corrected by multiplying the estimator by $n / (n-1)$, giving the unbiased estimator:

$$s^2 = \frac{1}{n - 1} \sum_{i = 1}^n \left( x_i - \bar{x} \right)^2$$

Its square root provides an estimator for the population standard deviation:

$$s = \sqrt{\frac{1}{n - 1} \sum_{i = 1}^n \left( x_i - \bar{x} \right)^2}$$

Unlike $s^2$, the $s$ estimator for the standard deviation is biased, but it remains the most commonly used one, since there is no simple and unbiased formula for standard deviation.

Finally, we also use $s$ as the $\sigma$ estimator for calculating the standard error of $\bar{x}$ ($s / \sqrt{n}$).

## Degrees of freedom

Another way to explain the division by ($n - 1$) in the calculation of $s^2$ is based on the concept of degrees of freedom.

The number of degrees of freedom is the number of independent data used in the calculation of a statistic. Here, $s^2$ is computed from the deviations between each observation of $x$ and its mean ($x_i - \bar{x}$). As we saw in the first class, the definition of $\bar{x}$ ensures that the sum of these deviations is equal to 0. In this case, when we know the first $n - 1$ deviations, we can automatically deduce the last, which is not an independent data point.

## Bias and standard error of an estimator

The notions of bias and standard error were briefly presented in the previous section.

More generally, if we estimate a parameter $\theta$ (e.g. $\mu$) with an estimator $\hat{\theta}$ (e.g. $\bar{x}$), we can divide the *mean square error* between $\hat {\theta}$ and $\theta$ into two components. (In the equation below, the function $E[]$ is another way of representing the mean.)

$$E[(\hat{\theta} - \theta)^2] = E[(\hat{\theta} - E[\hat{\theta}])^2] + (E[\hat{\theta}] - \theta)^2$$

This equation tells us that the mean square deviation between an estimator and the parameter is the sum of:

- the mean square deviation between the estimator and its mean (that is, the variance of the estimator, or the square of its standard error);

- the square of the difference between the mean of the estimator and the parameter (this difference is the bias);

So, we have the following relation: *Mean square error = (Standard error)$^2$ + (Bias)$^2$*.

These two sources of error have different properties. The standard error is due to the limited size of the sample and decreases as $n$ increases. Bias is a systematic error that does not depend on the size of the sample, but may be due to a biased estimator or unrepresentative sampling of the population.

## Exercise

In order to estimate the average wood density of jack pine on a site, you first sample 9 trees, which have an average wood density of 450 kg/m$^3$ with a standard deviation of 90 kg/m$^3$.

(a) What is the standard error of this mean?

(b) If you wanted to know the average with a standard error of no more than 10 kg/m$^3$, how many trees do you expect to sample?


# Confidence interval

## Estimator with a normal distribution

If a sample is drawn from a distribution with mean $\mu$ and standard deviation $\sigma$, we have seen that the sample mean $\bar{x}$ has a mean of $\mu$ and a standard deviation equal to $\sigma / \sqrt{n}$.

Suppose that $\bar{x}$ follows a normal distribution. This is always the case when $x$ itself is normally distributed. But thanks to the central limit theorem, it is also a good approximation for other distributions of $x$, as long as the sample is large enough.

In this case, the variable $z$ that we will define as:

$$ z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} $$

follows a standard normal distribution. We can therefore use that theoretical distribution to determine the probability that $\bar{x}$ is found in a given interval.

## Interval with a given probability

Alternatively, we can determine the interval of $\bar{x}$ corresponding to a given probability around the mean.

For example, the interval between the first quartile (cumulative probability of 25%) and the third quartile (cumulative probability of 75%) corresponds to a probability of 50%. These quantiles can be determined in R with `qnorm`.

```{r}
c(qnorm(0.25), qnorm(0.75))
```

*Note*: By default, `qnorm` uses the parameters `mean = 0` and `sd = 1`.

The interval is symmetrical around the mean (0) since the normal distribution is symmetrical and our chosen quantiles are equally distant from 50%.

Let's convert this interval of $z$ to an interval of $\bar{x}$:

$$ \left( -0.674 \le \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \le 0.674 \right)$$

$$ \left( - 0.674 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 0.674 \frac{\sigma}{\sqrt{n}} \right)$$

There is a 50% probability that the sample mean $\bar{x}$ is in a range of 0.674 standard errors on each side of the parameter $\mu$.

Suppose we represent the value of $z$ corresponding to a cumulative probability $p$ as $z_p$. For example, $z_{0.25}$ is the first quartile. Thus, we rewrite the interval above as:

$$ \left( z_{0.25} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{0.75} \frac{\sigma}{\sqrt{n}} \right)$$

For a 90% probability interval, we would replace $z_{0.25}$ and $z_{0.75}$ with $z_{0.05}$ and $z_{0.95}$. Indeed, a 90% interval excludes 10% of the distribution and since we want a centered interval, we exclude 5% of both ends of the distribution, as indicated by the red part of the distribution in the graph below.

```{r echo = FALSE}
ggplot(NULL, aes(x = c(-3, 3))) +
    labs(x = "z", y = "f(z)") +
    stat_function(fun = dnorm) +
    stat_function(fun = function(x) ifelse(abs(x) > qnorm(0.95), dnorm(x), NA), geom = "area", fill = "red") +
    scale_y_continuous(expand = c(0, 0))
```

More generally, if we represent the probability outside the interval as $\alpha$, an interval containing (100% - $\alpha$) of the distribution of $\bar{x}$ is given by: 

$$ \left( z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right)$$

For historical reasons, the 95% interval corresponding to $\alpha$ = 0.05 is used most often:

$$ \left( z_{0.025} \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le  z_{0.975} \frac{\sigma}{\sqrt{n}} \right)$$

By replacing the quantiles by their values, we obtain:

$$ \left(- 1.96 \frac{\sigma}{\sqrt{n}} \le \bar{x} - \mu \le 1.96 \frac{\sigma}{\sqrt{n}} \right)$$

## Confidence interval

To summarize, if we sample a variable $x$ and calculate its sample mean $\bar{x}$, we can say, for example, that we have a 95% probability of getting an estimate $\bar{x}$ that is $\pm$ 1.96 standard errors around the parameter $\mu$, which is unknown.

*This always assumes our model is good, that is, the $\bar{x}$ statistic is well represented by a normal distribution.*

So, after calculating $\bar{x}$ and calculating its standard error, we establish an interval of 1.96 standard errors on each side of $\bar{x}$:

$$ \left(\bar{x} - 1.96 \frac{\sigma}{\sqrt{n}}, \bar{x} + 1.96 \frac{\sigma}{\sqrt{n}} \right)$$

According to our model, we can say that for 95% of the possible samples of $x$, the interval thus calculated will contain the value of $\mu$. This is a 95% **confidence interval** for $\bar{x}$.

## Interpretation of the confidence interval

- The probability associated with a confidence interval is based on the variability of $\bar{x}$ from one sample to another. It constitutes a probability *a priori* (before having sampled).

- The parameter $\mu$ is fixed. Once the estimated $\bar{x}$ is obtained for a given sample, the confidence interval either contains $\mu$ or does not contain it.

- Since a parameter is not a random variable, it does not have a statistical distribution. It is therefore inaccurate to say, after we obtain a confidence interval for a given sample, that "the parameter $\mu$ has 95% probability of being within this interval".

## Confidence interval of a mean

We have seen that the (100% - $\alpha$) confidence interval of the mean $\bar{x}$ is given by:

$$ \left( \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + z_{1 - \alpha/2} \frac{\sigma}{\sqrt{n}} \right)$$

The only problem with this equation is that we do not know the parameter $\sigma$. And if we replace $\sigma$ with its estimator $s$, the probability associated with the interval becomes less than (100% - $\alpha$). In practice, we need to widen the interval to take into account our imperfect knowledge of the standard deviation of the data.

The solution to this problem was discovered by William Gosset, who published it under the pseudonym Student. When using an estimate of the standard deviation, the confidence interval is no longer based on the standard normal distribution $z$, but on the Student $t$ distribution.

The $t$ distribution has a parameter, the number of degrees of freedom, which in this case corresponds to $n$ - 1. Thus, the corrected version of the (100% - $\alpha$) confidence interval for $\bar{x}$ is:

$$ \left( \bar{x} + t_{(n-1)\alpha/2} \frac{s}{\sqrt{n}}, \bar{x} + t_{(n-1)1 - \alpha/2} \frac{s}{\sqrt{n}} \right)$$

where the $n-1$ in parentheses indicates the number of degrees of freedom of the $t$ distribution.

## The t Distribution

The graph below compares the standard normal distribution ($z$) with $t$ distributions having 4 and 9 degrees of freedom. The smaller the number of degrees of freedom, the further the $t$ distribution is from the normal. In particular, the standard deviation increases and values far from the mean have a higher probability, which explains why the confidence interval constructed from the $t$ distribution is wider.

```{r echo = FALSE}

ggplot(data.frame(x = c(-4, 4)), aes(x)) +
    labs(y = "f(x)", color = "Distribution") +
    stat_function(aes(color = "z"), fun = dnorm, args = list(mean = 0, sd = 1), size = 1) +
    stat_function(aes(color = "t[4]"), fun = dt, args = list(df = 4), size = 1) +
    stat_function(aes(color = "t[9]"), fun = dt, args = list(df = 9), size = 1) +
    scale_color_brewer(palette = "Dark2", 
                       labels = c(expression(t[4]), expression(t[9]), expression(z))) +
    theme(legend.position = c(0.8, 0.5))
```

While this is difficult to tell from the graph, the $t$ distribution also has a different shape. Even when compared with a normal distribution with the same standard deviation, the $t$ distribution puts a greater probability on extreme values (very far from the mean).

When the number of degrees of freedom is high, such as when we compute the mean of a large sample, the $t$ distribution approaches the standard normal distribution.

# Summary

- An estimator is biased when its average over all possible samples differs from the value of the parameter to be estimated.

- The standard error measures the dispersion of an estimator from one sample to another, and decreases with the size of the sample.

- A confidence interval is defined around an estimate so that across all the possible samples, there is a specific probability that the confidence interval obtained contains the value of the parameter to be estimated.

- Due to the central limit theorem, the difference between the sample mean and the population mean often follows an approximately normal distribution.

- Student's $t$ distribution replaces the standard normal distribution when estimating the confidence interval for the sample mean, when the population standard deviation is unknown. That distribution has more frequent extreme values, especially with a low number of degrees of freedom.

# Reference

The website [Seeing Theory](https://students.brown.edu/seeing-theory/) presents several statistical concepts in a visual and interactive way. For example, chapters 3 (*Probability Distributions*) and 4 (*Frequentist Inference*) relate to the concepts seen in this class.


