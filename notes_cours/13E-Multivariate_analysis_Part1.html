<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="date" content="2019-12-02" />

<title>Multivariate analysis, Part 1</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>
<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.39.2/plotly-latest.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Multivariate analysis, Part 1</h1>
<h4 class="date"><br/>December 2, 2019</h4>

</div>


<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li><p>Describe different roles of multivariate analysis techniques.</p></li>
<li><p>Describe the usefulness of ordination and clustering methods to reduce the dimensionality of a dataset.</p></li>
<li><p>Execute a principal component analysis and be able to interpret the results.</p></li>
<li><p>Divide observations into classes using hierarchical clustering methods.</p></li>
</ul>
</div>
<div id="why-multivariate-analysis" class="section level1">
<h1>Why multivariate analysis?</h1>
<p>Multivariate analysis aims to model the <em>joint distribution</em> of several variables, that is, the variation of individual variables and the correlations between them.</p>
<p>Here are some situations where these methods might be useful:</p>
<ul>
<li><p>In the description or exploration of a dataset with many variables, it can be useful to visualize the relationship between variables (e.g. correlations) and between observations (e.g. similarity between study sites) with a single graph.</p></li>
<li><p>The response we want to explain is multivariate. For example, the composition of an ecological community is defined as the presence or abundance of several species at different sites. Interactions between species (e.g. competition) make it impossible to study this response as the combination of several independent univariate responses (one species at a time).</p></li>
<li><p>We want to explain a univariate response according to a large number of predictors that are partially correlated. For example, the presence of a species depending on climatic conditions, the growth of trees of different species on a site depending on their functional traits, etc. In previous classes on regression, we have seen that it is difficult to estimate the effect of a large number of potentially correlated predictors in the same model. Model selection is also complicated when the number of predictors is large.</p></li>
</ul>
</div>
<div id="structure-of-multivariate-data" class="section level1">
<h1>Structure of multivariate data</h1>
<p>Suppose we make <span class="math inline">\(n\)</span> observations of <span class="math inline">\(p\)</span> variables. We represent these observations by a matrix where <span class="math inline">\(x_ {ij}\)</span> is the value of the variable <span class="math inline">\(j\)</span> for the observation <span class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[  
\begin{bmatrix}
x_{11} &amp; x_{12} &amp; ... &amp; x_{1p} \\
x_{21} &amp; x_{22} &amp; ... &amp; x_{2p} \\
... &amp; ... &amp; ... &amp; ... \\
x_{n1} &amp; ... &amp; ... &amp; x_{np}
\end{bmatrix}
\]</span></p>
<p>As an example, let’s consider the <a href="../donnees/cities_climate.csv">cities_climate.csv</a> dataset, which contains climatic data from WorldClim for 49 major cities.</p>
<pre class="r"><code>villes &lt;- read.csv(&quot;../donnees/cities_climate.csv&quot;)
head(villes)</code></pre>
<pre><code>##       city    long    lat t_mean t_diu  t_sd t_max t_min p_ann p_max p_min
## 1    Tokyo 139.751 35.685   15.6   7.9 7.492  30.7   0.7  1428   185    47
## 2 Shanghai 121.400 31.046   16.2   7.4 8.339  31.6   0.8  1050   167    41
## 3   Mumbai  72.826 18.975   26.8   7.3 1.767  32.7  18.9  2370   827     0
## 4  Karachi  67.082 24.906   25.9   9.9 4.035  34.2  12.0   194    82     0
## 5    Delhi  77.217 28.667   25.1  13.1 6.715  40.5   7.3   734   274     3
## 6   Manila 120.982 14.604   27.3   8.0 1.078  33.8  21.4  2150   466     8
##   p_cv
## 1   40
## 2   46
## 3  143
## 4  134
## 5  142
## 6   90</code></pre>
<p>Here is the list of climate variables in this data frame:</p>
<ul>
<li><em>t_mean</em>: Mean annual temperature.</li>
<li><em>t_diu</em>: Diurnal temperature variation, the mean difference between the maximum and minimum temperature in the same month.</li>
<li><em>t_sd</em>: Seasonal temperature variation, standard deviation of mean temperature between months.</li>
<li><em>t_max</em>: Maximum temperature of the hottest month.</li>
<li><em>t_min</em>: Minimum temperature of the coldest month.</li>
<li><em>p_ann</em>: Annual precipitation.</li>
<li><em>p_max</em>: Precipitation of the wettest month.</li>
<li><em>p_min</em>: Precipitation of the driest month.</li>
<li><em>p_cv</em>: Coefficient of variation (standard deviation / mean) of precipitation between months.</li>
</ul>
<p>All temperatures are in degrees C and all precipitation variables (except coefficient of variation) are in mm.</p>
<p>We will extract the climatic variables in a separate data frame, then add row names to indicate the city names.</p>
<pre class="r"><code>climat &lt;- villes[, 4:12]
rownames(climat) &lt;- villes$city</code></pre>
<p>In a multivariate analysis context, we will treat our 49 observations as “points” in a 9-dimensional space (one dimension for each variable).</p>
<div id="standardization-of-variables" class="section level2">
<h2>Standardization of variables</h2>
<p>The ordination and clustering methods we will see in this course are based on the concept of <em>distance</em> between points in this multidimensional space. However, we cannot calculate these distances if the units differ between the variables: for example, we cannot say whether a variation of temperature of a degree C is more or less important than a variation of precipitation of one mm.</p>
<p>For this reason, we begin by standardizing our variables. When applied to a data frame where all variables are numeric, the <code>scale</code> function transforms each variable by subtracting its mean and dividing by its standard deviation.</p>
<pre class="r"><code>clim_norm &lt;- scale(climat)
round(head(clim_norm), 2) # Only show two digits after decimal</code></pre>
<pre><code>##          t_mean t_diu  t_sd t_max t_min p_ann p_max p_min  p_cv
## Tokyo     -0.44 -0.77  0.91  0.07 -0.62  0.66 -0.06  0.78 -0.74
## Shanghai  -0.36 -0.96  1.17  0.24 -0.61  0.08 -0.17  0.58 -0.59
## Mumbai     1.17 -1.00 -0.86  0.46  1.14  2.12  3.86 -0.81  1.84
## Karachi    1.04  0.01 -0.16  0.74  0.47 -1.24 -0.69 -0.81  1.62
## Delhi      0.93  1.24  0.67  1.95  0.02 -0.41  0.48 -0.71  1.82
## Manila     1.25 -0.73 -1.07  0.67  1.39  1.78  1.66 -0.54  0.52</code></pre>
<p>The result is a <em>matrix</em> where each column has a mean of 0 and a standard deviation of 1. For example, in Tokyo the mean temperature is 0.44 standard deviations below the mean of the 49 cities, whereas the annual precipitation is 0.66 standard deviations above the mean of cities.</p>
<p><strong>Note</strong></p>
<p>You can check that the standard deviation of each column is 1 with the <code>apply</code> function:</p>
<pre class="r"><code>apply(clim_norm, 2, sd)</code></pre>
<pre><code>## t_mean  t_diu   t_sd  t_max  t_min  p_ann  p_max  p_min   p_cv 
##      1      1      1      1      1      1      1      1      1</code></pre>
<p>To understand this command, it is easier to read it backwards. The <code>apply(clim_norm, 2, sd)</code> command says to apply the standard deviation function <code>sd</code> to each column (the number 2 means column, the number 1 means row) of the <code>clim_norm</code> matrix.</p>
<p>There are shortcuts in R to calculate the sum or mean of the rows or columns of a matrix: <code>rowSums</code>, <code>colSums</code>, <code>rowMeans</code>, and <code>colMeans</code>.</p>
</div>
<div id="dimensionality-reduction" class="section level2">
<h2>Dimensionality reduction</h2>
<p>It is not possible to visualize a scatter of points in 9 dimensions. From an exploratory point of view, we would like to be able to reduce the dimensionality of our data while keeping as much information as possible on climate variation between cities. This class aims to provide an introduction to two types of methods for performing this reduction.</p>
<ul>
<li><p><strong>Ordination </strong> methods perform a transformation of the original <span class="math inline">\(p\)</span> variables into a smaller number of new variables, so that the representation of the points in the reduced space (often in 2 dimensions) reproduces as closely as possible the distances between the points in the original <span class="math inline">\(p\)</span>-dimensional space. These methods order observations according to the most important axes of variation, hence their name. <strong>Principal component analysis</strong>, discussed below, is an example of an ordination method.</p></li>
<li><p>If ordination aims to represent the original numeric variables by a reduced number of transformed numeric variables, clustering methods attempt instead to divide the observations into a discrete number of groups (or categories) according to their proximity in the <span class="math inline">\(p\)</span>-dimensional space. The clustering method we will see in this class is <strong>agglomerative hierarchical clustering</strong>.</p></li>
</ul>
</div>
</div>
<div id="principal-component-analysis" class="section level1">
<h1>Principal component analysis</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Here is a scatter plot representing three of the climate variables for the 49 cities: mean temperature, standard deviation of temperature, and annual precipitation.</p>
<div id="htmlwidget-eccc850455497682d1a3" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-eccc850455497682d1a3">{"x":{"visdat":{"23fc273f3815":["function () ","plotlyVisDat"]},"cur_data":"23fc273f3815","attrs":{"23fc273f3815":{"x":{},"y":{},"z":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","size":1,"inherit":true},"23fc273f3815.1":{"x":{},"y":{},"z":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"text":{},"type":"scatter3d","mode":"text","size":0.5,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"t_mean"},"yaxis":{"title":"t_sd"},"zaxis":{"title":"p_ann"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"x":[15.6,16.2,26.8,25.9,25.1,27.3,4.9,10.9,18.2,14,27,15.8,27.5,12,25.6,21.4,18.8,12.1,11,11.4,25.9,24,23.1,22.7,28,14.7,7.9,27.3,17.6,4.9,18.3,14.8,26.7,17.7,26.8,25.4,27.4,16.6,9,6.5,14.1,12.1,24.4,15.2,18.4,24.7,15.7,25.7,11.3],"y":[7.492,8.339,1.767,4.035,6.715,1.078,9.803,10.167,2.137,6.292,1.09,2.039,0.43,8.775,1.333,5.242,2.659,10.686,4.943,0.324,3.667,7.331,1.969,8.704,1.329,4.373,9.543,1.485,3.668,8.937,3.509,3.768,1.055,3.705,0.446,7.128,0.999,2.941,6.918,10.737,6.672,8.676,2.336,1.139,1.205,2.499,5.805,1.086,5.687],"z":[1428,1050,2370,194,734,2150,679,1420,1388,759,1672,641,1949,1150,1370,20,15,608,627,994,1979,608,1255,159,1457,355,793,2465,1275,636,394,656,1790,411,2363,111,1822,885,574,965,437,330,440,1156,935,461,788,1506,633],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","size":[100],"sizemode":"area","line":{"color":"rgba(31,119,180,1)"}},"textfont":{"size":100},"error_y":{"color":"rgba(31,119,180,1)","width":100},"error_x":{"color":"rgba(31,119,180,1)","width":100},"line":{"color":"rgba(31,119,180,1)","width":100},"frame":null},{"x":[15.6,16.2,26.8,25.9,25.1,27.3,4.9,10.9,18.2,14,27,15.8,27.5,12,25.6,21.4,18.8,12.1,11,11.4,25.9,24,23.1,22.7,28,14.7,7.9,27.3,17.6,4.9,18.3,14.8,26.7,17.7,26.8,25.4,27.4,16.6,9,6.5,14.1,12.1,24.4,15.2,18.4,24.7,15.7,25.7,11.3],"y":[7.492,8.339,1.767,4.035,6.715,1.078,9.803,10.167,2.137,6.292,1.09,2.039,0.43,8.775,1.333,5.242,2.659,10.686,4.943,0.324,3.667,7.331,1.969,8.704,1.329,4.373,9.543,1.485,3.668,8.937,3.509,3.768,1.055,3.705,0.446,7.128,0.999,2.941,6.918,10.737,6.672,8.676,2.336,1.139,1.205,2.499,5.805,1.086,5.687],"z":[1428,1050,2370,194,734,2150,679,1420,1388,759,1672,641,1949,1150,1370,20,15,608,627,994,1979,608,1255,159,1457,355,793,2465,1275,636,394,656,1790,411,2363,111,1822,885,574,965,437,330,440,1156,935,461,788,1506,633],"text":["Tokyo","Shanghai","Mumbai","Karachi","Delhi","Manila","Moscow","Seoul","Sao Paulo","Istanbul","Lagos","Mexico","Jakarta","New York","Kinshasa","Cairo","Lima","Beijing","London","Bogota","Dhaka","Lahore","Rio de Janeiro","Baghdad","Bangkok","Santiago","Toronto","Rangoon","Sydney","Saint Petersburg","Los Angeles","Melbourne","Abidjan","Casablanca","Singapore","Riyadh","Ho Chi Minh City","Cape Town","Berlin","Montreal","Madrid","Kabul","Luanda","Addis Abeba","Nairobi","Dakar","Rome","Santo Domingo","Paris"],"type":"scatter3d","mode":"text","marker":{"color":"rgba(255,127,14,1)","size":[10],"sizemode":"area","line":{"color":"rgba(255,127,14,1)"}},"textfont":{"size":10},"error_y":{"color":"rgba(255,127,14,1)","width":10},"error_x":{"color":"rgba(255,127,14,1)","width":10},"line":{"color":"rgba(255,127,14,1)","width":10},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":[]}</script>
<p>The position of the points on the screen represents the projection of the 3D scatter of points on a 2D image. By rotating the graph, this projection changes even if the value of the points remains the same for the three variables. Some rotations make it easier to differentiate the points than others, even if none perfectly reproduces the distances between every pair of points. Mathematically, these rotations are a linear transformation of the original variables into a new three-dimensional space composed of the width, the height and the depth of the screen; the variation according to this third dimension (depth) is of course hidden in the visualization.</p>
<p>Consider the extreme case where we have three variables <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span> and one of them is perfectly predicted by the other two (perfect collinearity), for example <span class="math inline">\(z = 2x - 5y\)</span>. The set of points is located on a single plane, but the axes of this plane correspond to a linear combination of the original variables. A rotation would align this plane with the first two dimensions of the visualization (width and height). In this case, since the data do not vary in the third dimension, the projection perfectly represents the variation of the original data.</p>
<div id="htmlwidget-e15f931f3cfd12972aa5" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-e15f931f3cfd12972aa5">{"x":{"visdat":{"23fc6c3e5c50":["function () ","plotlyVisDat"]},"cur_data":"23fc6c3e5c50","attrs":{"23fc6c3e5c50":{"x":{},"y":{},"z":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","size":1,"inherit":true},"23fc6c3e5c50.1":{"x":{},"y":{},"z":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"mesh3d","opacity":0.3,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x"},"yaxis":{"title":"y"},"zaxis":{"title":"z"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":[{"name":"Collaborate","icon":{"width":1000,"ascent":500,"descent":-50,"path":"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z"},"click":"function(gd) { \n        // is this being viewed in RStudio?\n        if (location.search == '?viewer_pane=1') {\n          alert('To learn about plotly for collaboration, visit:\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\n        } else {\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\n        }\n      }"}],"cloud":false},"data":[{"x":[-0.917668905417233,-0.242027669731013,0.220468155909859,0.132223198772399,-0.0801826240872043,0.697772513437718,-0.493475191496335,-0.928008488028086,0.790673156145411,-1.0573202494795,0.125293298305138,0.217088676119253,0.42025635087694,-0.741742541441975,-0.284784910857411,0.751040463704959,-0.403565851441859,-0.382124993981644,-0.882724900440829,-0.00529909226410351,-1.59585766074285,-0.624214884266681,0.185084127155323,-0.271315034658679,0.803816937125465,-0.200099682960448,-2.7460867608478,-2.09993030220045,0.96240846511266,-2.05125592122423,-0.642494593416522,-0.104299544084228,-0.537364282551025,-0.155902486046222,-0.492901149326643,-1.33886055242231,-0.577768453074321,1.20853966030566,0.456836354728771,0.55129980897302,-2.09297617075183,-1.73381286577643,-0.848027022430577,-0.780167385129148,0.692413048364599,0.190847734078548,-0.61680201398586,-0.314987313651069,1.76173403789391,0.96530225487169,-0.817738866283024,-0.490464110624922,1.24635354289307,0.607355595763656,-0.000916563626775591,0.136200382424348,0.102744100870647,-1.20203308548323,-0.73354432835259,0.642164521434812,0.490684821789338,-1.64612133979479,0.878398588901623,-0.894771907244679,-0.0357140002124425,-0.920707885522863,-0.790310522403883,1.039707258504,-1.5632722243679,-0.397697612318531,0.3645428039074,-0.5292655845213,0.194756075481029,0.248749875513929,0.208736022280922,-0.0311523298276034,-1.2719969662532,0.872653592662823,0.911192546431464,-0.574366156641821,-0.406222320770657,0.962199107596357,-0.00232216159481647,-2.11179044674658,-0.283425486647047,0.213358020434755,-0.288854044635076,1.32046284429722,0.276285253063004,-0.176772708892158,1.27243545901189,-0.676523278744296,-0.349704842082357,-0.188213363301288,0.244584206486939,-0.000430097965441949,0.649110473528029,0.23231315582629,-0.607009658594306,-0.465985239733559],"y":[-1.77588926150559,0.586407456620499,0.786469539985776,-0.482083242467127,1.29726120501846,0.251993738950341,2.25245403042559,0.773139988267253,-0.432322800418394,1.37414069762367,0.127422255550887,0.404143551152643,0.166121429818174,0.701044825506016,0.331259785479933,1.15645144506552,-1.6157505973623,-1.20681916747878,1.39899255509144,-0.351246383999285,0.531509916248753,-0.748990665027133,-1.67041844006074,1.26306714792368,1.5739130124365,1.32564425913293,-1.94830217477948,0.470699656520793,2.07593338854915,-1.8341718000882,0.552229914391009,0.312310724207507,0.175328137702862,-1.15258988843625,-1.58445382653703,-1.84376774587098,0.0154820277710508,0.328102876698759,-0.448112295831684,-0.541149734430787,1.97756868224151,0.00111650905242889,1.92347050196244,0.0887209491091299,0.588504202719538,0.804645021914718,-0.630203112267561,-0.374757773241637,-1.45573983798172,1.59998350276793,-2.61337841580196,1.26884323429995,0.945912434069485,-0.572078660927816,0.838339957514559,1.20200879184744,1.62147619096019,-0.402957204994025,-0.625465022311998,0.589473322304419,1.07395985050296,-0.0391169073096044,0.313847157660855,0.351306024730818,-0.71954477742685,1.27262926681548,-1.89990551510143,-0.649987265793217,-0.0423313902267518,0.146121628469731,-0.103212836105283,0.661165422350786,-0.554302192595557,0.465722144313833,0.466068547440708,0.14781508575693,-0.310427255596172,-2.18475022904281,-1.09614580535194,0.558696411099598,-0.749638112141526,0.292203715868165,0.0681955149638259,0.652407577658241,0.268993345134735,0.146794606669299,1.1453775047716,1.12815969835439,-0.169431411900348,0.242287709160378,0.916853667087782,0.399688204363963,-0.735831185632259,0.102529054587654,-1.11777761435963,-0.856492915031003,-1.56611561774516,0.871976300909086,0.461769046614601,-0.882586709023853],"z":[7.0441084966935,-3.41609262256452,-3.49141138810916,2.67486260988043,-6.64667127326669,0.13557633212373,-12.2492205351206,-5.72171691739244,3.74296031438279,-8.98534398707733,-0.386524681144159,-1.58654040352471,0.00990555266300974,-4.98870921041403,-2.22586874911449,-4.28017629791767,7.27162128392776,5.26984584943063,-8.76041257633885,1.74563373546822,-5.84926490272946,2.49652355660231,8.72226045461435,-6.85796580893577,-6.26193118793157,-7.02842066158555,4.24933735220177,-6.55335888700487,-8.45485001252043,5.06834715799254,-4.04613875878809,-1.77015270920599,-1.95136925361636,5.45114447008879,6.93646683403186,6.54111762451028,-1.2329470450039,0.776564937117536,3.15423418861596,3.80834829009997,-14.0737957527112,-3.47320827681501,-11.3134065546733,-2.00393951580395,-1.55769491686849,-3.64152964141649,1.91741153336609,1.24381423890605,10.8021672656964,-6.06931300409629,11.4314143464438,-7.32514439274961,-2.23685508456128,4.07510449616639,-4.19353291482635,-5.7376431943885,-7.90189275305964,-0.389280145996328,1.66023645485481,-1.66303756865247,-4.3884296089361,-3.09665814304156,0.187561389498973,-3.54607393814345,3.52629588670936,-8.20456210512313,7.91890653069938,5.32935084597409,-2.91488749760205,-1.52600336698572,1.24514978834121,-4.36435828079653,3.16102311393984,-1.83111097054131,-1.9128706926417,-0.801380088439858,-0.99185765452555,12.6690583305397,7.30311411962261,-3.94221436878163,2.93574591916631,0.463379635851892,-0.345621898008762,-7.48561878178438,-1.91181769896777,-0.307256992476985,-6.30459561312816,-2.99987280317751,1.39972756562775,-1.5649839635862,-2.03939741741514,-3.3514875793084,2.97974624399658,-0.889071999540845,6.07805648477203,4.28160437922413,9.12879903578184,-3.89525519289285,-3.52286455026161,3.48096306565214],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","size":[55],"sizemode":"area","line":{"color":"rgba(31,119,180,1)"}},"textfont":{"size":55},"error_y":{"color":"rgba(31,119,180,1)","width":55},"error_x":{"color":"rgba(31,119,180,1)","width":55},"line":{"color":"rgba(31,119,180,1)","width":55},"frame":null},{"colorbar":{"title":"z<br />z","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.200112466686993","rgba(65,68,134,1)"],["0.247852960927489","rgba(60,81,138,1)"],["0.279032170652505","rgba(57,89,140,1)"],["0.303427105201178","rgba(54,96,141,1)"],["0.353749127291012","rgba(48,109,142,1)"],["0.373587965735146","rgba(47,113,142,1)"],["0.393218682602942","rgba(45,118,142,1)"],["0.39852526947831","rgba(44,119,142,1)"],["0.420436138631186","rgba(42,124,142,1)"],["0.451821588108719","rgba(38,132,142,1)"],["0.45864753597308","rgba(38,134,141,1)"],["0.467880364359481","rgba(38,136,141,1)"],["0.491577267393632","rgba(37,142,140,1)"],["0.512957521894052","rgba(36,147,139,1)"],["0.533034318837574","rgba(34,152,138,1)"],["0.572823883844422","rgba(36,162,135,1)"],["0.600668733373618","rgba(42,168,131,1)"],["0.639317102416927","rgba(49,177,125,1)"],["0.661161323498077","rgba(52,182,122,1)"],["0.685763253291281","rgba(66,187,116,1)"],["0.728391500515156","rgba(89,195,104,1)"],["0.788659047687383","rgba(117,207,86,1)"],["0.848661735811156","rgba(157,216,64,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[-0.917668905417233,-0.242027669731013,0.220468155909859,0.132223198772399,-0.0801826240872043,0.697772513437718,-0.493475191496335,-0.928008488028086,0.790673156145411,-1.0573202494795,0.125293298305138,0.217088676119253,0.42025635087694,-0.741742541441975,-0.284784910857411,0.751040463704959,-0.403565851441859,-0.382124993981644,-0.882724900440829,-0.00529909226410351,-1.59585766074285,-0.624214884266681,0.185084127155323,-0.271315034658679,0.803816937125465,-0.200099682960448,-2.7460867608478,-2.09993030220045,0.96240846511266,-2.05125592122423,-0.642494593416522,-0.104299544084228,-0.537364282551025,-0.155902486046222,-0.492901149326643,-1.33886055242231,-0.577768453074321,1.20853966030566,0.456836354728771,0.55129980897302,-2.09297617075183,-1.73381286577643,-0.848027022430577,-0.780167385129148,0.692413048364599,0.190847734078548,-0.61680201398586,-0.314987313651069,1.76173403789391,0.96530225487169,-0.817738866283024,-0.490464110624922,1.24635354289307,0.607355595763656,-0.000916563626775591,0.136200382424348,0.102744100870647,-1.20203308548323,-0.73354432835259,0.642164521434812,0.490684821789338,-1.64612133979479,0.878398588901623,-0.894771907244679,-0.0357140002124425,-0.920707885522863,-0.790310522403883,1.039707258504,-1.5632722243679,-0.397697612318531,0.3645428039074,-0.5292655845213,0.194756075481029,0.248749875513929,0.208736022280922,-0.0311523298276034,-1.2719969662532,0.872653592662823,0.911192546431464,-0.574366156641821,-0.406222320770657,0.962199107596357,-0.00232216159481647,-2.11179044674658,-0.283425486647047,0.213358020434755,-0.288854044635076,1.32046284429722,0.276285253063004,-0.176772708892158,1.27243545901189,-0.676523278744296,-0.349704842082357,-0.188213363301288,0.244584206486939,-0.000430097965441949,0.649110473528029,0.23231315582629,-0.607009658594306,-0.465985239733559],"y":[-1.77588926150559,0.586407456620499,0.786469539985776,-0.482083242467127,1.29726120501846,0.251993738950341,2.25245403042559,0.773139988267253,-0.432322800418394,1.37414069762367,0.127422255550887,0.404143551152643,0.166121429818174,0.701044825506016,0.331259785479933,1.15645144506552,-1.6157505973623,-1.20681916747878,1.39899255509144,-0.351246383999285,0.531509916248753,-0.748990665027133,-1.67041844006074,1.26306714792368,1.5739130124365,1.32564425913293,-1.94830217477948,0.470699656520793,2.07593338854915,-1.8341718000882,0.552229914391009,0.312310724207507,0.175328137702862,-1.15258988843625,-1.58445382653703,-1.84376774587098,0.0154820277710508,0.328102876698759,-0.448112295831684,-0.541149734430787,1.97756868224151,0.00111650905242889,1.92347050196244,0.0887209491091299,0.588504202719538,0.804645021914718,-0.630203112267561,-0.374757773241637,-1.45573983798172,1.59998350276793,-2.61337841580196,1.26884323429995,0.945912434069485,-0.572078660927816,0.838339957514559,1.20200879184744,1.62147619096019,-0.402957204994025,-0.625465022311998,0.589473322304419,1.07395985050296,-0.0391169073096044,0.313847157660855,0.351306024730818,-0.71954477742685,1.27262926681548,-1.89990551510143,-0.649987265793217,-0.0423313902267518,0.146121628469731,-0.103212836105283,0.661165422350786,-0.554302192595557,0.465722144313833,0.466068547440708,0.14781508575693,-0.310427255596172,-2.18475022904281,-1.09614580535194,0.558696411099598,-0.749638112141526,0.292203715868165,0.0681955149638259,0.652407577658241,0.268993345134735,0.146794606669299,1.1453775047716,1.12815969835439,-0.169431411900348,0.242287709160378,0.916853667087782,0.399688204363963,-0.735831185632259,0.102529054587654,-1.11777761435963,-0.856492915031003,-1.56611561774516,0.871976300909086,0.461769046614601,-0.882586709023853],"z":[7.0441084966935,-3.41609262256452,-3.49141138810916,2.67486260988043,-6.64667127326669,0.13557633212373,-12.2492205351206,-5.72171691739244,3.74296031438279,-8.98534398707733,-0.386524681144159,-1.58654040352471,0.00990555266300974,-4.98870921041403,-2.22586874911449,-4.28017629791767,7.27162128392776,5.26984584943063,-8.76041257633885,1.74563373546822,-5.84926490272946,2.49652355660231,8.72226045461435,-6.85796580893577,-6.26193118793157,-7.02842066158555,4.24933735220177,-6.55335888700487,-8.45485001252043,5.06834715799254,-4.04613875878809,-1.77015270920599,-1.95136925361636,5.45114447008879,6.93646683403186,6.54111762451028,-1.2329470450039,0.776564937117536,3.15423418861596,3.80834829009997,-14.0737957527112,-3.47320827681501,-11.3134065546733,-2.00393951580395,-1.55769491686849,-3.64152964141649,1.91741153336609,1.24381423890605,10.8021672656964,-6.06931300409629,11.4314143464438,-7.32514439274961,-2.23685508456128,4.07510449616639,-4.19353291482635,-5.7376431943885,-7.90189275305964,-0.389280145996328,1.66023645485481,-1.66303756865247,-4.3884296089361,-3.09665814304156,0.187561389498973,-3.54607393814345,3.52629588670936,-8.20456210512313,7.91890653069938,5.32935084597409,-2.91488749760205,-1.52600336698572,1.24514978834121,-4.36435828079653,3.16102311393984,-1.83111097054131,-1.9128706926417,-0.801380088439858,-0.99185765452555,12.6690583305397,7.30311411962261,-3.94221436878163,2.93574591916631,0.463379635851892,-0.345621898008762,-7.48561878178438,-1.91181769896777,-0.307256992476985,-6.30459561312816,-2.99987280317751,1.39972756562775,-1.5649839635862,-2.03939741741514,-3.3514875793084,2.97974624399658,-0.889071999540845,6.07805648477203,4.28160437922413,9.12879903578184,-3.89525519289285,-3.52286455026161,3.48096306565214],"type":"mesh3d","opacity":0.3,"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"base_url":"https://plot.ly"},"evals":["config.modeBarButtonsToAdd.0.click"],"jsHooks":[]}</script>
<p>It is rare for variables to be perfectly collinear, so the dimensionality reduction will always be associated with a loss of information; the goal of ordination is to specifically choose the new axes in order to minimize the variation of the data in the dimensions that will be eliminated, or in an equivalent way, to maximize the variation “explained” by the selected axes.</p>
</div>
<div id="how-principal-component-analysis-works" class="section level2">
<h2>How principal component analysis works</h2>
<p>Consider a multivariate data set <span class="math inline">\(X\)</span>, a matrix of <span class="math inline">\(n\)</span> observations of <span class="math inline">\(p\)</span> variables, and assume that these variables have been previously standardized. Principal component analysis (PCA) is a linear transformation of the matrix <span class="math inline">\(X\)</span> (equivalent to a rotation in <span class="math inline">\(p\)</span> dimensions) determined as follows:</p>
<ul>
<li>the first axis (first new variable) is chosen so as to maximize the variance of the data along this axis;</li>
<li>the second axis is chosen so as to maximize the variance of the data provided that it is orthogonal (perpendicular) to the first axis;</li>
<li>the third axis is chosen so as to maximize the variance of the data provided that it is orthogonal to the two previous ones;</li>
<li>and so on.</li>
</ul>
<p>The transformed variables, or principal components, obtained by PCA thus have the property of being orthognal to each other (which means in particular that their correlation is 0) and of being listed in descending order of explained variance.</p>
<p>Mathematically, PCA requires the calculation of the eigenvalues and eigenvectors of the covariance matrix of <span class="math inline">\(X\)</span>, <span class="math inline">\(C_X\)</span>. An element of this matrix <span class="math inline">\(C_{X ~ i,j}\)</span> corresponds to the covariance between the columns (variables) <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> of <span class="math inline">\(X\)</span>. The eigenvectors indicate the direction of the principal components whereas the eigenvalues indicate the variance of the data according to each principal component.</p>
<p>The development of PCA is based on the theory of multivariate normal distributions and the method works best when the variables approach normality. In practice, it is enough to transform very asymmetric variables in order to avoid that certain extreme values have an excessive influence on the analysis.</p>
<p>In the next class, we will see ordination methods that are adapted to the species presence-absence and abundance data common in ecology.</p>
</div>
<div id="pca-with-r" class="section level2">
<h2>PCA with R</h2>
<p>There are several functions to perform PCA in R. In this course, we will use the <code>prcomp</code> function.</p>
<p><strong>Note</strong>: If we had not standardized the variables beforehand, we can do it directly in the <code>prcomp</code> function with the argument <code>scale. = TRUE</code>.</p>
<pre class="r"><code>clim_acp &lt;- prcomp(clim_norm)
summary(clim_acp)</code></pre>
<pre><code>## Importance of components:
##                           PC1    PC2    PC3     PC4     PC5     PC6
## Standard deviation     1.9785 1.5741 0.9774 0.90767 0.75003 0.46655
## Proportion of Variance 0.4349 0.2753 0.1061 0.09154 0.06251 0.02418
## Cumulative Proportion  0.4349 0.7102 0.8164 0.90792 0.97042 0.99461
##                            PC7     PC8     PC9
## Standard deviation     0.20003 0.08141 0.04355
## Proportion of Variance 0.00445 0.00074 0.00021
## Cumulative Proportion  0.99905 0.99979 1.00000</code></pre>
<p>The summary of PCA results first shows the standard deviation of each of the 9 principal components (rotation of the original variables). These values are also stored in the <code>sdev</code> element of the result (<code>clim_acp$sdev</code>). The next two rows indicate first the ratio between the variance of a component and the total variance (sum of the variances of the 9 variables), then the cumulative proportion.</p>
<p>The variances associated with each principal component can be visualized with the <code>screeplot</code> function.</p>
<pre class="r"><code>screeplot(clim_acp)</code></pre>
<p><img src="13E-Multivariate_analysis_Part1_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The transformation between the original (standardized) variables and the principal components is given by the <code>rotation</code> element.</p>
<pre class="r"><code>round(clim_acp$rotation, 2)</code></pre>
<pre><code>##          PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9
## t_mean  0.47 -0.11  0.11 -0.27  0.19 -0.01  0.08  0.80  0.03
## t_diu  -0.09 -0.46  0.05 -0.35 -0.78  0.06 -0.06  0.07 -0.19
## t_sd   -0.38 -0.16 -0.58 -0.08  0.30  0.00 -0.01  0.20 -0.60
## t_max   0.27 -0.37 -0.33 -0.51  0.30  0.21 -0.06 -0.41  0.34
## t_min   0.46  0.07  0.36 -0.12  0.13  0.03 -0.18 -0.34 -0.69
## p_ann   0.33  0.37 -0.39 -0.01 -0.29  0.24  0.66 -0.08 -0.11
## p_max   0.39  0.15 -0.48  0.30 -0.25  0.00 -0.66  0.08  0.06
## p_min  -0.09  0.49 -0.09 -0.57 -0.08 -0.63 -0.10 -0.04  0.04
## p_cv    0.27 -0.45 -0.13  0.33  0.00 -0.71  0.29 -0.14 -0.02</code></pre>
<p>For example, the first column of this matrix tells us that the first principal component is calculated by the equation:</p>
<p><span class="math display">\[ PC1 = 0.47 T_{mean} - 0.09 T_{diu} - 0.38 T_{sd} + 0.27 T_{max} + 0.46 T_{min} + 
0.33 P_{ann} + 0.39 P_{max} - 0.09 P_{min} + 0.27 P_{cv} \]</span></p>
<p>Since the variables have been standardized beforehand, the magnitude of these coefficients indicates the importance of the association between a variable and the given principal component, while the sign indicates the direction of this association. In this case:</p>
<ul>
<li><p>the first principal component is associated with high values of the mean temperature and the minimum temperature;</p></li>
<li><p>the second component is primarily associated with a high minimum precipitation, a low diurnal variation of temperature and a small variation in precipitation.</p></li>
</ul>
<p>Conversely, the rows of the rotation matrix indicate how a unit increase of each original variable is “decomposed” on the different axes of the principal components.</p>
<p>The <code>x</code> element of the result gives the coordinates of each observation on the axes of the principal components:</p>
<pre class="r"><code>round(head(clim_acp$x), 2)</code></pre>
<pre><code>##            PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9
## Tokyo    -0.83  1.14 -1.07 -0.37  0.48  0.15  0.29 -0.03  0.01
## Shanghai -1.00  0.71 -1.00 -0.30  0.99  0.05  0.04  0.03 -0.01
## Mumbai    4.39  0.53 -2.00  1.97 -0.49 -0.24 -0.58  0.04  0.00
## Karachi   0.80 -2.03  0.81  0.09  1.04 -0.78  0.14  0.20  0.08
## Delhi     1.22 -2.75 -1.11 -0.58  0.05 -0.48 -0.12  0.01  0.06
## Manila    3.30  0.64 -0.51  0.48 -0.02  0.52  0.17 -0.08 -0.06</code></pre>
</div>
<div id="visualization-of-pca-results" class="section level2">
<h2>Visualization of PCA results</h2>
<p>In the previous section, we saw that each observation is associated with coordinates on the 9 principal component axes. We have also seen that a unit increase of one of the original variables is associated with a variation on each axis.</p>
<p>The biplot simultaneously displays the coordinates of the observations and the vectors corresponding to the original variables in the space of the principal components. This graph usually shows the first two components, which account for most of the total variance.</p>
<pre class="r"><code>biplot(clim_acp)</code></pre>
<p><img src="13E-Multivariate_analysis_Part1_files/figure-html/unnamed-chunk-12-1.png" width="576" /></p>
<p>The biplot can help us interpret the principal components. For example:</p>
<ul>
<li><p>The cities with the coldest climate (e.g. St. Petersburg, Montreal) are concentrated on the left of the graph, a little above the median, in the direction opposite to vectors associated with a higher minimum, mean and maximum temperature .</p></li>
<li><p>The cities on the right (e.g. Mumbai, Manila) are characterized by high temperatures all year (high <code>t_min</code>) and a high precipitation in the wettest month (<code>p_max</code>).</p></li>
<li><p>The city of Singapore is set apart from the others, in particular because of large amount of rain all year round (high <code>p_ann</code> and<code>p_min</code>).</p></li>
<li><p>The cities at the bottom of the graph have a high maximum temperature, at least one very dry season (low <code>p_min</code>) and a large daily temperature variation.</p></li>
</ul>
<p>Since the first two components account for only 70% of the total variance, it is to be expected that they do not fully represent the climatic “distance” between cities. For example, the city with the lowest annual rainfall (Lima) is at the center of the graph rather than near other arid climates; this is due to the fact that temperature variations are less pronounced in this city.</p>
<p>The <code>choices</code> argument of <code>biplot</code> allows us to visualize other components than the first two.</p>
<pre class="r"><code>biplot(clim_acp, choices = c(1, 3))</code></pre>
<p><img src="13E-Multivariate_analysis_Part1_files/figure-html/unnamed-chunk-14-1.png" width="576" /></p>
<p>To interpret the third principal component, we can focus on the two cities at the extremes: Seoul and Lima were both near the center of the graph of components 1 and 2. In that graph, the temperature seasonality (<code>t_sd</code>) and the amount of precipitation (<code>p_ann</code> and <code>p_max</code>) pointed in opposite directions, because generally, the temperature varies less in cities with more precipitation. On the third principal component, these two variables are both associated with negative values, so this component makes it possible to contrast the cities with heavy precipitation and a large temperature variation (Seoul) from cities with low precipitation and a small temperature variation (Lima).</p>
</div>
<div id="how-many-principal-components" class="section level2">
<h2>How many principal components?</h2>
<p>Various criteria have been proposed to determine how many principal components are sufficient to describe muiltivariate data. In other words, which major components count for a sufficiently large portion of the total variance?</p>
<p>Here are the variances (square of the standard deviations) associated with the principal components in our example:</p>
<pre class="r"><code>clim_acp$sdev^2</code></pre>
<pre><code>## [1] 3.914417064 2.477765395 0.955214474 0.823859308 0.562545790 0.217664459
## [7] 0.040010038 0.006626862 0.001896610</code></pre>
<p>For standardized data, the sum of the variances is always equal to the number of variables (here, 9) and the mean of the variances is therefore 1. One of the simplest criteria therefore proposes to keep the components with a variance greater than 1. However, if all the variances are close to 1, this would mean that the original variables are almost independent, so that the PCA is not very useful.</p>
<p>Other criteria are based on more precise statistical tests, such as those evaluated in the article by Peres-Neto et al. (2005)</p>
<p>In practice, formal hypothesis tests are rarely necessary. For an exploratory data analysis, we always visualize the first two components, then we can inspect the following ones if they contribute to a non-negligible portion of the variance and are easily interpretable. For the use of principal components in a regression (see below), model selection methods can guide us on the number of principal components to be retained in the regression itself.</p>
</div>
<div id="pca-and-regression" class="section level2">
<h2>PCA and regression</h2>
<p>As mentioned above, it is difficult to estimate the parameters of a regression if the potential predictors are numerous and correlated. By transforming these predictors into a smaller number of uncorrelated variables, PCA can solve both of these problems.</p>
<p>The main disadvantage of this approach (PCA followed by a regression based on the principal components) is that the effects are generally more difficult to interpret according to the original variables, especially if several variables contribute substantially to each axis.</p>
<p>In some studies, the primary goal is to predict the response and there is little interest in the effect of individual predictors. For example, consider a project to identify dominant forest species from hyperspectral remote sensing imagery containing reflectance data for hundreds of wavelengths. In this case, multivariate methods such as PCA are used to reduce this reflectance spectrum to a smaller number of orthogonal components.</p>
</div>
</div>
<div id="agglomerative-hierarchical-clustering" class="section level1">
<h1>Agglomerative hierarchical clustering</h1>
<p>Agglomerative hierarchical clustering aims to create groups of similar observations according to a series of variables. The name comes from the fact that we start with the individual observations, which are progressively joined into groups until we obtain a single group. The resulting structure is a tree or <em>dendrogram</em> showing proximity relationships among all observations.</p>
<div id="distance-matrix" class="section level2">
<h2>Distance matrix</h2>
<p>The hierarchical clustering algorithm requires a matrix of the distance <span class="math inline">\(d_{ij}\)</span> for each pair of observations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p>
<p><span class="math display">\[  
\begin{bmatrix}
0 \\
d_{21} &amp; 0  \\
d_{31} &amp; d_{32} &amp; 0 \\
... \\
d_{n1} &amp; d_{n2} &amp; ... &amp; d_{n(n-1)} &amp; 0
\end{bmatrix}
\]</span></p>
<p>It is a <span class="math inline">\(n \times n\)</span> matrix where all diagonal values are zero (each observation is at a distance 0 of itself). Since the matrix is symmetric (<span class="math inline">\(d_{ji} = d_{ij}\)</span>), we can omit the values of the upper triangle in the representation.</p>
<p>For numerical variables, the most commonly used distance is the <em>Euclidean distance</em>, calculated as:</p>
<p><span class="math display">\[ d_{ij} = \sqrt{\sum_{k = 1}^p (x_{ik} - x_{jk})^2} \]</span></p>
<p>In two dimensions and replacing <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> with <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, this equation simplifies to:</p>
<p><span class="math display">\[  d_{ij} = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2} \]</span></p>
<p>This is the distance in a straight line between the two points calculated from the Pythagorean theorem. The function above is therefore the generalization of this distance to <span class="math inline">\(p\)</span> dimensions.</p>
<p>In R, the <code>dist</code> function calculates the distance matrix between the rows of a numeric data matrix. This function uses the Euclidean distance by default.</p>
<p>As in the case of PCA, it is recommended to standardize the variables before calculating the distance matrix, in order to place them on a comparable scale.</p>
</div>
<div id="agglomerative-hierarchical-clustering-algorithm" class="section level2">
<h2>Agglomerative hierarchical clustering algorithm</h2>
<p>Suppose we have the following distance matrix between 4 observations.</p>
<p><span class="math display">\[
\begin{array}
&amp;&amp; A &amp; B &amp; C \\
B &amp; 11 \\
C &amp; 13 &amp; 6 \\ 
D &amp; 7 &amp; 11 &amp; 9 
\end{array}
\]</span></p>
<ol style="list-style-type: decimal">
<li><p>The algorithm first joins the two nearest observations, B and C (distance of 6) into a group.</p></li>
<li><p>Next, replace B and C with a single observation BC and calculate the distance between this group and each of the other existing observations.</p></li>
</ol>
<p>How is the distance of the new group determined from the previous distances? Different rules are possible. For now, let’s use a rule where the distance from a new group to an existing observation is the mean of the distances between each group member and the observation.</p>
<p><span class="math display">\[
\begin{array}
&amp; &amp; A &amp; BC \\
BC &amp; 12 &amp; \\
D &amp; 7 &amp; 10 
\end{array}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>From that moment, the new group (BC) is treated as if it were an individual observation. Then just repeat steps 1 and 2 until there are only two groups, which are joined in the last step.</li>
</ol>
<p>In the example, the next step would be to group A and D (distance of 7). Finally, AD and BC are joined at a distance of 11 (mean of 10 and 12).</p>
</div>
<div id="hierarchical-clustering-with-r" class="section level2">
<h2>Hierarchical clustering with R</h2>
<p>The <code>hclust</code> function is used to perform agglomerative hierarchical clustering. Here is how we can apply it to the table of 9 climatic variables for 49 major cities. First, we compute the distance matrix, then perform clustering with the “average” method (the different methods will be presented later).</p>
<pre class="r"><code>clim_dist &lt;- dist(clim_norm)
clim_ch &lt;- hclust(clim_dist, method = &quot;average&quot;)
plot(clim_ch)</code></pre>
<p><img src="13E-Multivariate_analysis_Part1_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>The “leaves” at the bottom of the dendrogram correspond to the individual observations. A horizontal line represents the grouping of two existing observations or groups. The scale to the left of the dendrogram (<code>Height</code>) indicates the distance corresponding to each grouping. As the distance increases, the number of groups decreases and the size of the groups increases.</p>
<p>The <code>method</code> argument indicates how to calculate the distance between groups:</p>
<ul>
<li><p>single linkage (<code>method = "single"</code>): the distance between two groups A and B corresponds to the minimal distance between an observation in A and an observation in B.</p></li>
<li><p>complete linkage (<code>method = "complete"</code>): the distance between A and B corresponds to the maximum distance between an observation in A and an observation in B.</p></li>
<li><p>average linkage (<code>method = "average"</code>): the distance between A and B corresponds to the mean of the distances over all pairs of observations where one is in A and other one in B (this is the method shown in the previous example).</p></li>
<li><p>Ward’s method (<code>method = "ward.D2"</code>) consists of choosing the grouping which minimizes the increase of the total intra-group variance.</p></li>
</ul>
<p>Let’s first contrast the results of single and complete linkage:</p>
<pre class="r"><code>clim_ch_min &lt;- hclust(clim_dist, method = &quot;single&quot;)
plot(clim_ch_min)</code></pre>
<p><img src="13E-Multivariate_analysis_Part1_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>clim_ch_max &lt;- hclust(clim_dist, method = &quot;complete&quot;)
plot(clim_ch_max)</code></pre>
<p><img src="13E-Multivariate_analysis_Part1_files/figure-html/unnamed-chunk-17-2.png" width="672" /></p>
<p>For the single linkage criterion, the more observations a group contains, the closer it potentially gets to other groups and individual observations, since the distance is based on the nearest observation among all those in the group. This is why this method tends to create large groups to which observations join one at a time. The opposite situation occurs for complete linkage: the larger the group, the more it can move away from other groups and observations. This is why this method tends to create small groups first before growing larger groups. Average linkage is a compromise between these two extremes.</p>
<p>Ward’s method is different since it involves minimizing the overall variance between observations of the same group. However, since the addition of a single observation far from the others may have a large effect on the variance, this method just like complete linkage tends to favor the smaller groups initially. These two methods make it possible to create more compact and balanced groups.</p>
<pre class="r"><code>clim_ch_ward &lt;- hclust(clim_dist, method = &quot;ward.D2&quot;)
plot(clim_ch_ward)</code></pre>
<p><img src="13E-Multivariate_analysis_Part1_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="extract-groups-from-hierarchical-clustering-results" class="section level2">
<h2>Extract groups from hierarchical clustering results</h2>
<p>The hierarchical clustering defines a different number of groups along the dendrogram. The <code>cutree</code> function allows us to “cut” the dendrogram (1) to a certain height by specifying the <em>h</em> argument or (2) to get a specific number of groups by specifying the <em>k</em> argument. This function returns a vector indicating the index of the group corresponding to each observation, in the same order as the data matrix.</p>
<p>So let’s use this method to extract the three large groups from the last dendrogram (Ward’s method):</p>
<pre class="r"><code>groupes &lt;- cutree(clim_ch_ward, k = 3)
head(groupes)</code></pre>
<pre><code>##    Tokyo Shanghai   Mumbai  Karachi    Delhi   Manila 
##        1        1        2        3        3        2</code></pre>
</div>
<div id="compare-groups-to-ordination-results" class="section level2">
<h2>Compare groups to ordination results</h2>
<p>We will finally represent the groups obtained by hierarchical clustering on the graph of the first two PCA axes. First, we create a new data frame by attaching the first two principal components and the group index vector, which must be specified as a factor.</p>
<pre class="r"><code>ord_groupes &lt;- data.frame(clim_acp$x[, 1:2], groupe_ward = as.factor(groupes))
head(ord_groupes)</code></pre>
<pre><code>##                 PC1        PC2 groupe_ward
## Tokyo    -0.8292567  1.1390831           1
## Shanghai -0.9964678  0.7094018           1
## Mumbai    4.3904533  0.5328498           2
## Karachi   0.8027417 -2.0305728           3
## Delhi     1.2169313 -2.7525580           3
## Manila    3.2953893  0.6395510           2</code></pre>
<pre class="r"><code>ggplot(ord_groupes, aes(x = PC1, y = PC2, color = groupe_ward)) +
    geom_point() +
    geom_text(aes(label = rownames(ord_groupes)))</code></pre>
<p><img src="13E-Multivariate_analysis_Part1_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Let’s now compare with four groups extracted from the complete linkage dendrogram:</p>
<pre class="r"><code>ord_groupes$groupe_max &lt;- as.factor(cutree(clim_ch_max, k = 4))

ggplot(ord_groupes, aes(x = PC1, y = PC2, color = groupe_max)) +
    geom_point() +
    geom_text(aes(label = rownames(ord_groupes)))</code></pre>
<p><img src="13E-Multivariate_analysis_Part1_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Besides the decision to place Singapore in a separate group or not, the differences between the two methods are found mostly in areas where groups overlap. From the graph, it is quite clear that the distribution of cities along the climatic axes is gradual rather than forming a few well-separated groups. In this case, ordination is a better way to condense information on the differences between the climates of different cities.</p>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<ul>
<li><p>Multivariate analyzes are used to describe and explain the joint distribution of several variables.</p></li>
<li><p>When variables represent different units, it is important to standardize them to obtain comparable scales for multivariate analysis.</p></li>
<li><p>Ordination aims to produce in a reduced number of dimensions (often 2) the most faithful representation possible of the variation between the multivariate observations.</p></li>
<li><p>Principal component analysis (PCA) is an ordination method that performs a linear transformation (rotation) of the original variables, in order to obtain principal components that are uncorrelated and ranked in descending order of variance. The coordinates of the observations and the original variables can be represented simultaneously in the principal component space with a biplot.</p></li>
<li><p>Clustering methods serve to classify multivariate observations in a small number of groups of observations that are as similar as possible. Agglomerative hierarchical clustering performs gradual grouping starting from the individual observations and according to different rules that define the distance between groups. This clustering method produces a dendrogram.</p></li>
</ul>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p><strong>Textbooks</strong></p>
<ul>
<li><p>Manly, B.F. and Alberto, J.A.N. (2016) <em>Multivariate statistical methods: a primer</em>. Chapman and Hall/CRC.</p></li>
<li><p>Legendre, P. and Legendre, L. (2012) <em>Numerical Ecology</em>, 3e éd. Elsevier.</p></li>
<li><p>Borcard, D., Gillet, F. and Legendre, P. (2018) <em>Numerical Ecology with R</em>, 2e éd. Springer.</p></li>
</ul>
<p>Criteria to pick the number of principal components:</p>
<ul>
<li>Peres-Neto, P.R., Jackson, D.A. and Somers, K.M. (2005) “How many principal components? stopping rules for determining the number of non-trivial axes revisited.” <em>Computational Statistics and Data Analysis</em> 49: 974-997.</li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
