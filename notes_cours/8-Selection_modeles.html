<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Évaluation et sélection de modèles</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Évaluation et sélection de modèles</h1>
<h4 class="date"><br/>26 octobre 2020</h4>

</div>


<div id="objectifs" class="section level1">
<h1>Objectifs</h1>
<ul>
<li><p>Identifier les problèmes dûs à la collinéarité de plusieurs prédicteurs.</p></li>
<li><p>Identifier les problèmes liés au sous-ajustement et surajustement de modèles.</p></li>
<li><p>Connaître les avantages et inconvénients de différentes méthodes visant à évaluer l’ajustement de modèles.</p></li>
<li><p>Utiliser l’AIC pour comparer différents modèles.</p></li>
<li><p>Combiner les prédictions de plusieurs modèles.</p></li>
</ul>
</div>
<div id="collinéarité" class="section level1">
<h1>Collinéarité</h1>
<p>Lors du dernier cours, nous avons utilisé le jeu de données <code>hills</code> présentant les records de temps (<em>time</em>, en minutes) pour des courses de vélo en Écosse en fonction de la distance horizontale (<em>dist</em>, en milles) et le dénivelé total du parcours (<em>climb</em>, en pieds).</p>
<pre class="r"><code>library(MASS)
str(hills)</code></pre>
<pre><code>## &#39;data.frame&#39;:    35 obs. of  3 variables:
##  $ dist : num  2.5 6 6 7.5 8 8 16 6 5 6 ...
##  $ climb: int  650 2500 900 800 3070 2866 7500 800 800 650 ...
##  $ time : num  16.1 48.4 33.6 45.6 62.3 ...</code></pre>
<p>Notons que les deux prédicteurs (<em>dist</em> et <em>climb</em>) sont eux-mêmes corrélés.</p>
<pre class="r"><code>cor(hills$dist, hills$climb)</code></pre>
<pre><code>## [1] 0.6523461</code></pre>
<p>La corrélation entre deux prédicteurs complique l’estimation des effets de chaque prédicteur. Puisque chaque coefficient du modèle représente l’effet d’un prédicteur lorsque les autres prennent la même valeur, lorsque plusieurs d’entre eux varient ensemble, il devient difficile d’isoler l’effet de chacun. Ce problème se généralise aux modèles avec plus de deux prédicteurs, si l’un des prédicteurs est corrélé avec une combinaison linéaire des autres prédicteurs: on parle alors de <strong>collinéarité</strong>.</p>
<p>La fonction <code>vif</code> du package <em>car</em> calcule le facteur d’inflation de la variance (VIF, pour <em>variance inflation factor</em>) de chaque prédicteur d’un modèle.</p>
<pre class="r"><code>library(car)
mod_hills &lt;- lm(time ~ dist + climb, hills)
vif(mod_hills)</code></pre>
<pre><code>##     dist    climb 
## 1.740812 1.740812</code></pre>
<p>Le VIF est égal à <span class="math inline">\(1 - 1/R^2\)</span>, où <span class="math inline">\(R^2\)</span> est le coefficient de détermination d’un modèle linéaire du prédicteur considéré en fonction de tous les autres. Par exemple, si un des prédicteurs peut être déterminé à partir de la valeur des autres avec un <span class="math inline">\(R^2\)</span> de 0.9, VIF = 10. Lorsque le VIF de certains prédicteurs dépasse 10, il est recommandé d’éliminer un des prédicteurs redondants.</p>
<div id="exemple" class="section level2">
<h2>Exemple</h2>
<p>Le tableau de données <code>msleep</code> inclus avec le package <em>ggplot</em>, contient des données sur le sommeil de différentes espèces de mammifères. Nous choisissons trois colonnes correspondant au temps de sommeil total (<em>sleep_total</em>), au poids de l’animal (<em>bodywt</em>) et au poids de son cerveau (<em>brainwt</em>).</p>
<pre class="r"><code>msleep2 &lt;- dplyr::select(msleep, sleep_total, bodywt, brainwt)
summary(msleep2)</code></pre>
<pre><code>##   sleep_total        bodywt            brainwt       
##  Min.   : 1.90   Min.   :   0.005   Min.   :0.00014  
##  1st Qu.: 7.85   1st Qu.:   0.174   1st Qu.:0.00290  
##  Median :10.10   Median :   1.670   Median :0.01240  
##  Mean   :10.43   Mean   : 166.136   Mean   :0.28158  
##  3rd Qu.:13.75   3rd Qu.:  41.750   3rd Qu.:0.12550  
##  Max.   :19.90   Max.   :6654.000   Max.   :5.71200  
##                                     NA&#39;s   :27</code></pre>
<p>Les variables <em>bodywt</em> et <em>brainwt</em> sont très asymétriques et varient sur plusieurs ordres de grandeur, donc nous effectuons une transformation logarithmique des trois variables.</p>
<pre class="r"><code>msleep2 &lt;- log(msleep2)</code></pre>
<p>Sur une échelle logarithmique, les variables <em>bodywt</em> et <em>brainwt</em> sont fortement corrélées.</p>
<pre class="r"><code>plot(msleep2)</code></pre>
<p><img src="8-Selection_modeles_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Voici ce qu’il arrive lorsqu’on estime un modèle linéaire avec un ou deux prédicteurs.</p>
<pre class="r"><code>summary(lm(sleep_total ~ bodywt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ bodywt, data = msleep2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9757 -0.2712 -0.0096  0.2727  1.0004 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.31575    0.04647  49.830  &lt; 2e-16 ***
## bodywt      -0.10265    0.01389  -7.388 1.19e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4097 on 81 degrees of freedom
## Multiple R-squared:  0.4026, Adjusted R-squared:  0.3952 
## F-statistic: 54.58 on 1 and 81 DF,  p-value: 1.193e-10</code></pre>
<pre class="r"><code>summary(lm(sleep_total ~ brainwt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ brainwt, data = msleep2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.89025 -0.27043 -0.01841  0.30664  0.88271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.69551    0.10037  16.892  &lt; 2e-16 ***
## brainwt     -0.12640    0.02103  -6.011 1.64e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3943 on 54 degrees of freedom
##   (27 observations deleted due to missingness)
## Multiple R-squared:  0.4009, Adjusted R-squared:  0.3898 
## F-statistic: 36.13 on 1 and 54 DF,  p-value: 1.637e-07</code></pre>
<pre class="r"><code>summary(lm(sleep_total ~ bodywt + brainwt, data = msleep2))</code></pre>
<pre><code>## 
## Call:
## lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.92817 -0.25572 -0.01949  0.28150  1.01779 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.317881   0.382108   6.066 1.42e-07 ***
## bodywt      -0.105861   0.062787  -1.686   0.0977 .  
## brainwt      0.002517   0.079212   0.032   0.9748    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3877 on 53 degrees of freedom
##   (27 observations deleted due to missingness)
## Multiple R-squared:  0.4314, Adjusted R-squared:  0.4099 
## F-statistic:  20.1 on 2 and 53 DF,  p-value: 3.181e-07</code></pre>
<p>Comparez les erreurs-types des coefficients du troisième modèle par rapport à ceux où chacun des prédicteurs est considéré séparément. Nous pouvons confirmer que ces variables sont fortement collinéaires avec <code>vif</code>.</p>
<pre class="r"><code>vif(lm(sleep_total ~ bodywt + brainwt, data = msleep2))</code></pre>
<pre><code>##   bodywt  brainwt 
## 14.67386 14.67386</code></pre>
<p>S’il faut ne conserver qu’une seule des deux variables du modèle, laquelle choisir?</p>
<ul>
<li><p>Nous pouvons utiliser nos connaissances préalables ou nos hypothèses sur le fonctionnement du système pour déterminer lequel des prédicteurs est le plus pertinent.</p></li>
<li><p>Nous pouvons comparer l’ajustement des modèles utilisant un prédicteur ou l’autre.</p></li>
</ul>
<p>Les méthodes vues dans le reste du cours répondent au besoin de comparer différents modèles expliquant la même variable réponse. Ils s’appliquent à tout cas où nous avons plusieurs modèles “candidats” pour un même problème.</p>
</div>
</div>
<div id="types-derreurs-des-modèles" class="section level1">
<h1>Types d’erreurs des modèles</h1>
<div id="exemple-1" class="section level2">
<h2>Exemple</h2>
<p><img src="../images/polatouche.jpg" /></p>
<p>Supposons que nous souhaitons estimer la densité de population du grand polatouche (<em>Glaucomys sabrinus</em>) en fonction de différentes variables, incluant:</p>
<ul>
<li>le type de peuplement (feuillu, résineux, mixte);</li>
<li>l’âge du peuplement;</li>
<li>les températures moyennes du mois le plus froid et le plus chaud;</li>
<li>la surface de forêt continue autour du peuplement;</li>
<li>la présence ou absence d’autres espèces de petits mammifères.</li>
</ul>
<p>Pour ce faire, nous avons des données d’un nombre limité de sites (ex.: 40 peuplements). Quels sont les avantages et désavantages: (1) d’un modèle incluant le nombre maximal de prédicteurs et (2) d’un modèle incluant un nombre réduit (ex.: 1 ou 2) prédicteurs? Quelle est la différence entre le type d’erreurs commises dans chaque cas?</p>
</div>
<div id="modèle-trop-simple-sous-ajustement-et-biais" class="section level2">
<h2>Modèle trop simple: sous-ajustement et biais</h2>
<p>Avec un modèle trop simple, nous risquons d’omettre des prédicteurs qui ont un effet sur la variable étudiée. On parle de sous-ajustement aux données (<em>underfitting</em>). Par exemple, supposons que le type de peuplement n’est pas inclus dans le modèle estimé, mais que le polatouche est davantage associé aux conifères. Dans ce cas, les prédictions du modèle vont sous-estimer sa densité de population dans les peuplements résineux et surestimer sa densité de population dans les peuplements de feuillus.</p>
<p>Puisque ce problème crée une erreur systématique et indépendante du nombre d’observations, il est souvent nommé <em>biais</em>.</p>
</div>
<div id="modèle-trop-complexe-surajustement-et-variance" class="section level2">
<h2>Modèle trop complexe: surajustement et variance</h2>
<p>En contrepartie, un modèle avec trop de paramètres risque de mesurer des associations entre variables qui ne sont pas des effets généralisables, mais plutôt des coïncidences du jeu de données utilisé. Dans ce cas, il y a surajustement du modèle aux données (<em>overfitting</em>). De façon générale, plus un modèle inclut de prédicteurs, plus l’échantillon doit être grand pour estimer les effets avec la même précision. D’un point de vue graphique, l’étendue combinée de <span class="math inline">\(m\)</span> prédicteurs crée une “surface” à <span class="math inline">\(m\)</span> dimensions; plus ce nombre de dimensions est élevé, plus il faut de points pour bien couvrir l’étendue des valeurs possibles (voir l’illustration ci-dessous).</p>
<p><img src="../images/densite_pred.png" /></p>
<p>L’erreur liée au surajustement est aléatoire (dépend des coïncidences de l’échantillonnage) et diminue en augmentant la taille de l’échantillon, donc elle est souvent nommée <em>variance</em>.</p>
</div>
<div id="autre-exemple-ajuster-une-fonction-polynomiale" class="section level2">
<h2>Autre exemple: ajuster une fonction polynomiale</h2>
<p>Au dernier cours sur la régression linéaire, nous avons donné l’exemple de la croissance d’une espèce de plante en fonction de l’humidité du sol. Les graphiques ci-dessous montrent l’ajustement de ces données par des régressions effectuées avec des polynôme de degré 1 (ligne droite), 2 et 4.</p>
<p><img src="8-Selection_modeles_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Si le modèle linéaire pourrait être sous-ajusté ici (il semble y avoir une courbure dans le nuage de points), le surajustement du modèle de degré 4 est évident dès qu’on dépasse un peu l’étendue des données.</p>
</div>
<div id="résumé-sources-derreur-et-complexité-des-modèles" class="section level2">
<h2>Résumé: Sources d’erreur et complexité des modèles</h2>
<ul>
<li><p>On ne connait pas la fonction exacte reliant la réponse du modèle aux prédicteurs.</p></li>
<li><p>Un modèle qui comporte davantage de paramètres ajustables approxime cette fonction avec moins de biais (erreur systématique), mais plus de variance (sensibilité aux variations aléatoires des données). L’erreur totale est minimisée à un certain nombre de paramètres.</p></li>
<li><p>Avec un échantillon plus grand, le biais reste le même, mais la variance diminue. Non seulement l’erreur totale diminue pour tous les modèles, mais le nombre optimal de paramètres (qui minimise l’erreur totale) augmente, comme le montre le graphique ci-dessous.</p></li>
</ul>
<p><img src="../images/biais_variance.png" /></p>
</div>
<div id="prédiction-et-explication" class="section level2">
<h2>Prédiction et explication</h2>
<p>Les critères présentés dans ce cours sont basés sur la capacité prédictive des modèles. Autrement dit, nous recherchons le modèle dont les prédictions s’approcheront le plus de la réalité pour de nouvelles observations, indépendantes des observations utilisées pour estimer les paramètres du modèle.</p>
<p>Il arrive souvent, particulièrement dans un contexte expérimental, que le but d’un modèle ne soit pas nécessairement de prédire la réponse, mais plutôt d’en identifier les causes (explication). Dans ce cas, le désir de limiter la complexité des modèles réflète le principe de parcimonie, ou “rasoir d’Ockham”, selon lequel il est préférable d’utiliser le minimum de causes nécessaires pour expliquer un phénomène.</p>
</div>
</div>
<div id="critères-de-comparaison-des-modèles" class="section level1">
<h1>Critères de comparaison des modèles</h1>
<p>Dans cette section, nous verrons différents critères statistiques permettant de comparer l’ajustement de différents modèles visant à estimer une même variable réponse.</p>
<div id="coefficient-de-détermination-r2" class="section level2">
<h2>Coefficient de détermination (<span class="math inline">\(R^2\)</span>)</h2>
<p>Le coefficient de détermination indique quelle fraction de la variance totale de la réponse est expliquée par le modèle. Dans R, le résultat de <code>lm</code> rapportent deux versions du <span class="math inline">\(R^2\)</span>, <em>Multiple R-squared</em> et <em>Adjusted R-squared</em>. La première correspond au <span class="math inline">\(R^2\)</span> que nous avons déjà vu, basé sur la somme des écarts carrés résiduels et totaux:</p>
<p><span class="math display">\[ R^2 = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y_i})^2}{\sum_{i=1}^n (y_i - \bar{y})^2} \]</span></p>
<p>Le <span class="math inline">\(R^2\)</span> “multiple” augmente toujours lorsqu’on ajoute un nouveau prédicteur au modèle, puisque chaque nouveau prédicteur a une corrélation (même minime et aléatoire) avec la réponse. Cette mesure est donc biaisée en faveur de modèles plus complexes.</p>
<p>La version ajustée du <span class="math inline">\(R^2\)</span> divise chaque somme des écarts carrés par le nombre de degrés de liberté approprié. Puisqu’un modèle avec plus de paramètres a moins de degrés de liberté résiduels, le <span class="math inline">\(R^2\)</span> ajusté peut diminuer si on ajoute un paramètre qui ne contribue pas suffisamment à expliquer la réponse. Il s’agit donc d’un estimateur moins biaisé du <span class="math inline">\(R^2\)</span> de la population.</p>
<p>L’une ou l’autre version du <span class="math inline">\(R^2\)</span> convient pour comparer deux modèles linéaires comprenant le même nombre de paramètres ajustables. Toutefois, le <span class="math inline">\(R^2\)</span> ajusté ne pénalise pas les modèles plus complexes autant que l’AIC qui sera présenté plus loin.</p>
</div>
<div id="tests-de-significativité" class="section level2">
<h2>Tests de significativité</h2>
<p>Si deux modèles ne diffèrent que par un prédicteur, ex.: <code>y ~ x1 + x2</code> et <code>y ~ x1 + x2 + x3</code>, on peut choisir le modèle plus complexe si l’effet du nouveau prédicteur <code>x3</code> est significatif. Nous avons déjà utilisé cette technique pour déterminer s’il fallait inclure ou non l’interaction dans un modèle d’ANOVA à deux facteurs.</p>
<p>Il existe des méthodes séquentielles (<em>stepwise</em>) pour choisir entre plus de deux modèles en ajoutant ou supprimant une variable à la fois. Ces méthodes seront présentées avec plus de détail plus loin. Pour l’instant, notons que la sélection de modèles basée sur les tests de significativité comporte plusieurs inconvénients.</p>
<ul>
<li><p>La valeur <span class="math inline">\(p\)</span> mesure la probabilité d’obtenir les résultats observés selon l’hypothèse nulle, pas le pouvoir prédictif du modèle. Elle nous aide donc à répondre à des questions comme “L’effet du prédicteur <span class="math inline">\(x_3\)</span> est-il significativement différent de zéro?”, mais pas: "Quel est le meilleur modèle entre <span class="math inline">\(y \sim x_1 + x_2\)</span> et <span class="math inline">\(y \sim x_3\)</span>?</p></li>
<li><p>Le seuil de significativité pour l’inclusion d’une variable (ex.: <span class="math inline">\(\alpha = 0.05\)</span>) est arbitraire.</p></li>
<li><p>Avec les méthodes séquentielles, il faut considérer l’augmentation de la probabilité d’erreur de type I (problème des comparaisons multiples).</p></li>
</ul>
</div>
<div id="ensemble-de-validation" class="section level2">
<h2>Ensemble de validation</h2>
<p>Comme nous avons vu plus tôt, un modèle avec plusieurs paramètres peut être surajusté aux données, c’est-à-dire qu’il détecte des associations fortuites entre variables du jeu de données utilisé, qui ne sont pas généralisables au-delà de cet échantillon.</p>
<p>Dans cette perspective, la meilleure façon d’évaluer le pouvoir prédictif d’un modèle est de mesurer son erreur de prédiction (par exemple, la somme des carrés des résidus: <span class="math inline">\(\sum (y_i - \hat{y_i})^2\)</span>) pour un jeu de données <em>différent</em> de celui utilisé pour estimer les paramètres du modèle. Par exemple, avec des mesures prises sur 40 sites, on peut estimer les paramèters à partir des donneés de 30 sites (choisis au hasard), puis évaluer l’erreur de prédiction du modèle sur les 10 sites restants.</p>
<p>Cette séparation des données en un ensemble pour la calibration des modèles (<em>training set</em>) et un ensemble pour leur validation (<em>validation set</em>) est utilisée assez fréquemment lorsqu’on dispose d’un très grand échantillon. Il est préférable de placer la plus grande partie des données dans l’ensemble de calibration, par exemple une répartition 70%/30% ou 80%/20%.</p>
<p>Pour des jeux de données plus petits, ce type de validation n’est pas efficace. D’une part, on peut moins se permettre de laisser de côté une partie de nos données pour la validation, et d’autre part, un petit ensemble de validation (ex.: 10 cas) n’est pas très représentatif.</p>
</div>
<div id="validation-croisée" class="section level2">
<h2>Validation croisée</h2>
<p>La validation croisée (<em>cross-validation</em>) consiste à diviser aléatoirement les observations en groupes et mesurer l’erreur de prédiction des observations d’un groupe selon un modèle ajusté au reste des observations.</p>
<p>Par exemple, si chaque groupe ne comporte qu’une seule observation (<em>leave-one-out cross-validation</em>), la somme des erreurs carrées de prédiction est égale à <span class="math inline">\(\sum (y_i - \hat{y_i})^2\)</span>, où chaque valeur prédite <span class="math inline">\(\hat{y_i}\)</span> provient du modèle ajusté sans l’observation <span class="math inline">\(i\)</span>. Si on utilise cette méthode pour comparer l’erreur de prédiction de différents modèles à partir de <span class="math inline">\(n\)</span> observations, chacun des modèles doit donc être ajusté <span class="math inline">\(n\)</span> fois, en laissant chaque fois une observation de côté.</p>
<p>Si le nombre d’observations <span class="math inline">\(n\)</span> est grand, il peut être plus pratique de diviser les observations en <span class="math inline">\(k\)</span> groupes (<em>k-fold cross-validation</em>), par exemple <span class="math inline">\(k\)</span> = 10, et d’ajuster chaque modèle à évaluer <span class="math inline">\(k\)</span> fois en laissant une fraction <span class="math inline">\(1/k\)</span> des observations de côté.</p>
<p>La validation croisée peut être codée à partir d’une boucle dans R, mais il existe aussi des packages où ces fonctions sont déjà codées et prêtes à être appliquées à divers modèles, comme le package <strong>cvTools</strong>.</p>
</div>
</div>
<div id="comparaison-de-modèles-avec-laic" class="section level1">
<h1>Comparaison de modèles avec l’AIC</h1>
<div id="origine" class="section level2">
<h2>Origine</h2>
<p>Le critère d’information d’Akaike (AIC), proposé par Hirotugu Akaike en 1973, permet de comparer l’ajustement de différents modèles.</p>
<p>L’AIC est basé sur la théorie de l’information. Imaginons une fonction <span class="math inline">\(f\)</span> qui associe à chaque individu de la population une valeur correspondant exactement à la variable réponse <span class="math inline">\(y\)</span>, tandis que <span class="math inline">\(g\)</span> est un modèle qui tente d’approximer <span class="math inline">\(f\)</span>. L’AIC approxime la perte d’information encourue en estimant la vraie distribution <span class="math inline">\(f\)</span> par le modèle <span class="math inline">\(g\)</span>.</p>
<p>Comment peut-on estimer la “distance” entre notre modèle et un modèle idéal <span class="math inline">\(f\)</span> qu’on ne connaît pas? En fait, l’AIC mesure cette distance à une constante près, ce qui est suffisant pour comparer différents modèles en fonction de leur niveau d’ajustement relatif. La valeur absolue de l’AIC n’a aucune signification, tout ce qui compte c’est la différence d’AIC entre modèles.</p>
</div>
<div id="définition-de-laic" class="section level2">
<h2>Définition de l’AIC</h2>
<p>L’AIC est défini comme:</p>
<p><span class="math display">\[ AIC = -2 \log(L) + 2 K \]</span></p>
<p>où <span class="math inline">\(L\)</span> est la fonction de vraisemblance (<em>likelihood</em>) et <span class="math inline">\(K\)</span> est le nombre de paramètres estimés par le modèle. Selon ce critère, un AIC plus petit correspond à un meilleur modèle.</p>
<p>La vraisemblance <span class="math inline">\(L(\theta | y)\)</span> est égale à la probabilité des observations de <span class="math inline">\(y\)</span> selon la valeur des paramètres <span class="math inline">\(\theta\)</span> du modèle. Dans la méthode du <strong>maximum de vraisemblance</strong>, les estimés des paramètres du modèle sont ceux qui maximisent la vraisemblance. Il s’agit d’une méthode générale pour estimer les paramètres d’un modèle; pour un modèles linéaire, le maximum de vraisemblance donne les mêmes estimés que la méthode des moindre carrés. La valeur de <span class="math inline">\(L\)</span> à son maximum, correspondant aux paramètres estimés du modèle, est donc utilisée dans l’équation de l’AIC ci-dessus.</p>
<p>Comme le <span class="math inline">\(R^2\)</span>, la vraisemblance tend à augmenter avec chaque paramètre ajouté au modèle. Le deuxième terme de l’AIC, proportionnel au nombre de paramètres estimés <span class="math inline">\(K\)</span>, pénalise donc les modèles plus complexes.</p>
<p><strong>Notes</strong></p>
<ol style="list-style-type: decimal">
<li>Pour un modèle linéaire, <span class="math inline">\(K\)</span> doit compter l’ordonnée à l’origine ainsi que l’estimé de la variance des résidus, <span class="math inline">\(\sigma^2\)</span>.</li>
</ol>
<p>Ex.: Pour le modèle <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon\)</span>, <span class="math inline">\(K = 4\)</span> car on estime trois coefficients <span class="math inline">\(\beta\)</span> ainsi que la variance des résidus <span class="math inline">\(\epsilon\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>L’AIC aurait pu être défini comme <span class="math inline">\(-\log(L) + K\)</span>. Le facteur de 2 a été ajouté pour des raisons historiques (afin de relier l’AIC à d’autres mesures d’ajustement existantes).</li>
</ol>
</div>
<div id="aic-et-validation-croisée" class="section level2">
<h2>AIC et validation croisée</h2>
<p>Il a été démontré que la comparaison des modèles avec l’AIC est équivalente à une validation croisée qui met de côté une observation à la fois (<em>leave-one-out cross-validation</em>). Ainsi, l’AIC nous permet d’estimer le pouvoir prédictif de nos modèles sur de nouvelles données, sans avoir à répéter plusieurs fois l’ajustement du modèle comme dans la méthode de validation croisée.</p>
</div>
<div id="aicc-pour-petits-échantillons" class="section level2">
<h2>AICc pour petits échantillons</h2>
<p>La formule de l’AIC présentée ci-dessus est une bonne approximation lorsque le nombre d’observations <span class="math inline">\(n\)</span> est suffisant. Si <span class="math inline">\(n\)</span> est faible par rapport au nombre de paramètres estimés (si <span class="math inline">\(n/K\)</span> &lt; 40, selon le manuel de Burnham et Anderson), cette formule ne pénalise pas suffisamment les modèles complexes. Dans ce cas, on remplace l’AIC par l’AICc, qui inclut une correction supplémentaire:</p>
<p><span class="math display">\[ AICc = -2 \log(L) + 2 K \left( \frac{n}{n-K-1} \right) \]</span> La fraction <span class="math inline">\(n / (n-K-1)\)</span> est supérieure à 1, donc le deuxième terme de l’AICc est un peu plus grand que <span class="math inline">\(2K\)</span>, mais la différence entre les deux diminue à mesure que <span class="math inline">\(n\)</span> augmente.</p>
<p>Lorsqu’on compare plusieurs modèles, il faut utiliser la même mesure pour chacun d’eux. On utilise donc l’AICc pour tous les modèles dès qu’un des modèles a un petit ratio <span class="math inline">\(n/K\)</span>.</p>
</div>
<div id="classification-des-modèles-avec-laic" class="section level2">
<h2>Classification des modèles avec l’AIC</h2>
<p>Comme il a été mentionné plus haut, la valeur absolue de l’AIC n’est pas importante, seules les différences d’AIC entre modèles comptent. Après avoir calculé l’AIC pour différents modèles, on classe ceux-ci en fonction de la différence entre leur AIC et le minimum des AIC (qui correspond au meilleur modèle): <span class="math inline">\(\Delta AIC = AIC - \min AIC\)</span>. Le meilleur modèle a un <span class="math inline">\(\Delta AIC = 0\)</span>.</p>
<p>Bien sûr, puisque nos données sont basées sur un échantillonnage aléatoire, il n’est pas certain que le meilleur modèle identifié par l’AIC serait encore le même avec un échantillon différent.</p>
<p>L’expression:</p>
<p><span class="math display">\[ e^{-\frac{\Delta AIC}{2} } \]</span></p>
<p>correspond à au rapport de la plausibilité (<em>evidence ratio</em>) de chaque modèle vs. celui ayant l’AIC minimal. Par exemple, <span class="math inline">\(\Delta AIC = 2\)</span> correspond à un ratio de ~0.37 (~3 fois moins probable), tandis que <span class="math inline">\(\Delta AIC = 10\)</span> correspond à un ratio de ~0.0067 (~150 fois moins probable).</p>
<p>Si on a <span class="math inline">\(m\)</span> modèles possibles, on peut normaliser les rapports de plausibilité par leur somme pour obtenir le poids d’Akaike <span class="math inline">\(w_i\)</span> d’un modèle <span class="math inline">\(i\)</span>.</p>
<p><span class="math display">\[ w_i = \frac{e^{\frac{-\Delta AIC_i}{2}}}{\sum_{j=1}^{m} e^{\frac{-\Delta AIC_j}{2}}}\]</span></p>
<p>Puisque la somme des <span class="math inline">\(w_i\)</span> est égale à 1, ces poids représentent la probabilité que chaque modèle soit identifié comme le meilleur modèle par l’AIC, si on répétait l’échantillonnage plusieurs fois.</p>
</div>
<div id="exemple-2" class="section level2">
<h2>Exemple</h2>
<p>Prenons le jeu de données de Johnson et Simberloff sur le nombre d’espèces de plantes vasculaires de différentes îles britanniques, que nous avons utilisé lors d’un laboratoire précédent.</p>
<pre class="r"><code>iles &lt;- read.csv(&quot;../donnees/britain_species.csv&quot;)
str(iles)</code></pre>
<pre><code>## &#39;data.frame&#39;:    42 obs. of  7 variables:
##  $ island      : chr  &quot;Ailsa&quot; &quot;Anglesey&quot; &quot;Arran&quot; &quot;Barra&quot; ...
##  $ area        : num  0.8 712.5 429.4 18.4 31.1 ...
##  $ elevation   : int  340 127 874 384 226 1343 210 103 143 393 ...
##  $ soil_types  : int  1 3 4 2 1 16 1 3 1 1 ...
##  $ latitude    : num  55.3 53.3 55.6 57 60.1 54.3 57.1 56.6 56.1 56.9 ...
##  $ dist_britain: num  14 0.2 5.2 77.4 201.6 ...
##  $ species     : int  75 855 577 409 177 1666 300 443 482 453 ...</code></pre>
<p>Nous modifions d’abord le jeu de données pour exclure l’île de Grande-Bretagne et pour appliquer une transformation logarithmique à la superfice des îles, leur distance de la Grande-Bretagne et leur nombre d’espèces.</p>
<pre class="r"><code>iles2 &lt;- filter(iles, island != &quot;Britain&quot;) %&gt;%
    mutate(log_area = log(area), log_dist = log(dist_britain), 
           log_sp = log(species))</code></pre>
<p>Le modèle le plus complexe que nous considérons est le suivant, où le nombre d’espèce dépend de la superficie de l’île, sa distance de la Grande-Bretagne et sa latitude.</p>
<pre class="r"><code>mod_comp &lt;- lm(log(species) ~ log(area) + log(dist_britain) + latitude, data = iles2)
summary(mod_comp)</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(species) ~ log(area) + log(dist_britain) + latitude, 
##     data = iles2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.19638 -0.09484  0.04124  0.22668  0.52043 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       14.36786    1.94448   7.389 8.65e-09 ***
## log(area)          0.21112    0.03165   6.670 7.83e-08 ***
## log(dist_britain)  0.02452    0.05251   0.467    0.643    
## latitude          -0.16775    0.03624  -4.629 4.42e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3883 on 37 degrees of freedom
## Multiple R-squared:  0.6941, Adjusted R-squared:  0.6693 
## F-statistic: 27.99 on 3 and 37 DF,  p-value: 1.267e-09</code></pre>
<p>Vérifions d’abord les graphiques de diagnostic pour le modèle complet:</p>
<p><img src="8-Selection_modeles_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Trois îles ont des résidus très négatifs (moins d’espèces que prévues) et s’éloignent de la normale, mais nous jugeons que l’ajustement est acceptable.</p>
<p>Ensuite, nous créons une liste de modèles qui n’incluent qu’un ou deux des prédicteurs du modèle complet, ainsi qu’un modèle nul: <code>log_sp ~ 1</code>, qui représente une fonction constante (indépendante des prédicteurs).</p>
<pre class="r"><code>liste_mod &lt;- list(
    nul = lm(log_sp ~ 1, iles2),
    area = lm(log_sp ~ log_area, iles2),
    dist = lm(log_sp ~ log_dist, iles2),
    lat = lm(log_sp ~ latitude, iles2),
    area_dist = lm(log_sp ~ log_area + log_dist, iles2),
    area_lat = lm(log_sp ~ log_area + latitude, iles2),
    dist_lat = lm(log_sp ~ log_dist + latitude, iles2),
    comp = lm(log_sp ~ log_area + log_dist + latitude, iles2)
)</code></pre>
<p>Nous sommes maintenant prêts à comparer ces modèles avec l’AIC. Nous utiliserons le package <strong>AICcmodavg</strong>, qui compte plusieurs fonctions pour la comparaison de modèles et l’inférence multi-modèles.</p>
<p>Le modèle complet ici compte 5 paramètres estimés et 41 observations, donc il est préférable d’utiliser l’AICc. La fonction <code>AICc</code> calcule l’AICc pour un seul modèle.</p>
<pre class="r"><code>library(AICcmodavg)
AICc(mod_comp)</code></pre>
<pre><code>## [1] 46.29878</code></pre>
<p>Au lieu d’appeler <code>AICc</code> séparément pour chaque modèle, appliquons la fonction <code>aictab</code> pour produire un tableau de comparaison d’AICc à partir de notre liste de modèles.</p>
<pre class="r"><code>aictab(liste_mod)</code></pre>
<pre><code>## 
## Model selection based on AICc:
## 
##           K  AICc Delta_AICc AICcWt Cum.Wt     LL
## area_lat  4 43.94       0.00   0.77   0.77 -17.41
## comp      5 46.30       2.36   0.23   1.00 -17.29
## area_dist 4 62.42      18.49   0.00   1.00 -26.66
## area      3 69.16      25.23   0.00   1.00 -31.26
## lat       3 74.93      31.00   0.00   1.00 -34.14
## dist_lat  4 76.07      32.13   0.00   1.00 -33.48
## dist      3 78.85      34.92   0.00   1.00 -36.10
## nul       2 87.47      43.53   0.00   1.00 -41.58</code></pre>
<p>Le tableau présente les modèles en ordre croissant d’AICc, avec les informations supplémentaires suivantes:</p>
<ul>
<li>Les valeurs de <span class="math inline">\(\log(L)\)</span> (<code>LL</code>) et <span class="math inline">\(K\)</span> utilisées pour le calcul de l’AICc.</li>
<li>Le <span class="math inline">\(\Delta AICc\)</span> (<code>Delta_AICc</code>), les poids d’Akaike (<code>AICcWt</code>) et les poids cumulatifs (<code>Cum.Wt</code>).</li>
</ul>
<p>Nous voyons que le modèle incluant la superficie et la latitude reçoit 77% du poids, le modèle complet reçoit 23% du poids et le reste des modèles ont un poids négligeable.</p>
</div>
<div id="conseils-et-points-à-surveiller" class="section level2">
<h2>Conseils et points à surveiller</h2>
<ul>
<li><p>Pour que les valeurs de l’AIC (ou AICc) soient comparables, tous les modèles doivent avoir la même variable réponse. On ne pourrait pas par exemple comparer des modèles où la réponse est <code>species</code> avec d’autres où la réponse est <code>log(species)</code>.</p></li>
<li><p>Chaque modèle doit aussi être basé sur le même ensemble d’observations. Il faut particulièrement faire attention aux valeurs manquantes <code>NA</code> dans R. R exclut automatiquement d’un modèle toute observation qui a une valeur manquante pour une des variables incluses dans le modèle. Avant d’estimer une série de modèles, assurez-vous donc d’éliminer toutes les observations qui ont au moins une valeur manquante pour une des variables qui vous intéressent.</p></li>
<li><p>Dans l’exemple précédent, tous les modèles étaient nichés (<em>nested</em>), c’est-à-dire que chaque modèle contenait un sous-ensemble des prédicteurs du modèle complet. L’AIC peut aussi comparer des modèles non-nichés.</p></li>
<li><p>L’AIC compare les modèles entre eux, mais ne nous dit pas si le “meilleur” modèle est bien ajusté aux données. Pour un cas où tous les modèles sont nichés, il suffit de vérifier l’ajustement du modèle le plus complexe, comme nous l’avons fait ci-dessus. Dans ce cas, tout modèle plus simple avec un meilleur AIC sera aussi bien ajusté.</p></li>
<li><p>Dans l’exemple, les deux prédicteurs du meilleur modèle selon l’AIC (<code>log_area</code> et <code>lat</code>) étaient aussi les deux prédicteurs avec un effet significatif dans le modèle complet. Cela n’est pas le cas en général.</p></li>
<li><p>Il faut éviter de se fier aux valeurs <span class="math inline">\(p\)</span> et aux intervalles de confiance des paramètres du meilleur modèle choisi par l’AIC. Les résultats extraits d’un seul modèle ne tiennent pas compte du fait que nous avons comparé plusieurs modèles pour choisir le meilleur au bout du compte; comme tous les problèmes de comparaisons multiples, les valeurs <span class="math inline">\(p\)</span> et les intervalles de confiance du modèle choisi seront trop optimistes.</p></li>
</ul>
</div>
</div>
<div id="comment-choisir-lensemble-de-modèles-à-comparer" class="section level1">
<h1>Comment choisir l’ensemble de modèles à comparer?</h1>
<p>Dans leur manuel sur la sélection de modèles avec l’AIC (voir les références à la fin du document), Burnham et Anderson recommandent de choisir un ensemble de modèles restreint basé sur la connaissance du sujet et des hypothèses scientifiques intéressantes.</p>
<p>Il est important de noter que le nombre de modèles possibles augmente rapidement avec le nombre de prédicteurs, même si nous nous limitons aux modèles linéaires sans interaction. Pour <span class="math inline">\(k\)</span> prédicteurs, il y a <span class="math inline">\(2^k\)</span> modèles possibles. Dans l’exemple précédent, nous avions 8 modèles possibles pour 3 prédicteurs; avec 7 prédicteurs, ce nombre augmente à 128!</p>
<div id="algorithmes-de-sélection-séquentielle" class="section level2">
<h2>Algorithmes de sélection séquentielle</h2>
<p>Il existe des algorithmes de sélection séquentielle (<em>forward</em>, <em>backward</em> ou <em>stepwise</em>), comme la fonction <code>stepAIC</code> dans R, qui visent à trouver le meilleur modèle selon l’AIC sans avoir à évaluer tous les modèles possibles.</p>
<p>Par exemple, une sélection par addition (<em>forward selection</em>) fonctionne ainsi:</p>
<ol style="list-style-type: decimal">
<li>Calculer l’AIC du modèle nul (0 prédicteur);</li>
<li>Évaluer tous les modèles avec 1 prédicteur (<code>y ~ x1</code>, <code>y ~ x2</code>, etc.), conserver le meilleur si son AIC est inférieur au modèle nul.</li>
<li>Évaluer tous les modèles avec 2 prédicteurs incluant le 1er prédicteur choisi à l’étape 2, conserver le meilleur si son AIC est meilleur que le modèle de l’étape 2.</li>
<li>Continuer ainsi jusqu’à ce qu’aucun des modèles plus complexes n’obtienne un AIC plus bas que le modèle choisi à l’étape précédente.</li>
</ol>
<p>Les méthodes séquentielles ont plusieurs inconvénients:</p>
<ul>
<li>Il n’est pas garanti qu’elles trouvent le meilleur modèle.</li>
<li>Ces méthodes permettent de choisir un modèle, mais sans pouvoir calculer les poids d’Akaike, on ne sait pas si plusieurs autres modèles sont presque aussi probables.</li>
<li>Elles encouragent la comparaison d’un très grand nombre de modèles, ce qui est déconseillé pour les raisons mentionnées plus haut.</li>
</ul>
</div>
</div>
<div id="prédictions-multi-modèles" class="section level1">
<h1>Prédictions multi-modèles</h1>
<p>Comme dans le dernier laboratoire, supposons que nous souhaitions prédire le nombre d’espèces pour de nouvelles îles avec des valeurs connues des prédicteurs.</p>
<pre class="r"><code>iles_nouv &lt;- data.frame(area = c(1, 40), dist_britain = c(5, 20), 
                        latitude = c(60, 55)) %&gt;%
    mutate(log_area = log(area), log_dist = log(dist_britain))
iles_nouv</code></pre>
<pre><code>##   area dist_britain latitude log_area log_dist
## 1    1            5       60 0.000000 1.609438
## 2   40           20       55 3.688879 2.995732</code></pre>
<p>Nous pourrions faire les prédictions à partir du meilleur modèle identifié par l’AIC. Toutefois, lorsque plusieurs modèles sont assez plausibles, il est possible d’améliorer les prédictions en faisant la <em>moyenne pondérée</em> des valeurs prédites par les différents modèles, avec une pondération basée sur les poids d’Akaike:</p>
<p><span class="math display">\[ \hat{y} = \sum_{i = 1}^m w_i \hat{y_i} \]</span></p>
<p>Dans cette équation, <span class="math inline">\(\hat{y_i}\)</span> est la prédiction du modèle <span class="math inline">\(i\)</span> et <span class="math inline">\(\hat{y}\)</span> est la prédiction moyenne.</p>
<p>Dans le package <em>AICcmodavg</em>, la fonction <code>modavgPred</code> calcule les prédictions multi-modèles de cette façon.</p>
<pre class="r"><code>modavgPred(liste_mod, newdata = iles_nouv)</code></pre>
<pre><code>## 
## Model-averaged predictions on the response scale
## based on entire model set and 95% confidence interval:
## 
##   mod.avg.pred uncond.se lower.CL upper.CL
## 1        4.411     0.183    4.053    4.769
## 2        5.979     0.089    5.803    6.154</code></pre>
<p>Notez que l’intervalle fourni est un intervalle de confiance, pas un intervalle de prédiction. Autrement dit, il nous indique l’incertitude sur la valeur moyenne de <code>log_sp</code> pour ces valeurs des prédicteurs et non l’incertitude sur <code>log_sp</code> pour une seule île.</p>
</div>
<div id="résumé" class="section level1">
<h1>Résumé</h1>
<ul>
<li><p>Le facteur d’inflation de la variance (VIF) indique si la valeur d’un prédicteur est fortement corrélée à celles des autres prédicteurs. Un VIF élevé rend difficile l’estimation des coefficients pour les prédicteurs corrélés.</p></li>
<li><p>Le choix entre plusieurs modèles avec différents niveaux de complexité requiert un compromis entre le biais (sous-ajustement) et la variance (surajustement).</p></li>
<li><p>Pour estimer la qualité d’un modèle, il est utile de comparer ses prédictions à de nouvelles observations, différentes de celles utilisées pour estimer les paramètres du modèle.</p></li>
<li><p>L’AIC (ou l’AICc, pour de petits échantillons) est basé sur la vraisemblance du modèle, avec une pénalité pour les modèles comprenant un plus grand nombre de paramètres.</p></li>
<li><p>L’AIC approxime bien la capacité de prédiction relative de différents modèles et peut donc servir à classer ceux-ci.</p></li>
<li><p>La valeur absolue de l’AIC n’a aucune signification. Seules les différences d’AIC entre modèles basés sur les mêmes observations peuvent être interprétées.</p></li>
<li><p>Les poids d’Akaike, calculés à partir des différences d’AIC entre modèles, estiment la probabilité que chaque modèle soit identifié comme le meilleur modèle si on répétait l’échantillonnage. Ces poids servent aussi à faire la moyenne pondérée des prédictions de différents modèles.</p></li>
</ul>
</div>
<div id="références" class="section level1">
<h1>Références</h1>
<p>Burnham, K.P et Anderson, D.R. (2002) <em>Model selection and multimodel inference : a practical information-theoretic approach</em>, 2e éd. Springer-Verlag, New York.</p>
<p>Anderson, D.R. et Burnham, K.P. (2002) Avoiding pitfalls when using information-theoretic methods. <em>The Journal of Wildlife Management</em> 66: 912-918.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
